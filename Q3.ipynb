{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv \n",
    "import time\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Vocabulary from ./Data/A3 files/train.targets train.sources dev.targets dev.sources test.targets test.sources\n",
    "\n",
    "train_sources_text = open(\"./Data/A3 files/train.sources\", \"r\")\n",
    "train_targets_text = open(\"./Data/A3 files/train.targets\", \"r\")\n",
    "dev_sources_text = open(\"./Data/A3 files/dev.sources\", \"r\")\n",
    "dev_targets_text = open(\"./Data/A3 files/dev.targets\", \"r\")\n",
    "test_sources_text = open(\"./Data/A3 files/test.sources\", \"r\")\n",
    "test_targets_text = open(\"./Data/A3 files/test.targets\", \"r\")\n",
    "\n",
    "train_sources = train_sources_text.readlines()\n",
    "train_targets = train_targets_text.readlines()\n",
    "dev_sources = dev_sources_text.readlines()\n",
    "dev_targets = dev_targets_text.readlines()\n",
    "test_sources = test_sources_text.readlines()\n",
    "test_targets = test_targets_text.readlines()\n",
    "\n",
    "train_vocab = set()\n",
    "test_vocab = set()\n",
    "\n",
    "\n",
    "train_sources_text.close()\n",
    "train_targets_text.close()\n",
    "dev_sources_text.close()\n",
    "dev_targets_text.close()\n",
    "test_sources_text.close()\n",
    "test_targets_text.close()\n",
    "\n",
    "train_sources_list = []\n",
    "train_targets_list = []\n",
    "dev_sources_list = []\n",
    "dev_targets_list = []\n",
    "test_sources_list = []\n",
    "test_targets_list = []\n",
    "\n",
    "for line in train_sources:\n",
    "    # Get individual charecters from line, add unique charecters to vocab\n",
    "    train_vocab.update(set(line))\n",
    "    # Add line to list after stripping \\n\n",
    "    train_sources_list.append(line.strip('\\n'))\n",
    "\n",
    "for line in train_targets:\n",
    "    # Get individual charecters from line, add unique charecters to vocab\n",
    "    test_vocab.update(set(line))\n",
    "    # Add line to list after stripping \\n\n",
    "    train_targets_list.append(line.strip('\\n'))\n",
    "\n",
    "for line in dev_sources:\n",
    "    # Get individual charecters from line, add unique charecters to vocab\n",
    "    # vocab.update(set(line))\n",
    "    # Add line to list after stripping \\n\n",
    "    dev_sources_list.append(line.strip('\\n'))\n",
    "\n",
    "for line in dev_targets:\n",
    "    # Get individual charecters from line, add unique charecters to vocab\n",
    "    # vocab.update(set(line))\n",
    "    # Add line to list after stripping \\n\n",
    "    dev_targets_list.append(line.strip('\\n'))\n",
    "\n",
    "for line in test_sources:\n",
    "    # Get individual charecters from line, add unique charecters to vocab\n",
    "    # vocab.update(set(line))\n",
    "    # Add line to list after stripping \\n\n",
    "    test_sources_list.append(line.strip('\\n'))\n",
    "\n",
    "for line in test_targets:\n",
    "    # Get individual charecters from line, add unique charecters to vocab\n",
    "    # vocab.update(set(line))\n",
    "    # Add line to list after stripping \\n\n",
    "    test_targets_list.append(line.strip('\\n'))\n",
    "\n",
    "\n",
    "# Add <pad> and <SOS> , <EOS> to vocab\n",
    "train_vocab.add('<pad>')\n",
    "train_vocab.add('<SOS>')\n",
    "train_vocab.add('<EOS>')\n",
    "\n",
    "# Remove \\n from vocab\n",
    "train_vocab.remove('\\n')\n",
    "\n",
    "# Add <pad> and <SOS> , <EOS> to vocab\n",
    "test_vocab.add('<pad>')\n",
    "test_vocab.add('<SOS>')\n",
    "test_vocab.add('<EOS>')\n",
    "\n",
    "# Remove \\n from vocab\n",
    "test_vocab.remove('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionary from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 0, 'h': 1, 'c': 2, '{': 3, 'r': 4, '2': 5, 'y': 6, 'e': 7, ':': 8, 'z': 9, 'a': 10, 'b': 11, 'T': 12, ',': 13, '}': 14, 'f': 15, 'q': 16, '6': 17, 'v': 18, 'm': 19, '<SOS>': 20, '1': 21, '7': 22, 'd': 23, '5': 24, 'n': 25, 'k': 26, '4': 27, 'i': 28, 'x': 29, 'l': 30, '0': 31, '8': 32, ' ': 33, 'U': 34, 'o': 35, '3': 36, 'g': 37, 'p': 38, '<EOS>': 39, '\"': 40, 's': 41, '9': 42, '*': 43, 'u': 44, '<pad>': 45}\n",
      "46\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "## create vocab to index and index to vocab dictionaries\n",
    "source_char_to_int = {}\n",
    "source_int_to_char = {}\n",
    "\n",
    "for i, word in enumerate(train_vocab):\n",
    "    source_char_to_int[word] = i\n",
    "    source_int_to_char[i] = word\n",
    "\n",
    "# print(source_char_to_int)\n",
    "\n",
    "target_char_to_int = {}\n",
    "target_int_to_char = {}\n",
    "\n",
    "for i, word in enumerate(test_vocab):\n",
    "    target_char_to_int[word] = i\n",
    "    target_int_to_char[i] = word\n",
    "\n",
    "print(target_char_to_int)\n",
    "\n",
    "print(len(target_char_to_int))\n",
    "print(len(source_char_to_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by Charecters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "def encode_data_sources(data):\n",
    "    encoded_data = []\n",
    "    for i in range(len(data)):                  # appending 0 for <SOS> token \n",
    "        encoded_data.append([source_char_to_int[char] for char in data[i]])\n",
    "    \n",
    "    # encoded_data = [[[0]]] + encoded_data         # appending 0 for <SOS> token\n",
    "    return encoded_data\n",
    "\n",
    "def encode_data_targets(data):\n",
    "    encoded_data = []\n",
    "    for i in range(len(data)):                  # appending 0 for <SOS> token \n",
    "        encoded_data.append([target_char_to_int[char] for char in data[i]])\n",
    "    \n",
    "    # encoded_data = [[[0]]] + encoded_data         # appending 0 for <SOS> token\n",
    "    return encoded_data\n",
    "\n",
    "def decode_data_sources(data):\n",
    "    decoded_data = []\n",
    "    for i in range(len(data)+1):\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        decoded_data.append([source_int_to_char[int] for int in data[i]])\n",
    "    return decoded_data\n",
    "\n",
    "def decode_data_targets(data):\n",
    "    decoded_data = []\n",
    "    for i in range(len(data)+1):\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        decoded_data.append([target_int_to_char[int] for int in data[i]])\n",
    "    return decoded_data\n",
    "\n",
    "\n",
    "train_sources_encoded = encode_data_sources(train_sources_list)\n",
    "train_targets_encoded = encode_data_targets(train_targets_list)\n",
    "dev_sources_encoded = encode_data_sources(dev_sources_list)\n",
    "dev_targets_encoded = encode_data_targets(dev_targets_list)\n",
    "test_sources_encoded = encode_data_sources(test_sources_list)\n",
    "test_targets_encoded = encode_data_targets(test_targets_list)\n",
    "\n",
    "### For every sequence in train_sources_encoded, train_targets_encoded, dev_sources_encoded, dev_targets_encoded, \n",
    "# test_sources_encoded, test_targets_encoded\n",
    "#  add <SOS> token at start of sequence \n",
    "# add <EOS> token at the end of the sequence and <pad> \n",
    "# tokens to make the sequence length equal to the maximum sequence length in the dataset which is 500\n",
    "\n",
    "max_len = 500\n",
    "\n",
    "for i in range(len(train_sources_encoded)):\n",
    "    train_sources_encoded[i] = [source_char_to_int['<SOS>']] + train_sources_encoded[i] \n",
    "    train_targets_encoded[i].append(target_char_to_int['<EOS>'])\n",
    "\n",
    "    if len(train_sources_encoded[i]) > max_len:\n",
    "        train_sources_encoded[i] = train_sources_encoded[i][:max_len]       # Truncating the sequence to max_len\n",
    "        train_targets_encoded[i] = train_targets_encoded[i][:max_len]       # Truncating the sequence to max_len\n",
    "    else:\n",
    "        train_sources_encoded[i] = train_sources_encoded[i] + [source_char_to_int['<pad>']] * (max_len - len(train_sources_encoded[i]))\n",
    "        train_targets_encoded[i] = train_targets_encoded[i] + [target_char_to_int['<pad>']] * (max_len - len(train_targets_encoded[i]))\n",
    "\n",
    "for i in range(len(dev_sources_encoded)):\n",
    "    dev_sources_encoded[i] = [source_char_to_int['<SOS>']] + dev_sources_encoded[i] \n",
    "    dev_targets_encoded[i].append(target_char_to_int['<EOS>'])\n",
    "    \n",
    "    if len(dev_sources_encoded[i]) > max_len:\n",
    "        dev_sources_encoded[i] = dev_sources_encoded[i][:max_len]       # Truncating the sequence to max_len\n",
    "        dev_targets_encoded[i] = dev_targets_encoded[i][:max_len]       # Truncating the sequence to max_len\n",
    "    else:\n",
    "        dev_sources_encoded[i] = dev_sources_encoded[i] + [source_char_to_int['<pad>']] * (max_len - len(dev_sources_encoded[i]))\n",
    "        dev_targets_encoded[i] = dev_targets_encoded[i] + [target_char_to_int['<pad>']] * (max_len - len(dev_targets_encoded[i]))\n",
    "\n",
    "for i in range(len(test_sources_encoded)):\n",
    "    test_sources_encoded[i] = [source_char_to_int['<SOS>']] + test_sources_encoded[i] \n",
    "    test_targets_encoded[i].append(target_char_to_int['<EOS>'])\n",
    "    \n",
    "    if len(test_sources_encoded[i]) > max_len:\n",
    "        test_sources_encoded[i] = test_sources_encoded[i][:max_len]       # Truncating the sequence to max_len\n",
    "        test_targets_encoded[i] = test_targets_encoded[i][:max_len]       # Truncating the sequence to max_len\n",
    "    else:\n",
    "        test_sources_encoded[i] = test_sources_encoded[i] + [source_char_to_int['<pad>']] * (max_len - len(test_sources_encoded[i]))\n",
    "        test_targets_encoded[i] = test_targets_encoded[i] + [target_char_to_int['<pad>']] * (max_len - len(test_targets_encoded[i]))\n",
    "\n",
    "print(len(train_sources_encoded[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([172719, 500])\n",
      "torch.Size([172719, 500])\n",
      "torch.Size([21590, 500])\n",
      "torch.Size([21590, 500])\n",
      "torch.Size([21590, 500])\n",
      "torch.Size([21590, 500])\n"
     ]
    }
   ],
   "source": [
    "train_sources_tensor = torch.Tensor(train_sources_encoded)\n",
    "train_targets_tensor = torch.Tensor(train_targets_encoded)\n",
    "dev_sources_tensor = torch.Tensor(dev_sources_encoded)\n",
    "dev_targets_tensor = torch.Tensor(dev_targets_encoded)\n",
    "test_sources_tensor = torch.Tensor(test_sources_encoded)\n",
    "test_targets_tensor = torch.Tensor(test_targets_encoded)\n",
    "\n",
    "\n",
    "print(train_sources_tensor.shape)\n",
    "print(train_targets_tensor.shape)\n",
    "print(dev_sources_tensor.shape)\n",
    "print(dev_targets_tensor.shape)\n",
    "print(test_sources_tensor.shape)\n",
    "print(test_targets_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, sources, targets):\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.sources[idx]\n",
    "        target = self.targets[idx]\n",
    "        return source, target\n",
    "    \n",
    "train_dataset = MyDataset(train_sources_tensor, train_targets_tensor)\n",
    "dev_dataset = MyDataset(dev_sources_tensor, dev_targets_tensor)\n",
    "test_dataset = MyDataset(test_sources_tensor, test_targets_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Seq2Seq Model with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seq2Seq with Attention\n",
    "\n",
    "## Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=512, hidden_size=512, num_layers=2, dropout=0.5,bidirectional=True,batch_first=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=batch_first)\n",
    "\n",
    "    def encode_input(self, x):\n",
    "        encoded_x = torch.zeros(len(x),500,dtype= int)\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[i])):\n",
    "                print(x[i][j].cpu().numpy())\n",
    "                encoded_x[i][j] = source_char_to_int[int(x[i][j].cpu().numpy())]\n",
    "            encoded_x[i][len(x[i])] = source_char_to_int['<EOS>']\n",
    "        \n",
    "        return encoded_x.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "\n",
    "        # encoded_x = self.encode_input(x)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))                     # As per assignment, dropout is applied to embedding and not to inputs of hidden layer\n",
    "\n",
    "        # embedding shape: (batch_size, seq_length, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(embedding)\n",
    "\n",
    "        # print(\"Outputs (Enc)\",outputs.shape)\n",
    "\n",
    "        # outputs shape: (N, 500, 1024)\n",
    "        # hidden shape: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # cell shape: (num_layers*num_directions, batch_size, hidden_size)\n",
    "\n",
    "        # concatenate hiden states of last layer of bidrectional LSTM\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1).unsqueeze(0).expand(self.num_layers, -1, -1).contiguous()\n",
    "        # hidden shape: (2,N,1024)\n",
    "\n",
    "        return outputs, hidden, cell\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # self.linear = nn.Linear()\n",
    "\n",
    "    \n",
    "    def forward(self,decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden shape: (2, N, 1024)\n",
    "        # encoder_outputs shape: (N, 500, 1024)\n",
    "\n",
    "        # attention shape: (N,500)\n",
    "        hidden_last = decoder_hidden[-1,:,:].unsqueeze(0)\n",
    "        # print(\"Hidden last\", hidden_last.shape)\n",
    "                \n",
    "\n",
    "        # print(encoder_outputs.shape)\n",
    "        attention = torch.matmul(hidden_last.permute(1,0,2),encoder_outputs.permute(0,2,1))\n",
    "\n",
    "        attention = self.softmax(attention)\n",
    "\n",
    "        context = torch.matmul(attention,encoder_outputs)\n",
    "\n",
    "        # context shape: (N,1,1024)\n",
    "\n",
    "        return context, attention\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size=512, hidden_size=512, num_layers=2, dropout=0.5, batch_first=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size+hidden_size*2, hidden_size*2, num_layers, dropout=dropout, batch_first=batch_first)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        self.attention = Attention()\n",
    "    \n",
    "    def forward(self, x, encoder_hidden, encoder_outputs,teacher_forcing_ratio=1.0):\n",
    "        ### x : (N,500)\n",
    "        ### hidden : (2,N,1024)\n",
    "        ### encoder_outputs : (N,500,1024)\n",
    "        ### teacher_forcing_ratio : float\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        max_len = x.shape[1]\n",
    "        # print(\"Encoder outputs (Dec)\",encoder_outputs.shape)\n",
    "        vocab_size = self.output_size\n",
    "\n",
    "        target_embedding = self.dropout(self.embedding(x))\n",
    "        # target_embedding shape: (N,500,512)\n",
    "\n",
    "        initial_hidden = torch.randn(self.num_layers, batch_size, self.hidden_size*2).to(device)    # (2,N,1024)\n",
    "        initial_cell = torch.randn(self.num_layers, batch_size, self.hidden_size*2).to(device)      # (2,N,1024)\n",
    "\n",
    "        outputs = []\n",
    "        query = []\n",
    "        cell_states = []\n",
    "        hidden_states = [initial_hidden]            # (2,N,1024)\n",
    "\n",
    "        for timestep in range(max_len):\n",
    "\n",
    "            if (timestep == 0):\n",
    "                context , attention = self.attention(hidden_states[-1],encoder_outputs)\n",
    "\n",
    "                if np.random.random() < teacher_forcing_ratio:\n",
    "                    input = target_embedding[:,timestep,:]\n",
    "                    input = input.unsqueeze(1)\n",
    "                else:\n",
    "                    input = torch.tensor([target_char_to_int['SOS']]*batch_size).to(device) # (N)\n",
    "                    input  = input.unsqueeze(1)         # (N,1)\n",
    "                    input  = self.embedding(input)      # (N,1,512)\n",
    "                \n",
    "                # print(\"Input shape\",input.shape)\n",
    "                # print(\"Context shape\",context.shape)\n",
    "                \n",
    "                input = torch.cat((input,context),dim=2)\n",
    "                # input shape: (N,1,1536)\n",
    "                # print(\"Input shape\",input.shape)\n",
    "\n",
    "                output, (dec_hidden, cell) = self.lstm(input, (initial_hidden, initial_cell))    # (N,1,1024)\n",
    "                # print(\"Output shape\",output.shape)\n",
    "                # print(\"Hidden shape\",hidden.shape)\n",
    "                # print(\"Cell shape\",cell.shape)\n",
    "                # output shape: (N,1,1024)\n",
    "                # hidden shape: (2,N,1024)\n",
    "                # cell shape: (2,N,1024)\n",
    "            else:\n",
    "\n",
    "                context , attention = self.attention(hidden_states[-1],encoder_outputs)\n",
    "\n",
    "                if np.random.random() < teacher_forcing_ratio:\n",
    "                    input = target_embedding[:,timestep,:]\n",
    "                    input = input.unsqueeze(1)\n",
    "\n",
    "                else:\n",
    "                    input = output[-1]\n",
    "                    input = self.embedding(input)\n",
    "                    # input shape: (N,1,512)\n",
    "                \n",
    "                input = torch.cat((input,context),dim=2)\n",
    "                # input shape: (N,1,1536)\n",
    "\n",
    "                # print(\"Input shape\",input.shape)\n",
    "\n",
    "                # hidden here is from the last time step of the last layer, not last layer itself\n",
    "\n",
    "                output, (hidden, cell) = self.lstm(input, (hidden_states[-1], cell))    # (N,1,1024)\n",
    "                # output shape: (N,1,1024)\n",
    "                # hidden shape: (2,N,1024)\n",
    "                # cell shape: (2,N,1024)\n",
    "            \n",
    "            # print(\"Output shape\",output.shape)\n",
    "            output = self.fc(output.squeeze(1))\n",
    "\n",
    "            # output = F.softmax(output, dim=1)\n",
    "\n",
    "            # output shape: (N, 500)\n",
    "            outputs.append(output)\n",
    "            hidden_states.append(dec_hidden)\n",
    "            cell_states.append(cell)\n",
    "            query.append(dec_hidden)        # previous dec_hidden\n",
    "\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        hidden_states = torch.stack(hidden_states, dim=1)\n",
    "        cell_states = torch.stack(cell_states, dim=1)\n",
    "\n",
    "        outputs = nn.LogSoftmax(dim=2)(outputs)\n",
    "        \n",
    "        return outputs, hidden_states, cell_states\n",
    "    \n",
    "    def predict(self,hidden,encoder_outputs):\n",
    "\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        \n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        cell_states = []\n",
    "        query = [hidden]            # (2,N,1024)\n",
    "\n",
    "        input = torch.tensor([target_char_to_int['<SOS>']]*batch_size).unsqueeze(1).to(device)    # (N,1)\n",
    "\n",
    "        for timestep in range(500):\n",
    "            \n",
    "            output, hidden , cell = self.forward(input,hidden,encoder_outputs,teacher_forcing_ratio=1.0)            # (N,2)\n",
    "\n",
    "            output = F.softmax(output, dim=-1)\n",
    "\n",
    "            outputs.append(output)\n",
    "            \n",
    "            output = torch.argmax(output,dim=-1)\n",
    "\n",
    "            # Concatenate x with output\n",
    "            # print(\"Output shape\",output.shape)\n",
    "            input = output\n",
    "            # print(\"Input shape\",input.shape)\n",
    "\n",
    "            # output = output.detach()\n",
    "            hidden_states.append(hidden)\n",
    "            cell_states.append(cell)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "\n",
    "        outputs = torch.argmax(nn.Softmax(dim=2)(outputs),dim=2)\n",
    "        # print(\"Final outputs\",outputs.shape)\n",
    "        hidden_states = torch.stack(hidden_states, dim=1)\n",
    "        cell_states = torch.stack(cell_states, dim=1)\n",
    "\n",
    "        return outputs, hidden_states, cell_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing=1.0):\n",
    "        # source shape: (batch_size, seq_length)\n",
    "        # target shape: (batch_size, seq_length)\n",
    "\n",
    "        batch_size = source.shape[0]\n",
    "        seq_length = source.shape[1]\n",
    "\n",
    "        encoder_outputs, encoder_hidden, encoder_cell = self.encoder(source)\n",
    "        \n",
    "        # encoder_outputs shape: (batch_size, seq_length, hidden_size*num_directions)\n",
    "        # encoder_hidden shape: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # encoder_cell shape: (num_layers*num_directions, batch_size, hidden_size)\n",
    "\n",
    "        decoder_outputs, decoder_hidden, attentions = self.decoder(target,encoder_hidden,encoder_outputs,teacher_forcing)\n",
    "        # decoder_outputs shape: (batch_size, seq_length, hidden_size*num_directions*2)\n",
    "        # decoder_hidden shape: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # attentions shape: (batch_size, seq_length)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train function\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time/60)\n",
    "    elapsed_secs = int(elapsed_time - elapsed_mins*60)\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, dev_loader, num_epochs):\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "\n",
    "    steps = 0\n",
    "    eval_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, (source, target) in enumerate(train_loader):\n",
    "            source = source.long().to(device)\n",
    "            target = target.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            encoder_outputs, encoder_hidden, encoder_cell = model.encoder(source)\n",
    "            # print(\"Encoder Outputs:\",encoder_outputs.shape)\n",
    "            # print(\"Encoder Hidden:\",encoder_hidden.shape)\n",
    "            # print(\"Encoder Cell:\",encoder_cell.shape)\n",
    "            \n",
    "            decoder_outputs, decoder_hidden, attention_weights = model.decoder(target,encoder_hidden, encoder_outputs)\n",
    "\n",
    "            # decoder_outputs shape: (batch_size, seq_length, hidden_size*num_directions*2)\n",
    "            # target shape: (batch_size, seq_length)\n",
    "            # decoder_outputs = decoder_outputs.view(-1, decoder_outputs.shape[-1])\n",
    "            # target = target.view(-1)\n",
    "            # print(\"Decoder Outputs:\",decoder_outputs.shape)\n",
    "            # print(\"Target:\",target.shape)\n",
    "\n",
    "            loss = criterion(decoder_outputs.view(-1,46), target.view(-1))\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            perplexity = np.exp(loss.item())\n",
    "\n",
    "            steps += 1\n",
    "            print(\"Batch:\",steps+1,\"Loss:\",loss.item(),\"Perplexity:\",perplexity)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        dev_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (source, target) in enumerate(dev_loader):\n",
    "                source = source.long().to(device)\n",
    "                target = target.long().to(device)\n",
    "\n",
    "                encoder_outputs, encoder_hidden, encoder_cell = model.encoder(source)\n",
    "                decoder_outputs, decoder_hidden, decoder_cell = model.decoder(target,encoder_hidden, encoder_outputs,1.0)\n",
    "\n",
    "                # decoder_outputs shape: (batch_size, seq_length, hidden_size*num_directions)\n",
    "                # target shape: (batch_size, seq_length)\n",
    "                # decoder_outputs = decoder_outputs.view(-1, decoder_outputs.shape[-1])\n",
    "                # target = target.view(-1)\n",
    "\n",
    "                decoder_outputs = decoder_outputs.view(-1,46)\n",
    "                target = target.view(-1)\n",
    "                \n",
    "                loss = criterion(decoder_outputs, target)\n",
    "                dev_loss += loss.item()\n",
    "\n",
    "        dev_loss /= len(dev_loader)\n",
    "        dev_losses.append(dev_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s',f'\\tTrain Loss: {train_loss:.3f}',f'\\t Val. Loss: {dev_loss:.3f}' , f'\\t Val. PPL: {np.exp(dev_loss):7.3f}')\n",
    "\n",
    "        torch.save(model.state_dict(),\"model_{epoch}.pt\")\n",
    "    \n",
    "\n",
    "    return train_losses, dev_losses\n",
    "\n",
    "\n",
    "### Test function\n",
    "# def test(model, criterion, test_loader):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for i, (source, target) in enumerate(test_loader):\n",
    "#             source = source.long().to(device)\n",
    "#             target = target.long().to(device)\n",
    "\n",
    "#             encoder_outputs, encoder_hidden, encoder_cell = model.encoder(source)\n",
    "#             decoder_outputs, decoder_hidden, decoder_cell = model.decoder.predict(encoder_hidden, encoder_outputs)\n",
    "\n",
    "#             # print(\"decoder outputs shape\",decoder_outputs.squeeze(2).shape)\n",
    "#             # print(\"target shape\",target.shape)\n",
    "\n",
    "#             # decoder_outputs shape: (batch_size, seq_length, hidden_size*num_directions)\n",
    "#             # target shape: (batch_size, seq_length)\n",
    "#             # decoder_outputs = decoder_outputs.view(-1, decoder_outputs.shape[-1])\n",
    "#             # target = target.view(-1)\n",
    "\n",
    "#             # loss = criterion(decoder_outputs.view(-1,46), target.view(-1))\n",
    "#             # test_loss += loss.item()\n",
    "\n",
    "#     # test_loss /= len(test_loader)\n",
    "#     # print(f'Test Loss: {test_loss:.3f}')\n",
    "\n",
    "#     return  , decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2 Loss: 3.82997727394104 Perplexity: 46.06149142614967\n",
      "Batch: 3 Loss: 3.7008368968963623 Perplexity: 40.4811687520783\n",
      "Batch: 4 Loss: 3.6206541061401367 Perplexity: 37.36199854369528\n",
      "Batch: 5 Loss: 3.545954465866089 Perplexity: 34.67276352301812\n",
      "Batch: 6 Loss: 3.4738852977752686 Perplexity: 32.26184613384499\n",
      "Batch: 7 Loss: 3.4077179431915283 Perplexity: 30.196256000180842\n",
      "Batch: 8 Loss: 3.3460822105407715 Perplexity: 28.39128439423958\n",
      "Batch: 9 Loss: 3.290766954421997 Perplexity: 26.863458805639564\n",
      "Batch: 10 Loss: 3.241175413131714 Perplexity: 25.56375206475212\n",
      "Batch: 11 Loss: 3.20162034034729 Perplexity: 24.572313468124165\n",
      "Batch: 12 Loss: 3.1706302165985107 Perplexity: 23.82249295707675\n",
      "Batch: 13 Loss: 3.1407699584960938 Perplexity: 23.121662727463672\n",
      "Batch: 14 Loss: 3.1237664222717285 Perplexity: 22.731836303748754\n",
      "Batch: 15 Loss: 3.1036410331726074 Perplexity: 22.278922077645486\n",
      "Batch: 16 Loss: 3.0890817642211914 Perplexity: 21.95690709868019\n",
      "Batch: 17 Loss: 3.0759589672088623 Perplexity: 21.670653396335528\n",
      "Batch: 18 Loss: 3.066516637802124 Perplexity: 21.466994966636804\n",
      "Batch: 19 Loss: 3.057760238647461 Perplexity: 21.279841979009806\n",
      "Batch: 20 Loss: 3.0511229038238525 Perplexity: 21.139068240824976\n",
      "Batch: 21 Loss: 3.038393497467041 Perplexity: 20.871685871095796\n",
      "Batch: 22 Loss: 3.030656337738037 Perplexity: 20.710821423155245\n",
      "Batch: 23 Loss: 3.0329995155334473 Perplexity: 20.75940746067076\n",
      "Batch: 24 Loss: 3.028259038925171 Perplexity: 20.661230860991846\n",
      "Batch: 25 Loss: 3.023106575012207 Perplexity: 20.555048400141903\n",
      "Batch: 26 Loss: 3.0219991207122803 Perplexity: 20.53229722367534\n",
      "Batch: 27 Loss: 3.0305705070495605 Perplexity: 20.709043875378743\n",
      "Batch: 28 Loss: 3.0211212635040283 Perplexity: 20.514280707676793\n",
      "Batch: 29 Loss: 3.0165910720825195 Perplexity: 20.421557275222963\n",
      "Batch: 30 Loss: 3.017376661300659 Perplexity: 20.437606533672795\n",
      "Batch: 31 Loss: 3.008213520050049 Perplexity: 20.25118924649419\n",
      "Batch: 32 Loss: 3.0122718811035156 Perplexity: 20.33354288157881\n",
      "Batch: 33 Loss: 3.0081045627593994 Perplexity: 20.248982851985026\n",
      "Batch: 34 Loss: 3.0097129344940186 Perplexity: 20.281576948344235\n",
      "Batch: 35 Loss: 3.005117893218994 Perplexity: 20.188596054269997\n",
      "Batch: 36 Loss: 2.99943470954895 Perplexity: 20.0741859695559\n",
      "Batch: 37 Loss: 3.004760980606079 Perplexity: 20.181391775426654\n",
      "Batch: 38 Loss: 2.998612880706787 Perplexity: 20.057695201765632\n",
      "Batch: 39 Loss: 2.994403839111328 Perplexity: 19.973448950715557\n",
      "Batch: 40 Loss: 2.994398832321167 Perplexity: 19.973348948098216\n",
      "Batch: 41 Loss: 2.995612859725952 Perplexity: 19.99761186603017\n",
      "Batch: 42 Loss: 2.98704195022583 Perplexity: 19.826946567683084\n",
      "Batch: 43 Loss: 2.9850122928619385 Perplexity: 19.78674547060559\n",
      "Batch: 44 Loss: 2.985375165939331 Perplexity: 19.79392685071204\n",
      "Batch: 45 Loss: 2.9874534606933594 Perplexity: 19.835107242721268\n",
      "Batch: 46 Loss: 2.9818830490112305 Perplexity: 19.72492469560279\n",
      "Batch: 47 Loss: 2.9789695739746094 Perplexity: 19.667540254562685\n",
      "Batch: 48 Loss: 2.9823057651519775 Perplexity: 19.733264502207888\n",
      "Batch: 49 Loss: 2.972959041595459 Perplexity: 19.549682416037403\n",
      "Batch: 50 Loss: 2.975106716156006 Perplexity: 19.591713890437386\n",
      "Batch: 51 Loss: 2.9670751094818115 Perplexity: 19.434991160529155\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mtarget_char_to_int[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m train_losses, dev_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[96], line 43\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader, dev_loader, num_epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(decoder_outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m46\u001b[39m), target\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     41\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     46\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/DLA3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DLA3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### Training and Testing\n",
    "\n",
    "## Hyperparameters\n",
    "input_size = len(train_vocab)\n",
    "output_size = len(test_vocab)\n",
    "embedding_size = 512\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "batch_first = True\n",
    "teacher_forcing_ratio = 1.0\n",
    "num_epochs = 10\n",
    "\n",
    "encoder = Encoder(input_size, embedding_size, hidden_size, num_layers, dropout, bidirectional, batch_first).to(device)\n",
    "decoder = Decoder(output_size, embedding_size, hidden_size, num_layers, dropout, batch_first).to(device)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder,device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=target_char_to_int['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "train_losses, dev_losses = train(model, criterion, optimizer, train_loader, dev_loader, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Model\n",
    "torch.save(model.state_dict(), './model_e0b1600.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Keys from Model\n",
    "encoder = Encoder(input_size, embedding_size, hidden_size, num_layers, dropout, bidirectional, batch_first).to(device)\n",
    "decoder = Decoder(output_size, embedding_size, hidden_size, num_layers, dropout, batch_first).to(device)\n",
    "model = Seq2Seq(encoder, decoder,device)\n",
    "\n",
    "model.load_state_dict(torch.load('./model.pt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (source, target) in enumerate(test_loader):\n",
    "            source = source.long().to(device)\n",
    "            target = target.long().to(device)\n",
    "\n",
    "            encoder_outputs, encoder_hidden, encoder_cell = model.encoder(source)\n",
    "            decoder_outputs, decoder_hidden, decoder_cell = model.decoder.predict(encoder_hidden, encoder_outputs)\n",
    "\n",
    "            # print(\"decoder outputs shape\",decoder_outputs.squeeze(2).shape)\n",
    "            \n",
    "            # print(\"target shape\",target.shape)\n",
    "\n",
    "            # decoder_outputs shape: (batch_size, seq_length, hidden_size*num_directions)\n",
    "            # target shape: (batch_size, seq_length)\n",
    "            # decoder_outputs = decoder_outputs.view(-1, decoder_outputs.shape[-1])\n",
    "            # target = target.view(-1)\n",
    "\n",
    "            # loss = criterion(decoder_outputs.squeeze(2).view(-1,46), target.view(-1))   # for test dataset\n",
    "            # test_loss += loss.item()\n",
    "\n",
    "    # test_loss /= len(test_loader)\n",
    "    # print(f'Test Loss: {test_loss:.3f}')\n",
    "\n",
    "    return test_loss , decoder_outputs\n",
    "\n",
    "test_loss , _ = test(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_loss\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diversity_penalty(new_sequence, existing_sequences):\n",
    "    \"\"\"\n",
    "    Calculate a diversity penalty based on the new sequence and existing sequences.\n",
    "    This is a simple example of diversity penalty calculation and can be customized.\n",
    "\n",
    "    Args:\n",
    "    - new_sequence (torch.Tensor): The new sequence to be penalized.\n",
    "    - existing_sequences (list of torch.Tensor): A list of existing sequences.\n",
    "\n",
    "    Returns:\n",
    "    - float: The diversity penalty score.\n",
    "    \"\"\"\n",
    "    penalty = 0.0\n",
    "    for seq in existing_sequences:\n",
    "        similarity = torch.sum(torch.eq(new_sequence, seq[0]).float()) / len(new_sequence)\n",
    "        penalty += similarity\n",
    "    return penalty\n",
    "\n",
    "def beam_search_decoder(probabilities, beam_width, max_length, diversity_penalty_weight=0.7):\n",
    "    \"\"\"\n",
    "    Beam search decoder for sequence generation.\n",
    "\n",
    "    Args:\n",
    "    - probabilities (torch.Tensor): A 2D tensor of shape (sequence_length, vocab_size)\n",
    "      containing the predicted probabilities for each token at each time step.\n",
    "    - beam_width (int): The number of sequences to consider at each decoding step.\n",
    "    - max_length (int): The maximum length of the generated sequence.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples, each containing (sequence, score), where:\n",
    "      - sequence (list): A list of token IDs representing the generated sequence.\n",
    "      - score (float): The log-likelihood score of the sequence.\n",
    "    \"\"\"\n",
    "    out  = torch.argmax(nn.Softmax(dim = 1)(probabilities), dim = 1)\n",
    "\n",
    "    # out = out.squeeze(0)\n",
    "    # out = out.squeeze(0)\n",
    "    # print(\"Output shape\",out.shape)\n",
    "    # print(out)\n",
    "    seq_len = 0\n",
    "    for char in out:\n",
    "        if(char == target_char_to_int[\"<EOS>\"]):\n",
    "            break\n",
    "        else:\n",
    "            seq_len += 1\n",
    "\n",
    "    # Get the sequence length and vocabulary size\n",
    "    sequence_length, vocab_size = probabilities.shape\n",
    "    sequence_length = seq_len\n",
    "    max_length = seq_len\n",
    "    print(seq_len)\n",
    "\n",
    "    # Initialize the beam with the empty sequence\n",
    "    beam = [(torch.tensor([], dtype=torch.long).to(device), 0.0)]\n",
    "\n",
    "    # Iterate through each time step\n",
    "    for t in range(max_length):\n",
    "        new_beam = []\n",
    "\n",
    "        # Expand the beam by considering the top 'beam_width' candidates at each step\n",
    "        for sequence, score in beam:\n",
    "            # If the sequence is already at the maximum length, keep it as is\n",
    "            if len(sequence) == max_length:\n",
    "                new_beam.append((sequence, score))\n",
    "                continue\n",
    "\n",
    "            # Get the probabilities for the next token\n",
    "            t_probs = probabilities[t]\n",
    "\n",
    "            # Get the top 'beam_width' token IDs and their corresponding log-likelihood scores\n",
    "            top_scores, top_tokens = torch.topk(t_probs, beam_width)\n",
    "\n",
    "            # Expand the current sequence with each of the top tokens\n",
    "            for token, token_score in zip(top_tokens, top_scores):\n",
    "                new_sequence = torch.cat([sequence, token.unsqueeze(0)], dim=0)\n",
    "                new_score = score + token_score.item()\n",
    "    \n",
    "                # Apply the diversity penalty\n",
    "                if len(new_sequence) > 1:\n",
    "                    # Calculate a penalty based on sequence diversity\n",
    "                    diversity_penalty = diversity_penalty_weight * calculate_diversity_penalty(new_sequence, new_beam)\n",
    "                    new_score -= diversity_penalty\n",
    "                    \n",
    "                new_beam.append((new_sequence, new_score))\n",
    "        print(t)\n",
    "\n",
    "        # Keep the top 'beam_width' candidates\n",
    "        new_beam.sort(key=lambda x: -x[1])\n",
    "        beam = new_beam[:beam_width]\n",
    "\n",
    "    # Return the top sequence and its score\n",
    "    return [(sequence.tolist(), score) for sequence, score in beam]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_char(seq):\n",
    "        vis = \"\"\n",
    "        for char in seq:\n",
    "            char = char\n",
    "            if(char == '<EOS>'):\n",
    "                return vis\n",
    "            vis += target_int_to_char[char]\n",
    "        \n",
    "        return vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Progression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Transform Progression data\n",
    "\n",
    "with open(\"./Data/A3 files/progression.txt\", \"r\") as f:\n",
    "    progression_dev = f.readlines()\n",
    "\n",
    "f.close()\n",
    "\n",
    "transform = {}\n",
    "transform[\"year\"] = \"num0\"\n",
    "transform[\"race\"] = \"str0\"\n",
    "transform[\"Time\"] = \"num1\"\n",
    "transform[\"Distance\"] = \"num2\"\n",
    "\n",
    "reverse_transform = {}\n",
    "reverse_transform[\"num0\"] = \"year\"\n",
    "reverse_transform[\"str0\"] = \"race\"\n",
    "reverse_transform[\"num1\"] = \"Time\"\n",
    "reverse_transform[\"num2\"] = \"Distance\"\n",
    "\n",
    "progression_dev = [line.strip('\\n') for line in progression_dev]\n",
    "\n",
    "data = \"[\" + progression_dev[1] + \"]\"\n",
    "\n",
    "\n",
    "# Create dataset using data\n",
    "\n",
    "data = encode_data_sources(data)\n",
    "\n",
    "data = [[source_char_to_int[\"<SOS>\"]]] + data\n",
    "data.append([source_char_to_int[\"<EOS>\"]])\n",
    "\n",
    "data = data + [[source_char_to_int['<pad>']]] * (500 - len(data))\n",
    "\n",
    "data = torch.Tensor(data).long().reshape(1,-1)\n",
    "\n",
    "progression_dataset = MyDataset(data,torch.Tensor([[0]]))\n",
    "\n",
    "progression_loader = DataLoader(progression_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 500])\n"
     ]
    }
   ],
   "source": [
    "print(progression_loader.dataset.sources.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "predictions , probs = test(model, criterion, progression_loader)\n",
    "\n",
    "\n",
    "# print(\"1\",probs.shape)\n",
    "probs = probs.reshape(500,46)\n",
    "# print(\"2\",probs.shape)\n",
    "# probabilities = probs.squeeze(0)\n",
    "# print(probs.shape)\n",
    "\n",
    "seq_and_score = beam_search_decoder(probs, 15, 500, diversity_penalty_weight=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 45, 21, 44, 12, 21, 24, 18, 9, 12, 25, 0, 31, 45, 44, 45, 43, 45, 26, 31, 23, 4, 28, 23, 23, 43, 24, 25]\n",
      "31\n",
      ",roafolx0fbc<pad>rarir <pad>:{y::ilb\n"
     ]
    }
   ],
   "source": [
    "print(seq_and_score[0][0])\n",
    "\n",
    "print(target_char_to_int[\"<pad>\"])\n",
    "\n",
    "print(convert_to_char(seq_and_score[0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
