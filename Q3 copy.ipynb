{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv \n",
    "import time\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Vocabulary from ./Data/A3 files/train.targets train.sources dev.targets dev.sources test.targets test.sources\n",
    "\n",
    "train_sources_text = open(\"./Data/A3 files/train.sources\", \"r\")\n",
    "train_targets_text = open(\"./Data/A3 files/train.targets\", \"r\")\n",
    "dev_sources_text = open(\"./Data/A3 files/dev.sources\", \"r\")\n",
    "dev_targets_text = open(\"./Data/A3 files/dev.targets\", \"r\")\n",
    "test_sources_text = open(\"./Data/A3 files/test.sources\", \"r\")\n",
    "test_targets_text = open(\"./Data/A3 files/test.targets\", \"r\")\n",
    "\n",
    "train_sources = train_sources_text.readlines()\n",
    "train_targets = train_targets_text.readlines()\n",
    "dev_sources = dev_sources_text.readlines()\n",
    "dev_targets = dev_targets_text.readlines()\n",
    "test_sources = test_sources_text.readlines()\n",
    "test_targets = test_targets_text.readlines()\n",
    "\n",
    "train_vocab = set()\n",
    "test_vocab = set()\n",
    "\n",
    "\n",
    "train_sources_text.close()\n",
    "train_targets_text.close()\n",
    "dev_sources_text.close()\n",
    "dev_targets_text.close()\n",
    "test_sources_text.close()\n",
    "test_targets_text.close()\n",
    "\n",
    "train_sources_list = []\n",
    "train_targets_list = []\n",
    "dev_sources_list = []\n",
    "dev_targets_list = []\n",
    "test_sources_list = []\n",
    "test_targets_list = []\n",
    "\n",
    "for line in train_sources:\n",
    "    # Get individual charecters from line, add unique charecters to vocab\n",
    "    train_vocab.update(set(line))\n",
    "    # Add line to list after stripping \\n\n",
    "    train_sources_list.append(line.strip('\\n'))\n",
    "\n",
    "for line in train_targets:\n",
    "    # Get individual charecters from line, add unique charecters to vocab\n",
    "    test_vocab.update(set(line))\n",
    "    # Add line to list after stripping \\n\n",
    "    train_targets_list.append(line.strip('\\n'))\n",
    "\n",
    "for line in dev_sources:\n",
    "    # Get individual charecters from line, add unique charecters to vocab\n",
    "    # vocab.update(set(line))\n",
    "    # Add line to list after stripping \\n\n",
    "    dev_sources_list.append(line.strip('\\n'))\n",
    "\n",
    "for line in dev_targets:\n",
    "    # Get individual charecters from line, add unique charecters to vocab\n",
    "    # vocab.update(set(line))\n",
    "    # Add line to list after stripping \\n\n",
    "    dev_targets_list.append(line.strip('\\n'))\n",
    "\n",
    "for line in test_sources:\n",
    "    # Get individual charecters from line, add unique charecters to vocab\n",
    "    # vocab.update(set(line))\n",
    "    # Add line to list after stripping \\n\n",
    "    test_sources_list.append(line.strip('\\n'))\n",
    "\n",
    "for line in test_targets:\n",
    "    # Get individual charecters from line, add unique charecters to vocab\n",
    "    # vocab.update(set(line))\n",
    "    # Add line to list after stripping \\n\n",
    "    test_targets_list.append(line.strip('\\n'))\n",
    "\n",
    "\n",
    "# Add <pad> and <SOS> , <EOS> to vocab\n",
    "train_vocab.add('<pad>')\n",
    "train_vocab.add('<SOS>')\n",
    "train_vocab.add('<EOS>')\n",
    "\n",
    "# Remove \\n from vocab\n",
    "train_vocab.remove('\\n')\n",
    "\n",
    "# Add <pad> and <SOS> , <EOS> to vocab\n",
    "test_vocab.add('<pad>')\n",
    "test_vocab.add('<SOS>')\n",
    "test_vocab.add('<EOS>')\n",
    "\n",
    "# Remove \\n from vocab\n",
    "test_vocab.remove('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionary from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<SOS>': 0, 'o': 1, '3': 2, 'm': 3, 'q': 4, '\"': 5, '6': 6, 'p': 7, '4': 8, '0': 9, 'n': 10, ' ': 11, 'f': 12, '<pad>': 13, 'x': 14, 's': 15, 'k': 16, 'h': 17, '2': 18, 'T': 19, ',': 20, 'e': 21, '}': 22, '<EOS>': 23, '8': 24, '7': 25, 'g': 26, 'c': 27, 'a': 28, '5': 29, 'u': 30, '{': 31, '*': 32, ':': 33, 'b': 34, 'z': 35, 'd': 36, 't': 37, '9': 38, 'v': 39, 'i': 40, 'l': 41, 'r': 42, '1': 43, 'y': 44, 'U': 45}\n",
      "46\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "## create vocab to index and index to vocab dictionaries\n",
    "source_char_to_int = {}\n",
    "source_int_to_char = {}\n",
    "\n",
    "for i, word in enumerate(train_vocab):\n",
    "    source_char_to_int[word] = i\n",
    "    source_int_to_char[i] = word\n",
    "\n",
    "# print(source_char_to_int)\n",
    "\n",
    "target_char_to_int = {}\n",
    "target_int_to_char = {}\n",
    "\n",
    "for i, word in enumerate(test_vocab):\n",
    "    target_char_to_int[word] = i\n",
    "    target_int_to_char[i] = word\n",
    "\n",
    "print(target_char_to_int)\n",
    "\n",
    "print(len(target_char_to_int))\n",
    "print(len(source_char_to_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by Charecters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "def encode_data_sources(data):\n",
    "    encoded_data = []\n",
    "    for i in range(len(data)):                  # appending 0 for <SOS> token \n",
    "        encoded_data.append([source_char_to_int[char] for char in data[i]])\n",
    "    \n",
    "    # encoded_data = [[[0]]] + encoded_data         # appending 0 for <SOS> token\n",
    "    return encoded_data\n",
    "\n",
    "def encode_data_targets(data):\n",
    "    encoded_data = []\n",
    "    for i in range(len(data)):                  # appending 0 for <SOS> token \n",
    "        encoded_data.append([target_char_to_int[char] for char in data[i]])\n",
    "    \n",
    "    # encoded_data = [[[0]]] + encoded_data         # appending 0 for <SOS> token\n",
    "    return encoded_data\n",
    "\n",
    "def decode_data_sources(data):\n",
    "    decoded_data = []\n",
    "    for i in range(len(data)+1):\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        decoded_data.append([source_int_to_char[int] for int in data[i]])\n",
    "    return decoded_data\n",
    "\n",
    "def decode_data_targets(data):\n",
    "    decoded_data = []\n",
    "    for i in range(len(data)+1):\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        decoded_data.append([target_int_to_char[int] for int in data[i]])\n",
    "    return decoded_data\n",
    "\n",
    "\n",
    "train_sources_encoded = encode_data_sources(train_sources_list)\n",
    "train_targets_encoded = encode_data_targets(train_targets_list)\n",
    "dev_sources_encoded = encode_data_sources(dev_sources_list)\n",
    "dev_targets_encoded = encode_data_targets(dev_targets_list)\n",
    "test_sources_encoded = encode_data_sources(test_sources_list)\n",
    "test_targets_encoded = encode_data_targets(test_targets_list)\n",
    "\n",
    "### For every sequence in train_sources_encoded, train_targets_encoded, dev_sources_encoded, dev_targets_encoded, \n",
    "# test_sources_encoded, test_targets_encoded\n",
    "#  add <SOS> token at start of sequence \n",
    "# add <EOS> token at the end of the sequence and <pad> \n",
    "# tokens to make the sequence length equal to the maximum sequence length in the dataset which is 500\n",
    "\n",
    "max_len = 500\n",
    "\n",
    "for i in range(len(train_sources_encoded)):\n",
    "    train_sources_encoded[i] = [source_char_to_int['<SOS>']] + train_sources_encoded[i] \n",
    "    train_targets_encoded[i].append(target_char_to_int['<EOS>'])\n",
    "\n",
    "    if len(train_sources_encoded[i]) > max_len:\n",
    "        train_sources_encoded[i] = train_sources_encoded[i][:max_len]       # Truncating the sequence to max_len\n",
    "        train_targets_encoded[i] = train_targets_encoded[i][:max_len]       # Truncating the sequence to max_len\n",
    "    else:\n",
    "        train_sources_encoded[i] = train_sources_encoded[i] + [source_char_to_int['<pad>']] * (max_len - len(train_sources_encoded[i]))\n",
    "        train_targets_encoded[i] = train_targets_encoded[i] + [target_char_to_int['<pad>']] * (max_len - len(train_targets_encoded[i]))\n",
    "\n",
    "for i in range(len(dev_sources_encoded)):\n",
    "    dev_sources_encoded[i] = [source_char_to_int['<SOS>']] + dev_sources_encoded[i] \n",
    "    dev_targets_encoded[i].append(target_char_to_int['<EOS>'])\n",
    "    \n",
    "    if len(dev_sources_encoded[i]) > max_len:\n",
    "        dev_sources_encoded[i] = dev_sources_encoded[i][:max_len]       # Truncating the sequence to max_len\n",
    "        dev_targets_encoded[i] = dev_targets_encoded[i][:max_len]       # Truncating the sequence to max_len\n",
    "    else:\n",
    "        dev_sources_encoded[i] = dev_sources_encoded[i] + [source_char_to_int['<pad>']] * (max_len - len(dev_sources_encoded[i]))\n",
    "        dev_targets_encoded[i] = dev_targets_encoded[i] + [target_char_to_int['<pad>']] * (max_len - len(dev_targets_encoded[i]))\n",
    "\n",
    "for i in range(len(test_sources_encoded)):\n",
    "    test_sources_encoded[i] = [source_char_to_int['<SOS>']] + test_sources_encoded[i] \n",
    "    test_targets_encoded[i].append(target_char_to_int['<EOS>'])\n",
    "    \n",
    "    if len(test_sources_encoded[i]) > max_len:\n",
    "        test_sources_encoded[i] = test_sources_encoded[i][:max_len]       # Truncating the sequence to max_len\n",
    "        test_targets_encoded[i] = test_targets_encoded[i][:max_len]       # Truncating the sequence to max_len\n",
    "    else:\n",
    "        test_sources_encoded[i] = test_sources_encoded[i] + [source_char_to_int['<pad>']] * (max_len - len(test_sources_encoded[i]))\n",
    "        test_targets_encoded[i] = test_targets_encoded[i] + [target_char_to_int['<pad>']] * (max_len - len(test_targets_encoded[i]))\n",
    "\n",
    "print(len(train_sources_encoded[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([172719, 500])\n",
      "torch.Size([172719, 500])\n",
      "torch.Size([21590, 500])\n",
      "torch.Size([21590, 500])\n",
      "torch.Size([21590, 500])\n",
      "torch.Size([21590, 500])\n"
     ]
    }
   ],
   "source": [
    "train_sources_tensor = torch.Tensor(train_sources_encoded)\n",
    "train_targets_tensor = torch.Tensor(train_targets_encoded)\n",
    "dev_sources_tensor = torch.Tensor(dev_sources_encoded)\n",
    "dev_targets_tensor = torch.Tensor(dev_targets_encoded)\n",
    "test_sources_tensor = torch.Tensor(test_sources_encoded)\n",
    "test_targets_tensor = torch.Tensor(test_targets_encoded)\n",
    "\n",
    "\n",
    "print(train_sources_tensor.shape)\n",
    "print(train_targets_tensor.shape)\n",
    "print(dev_sources_tensor.shape)\n",
    "print(dev_targets_tensor.shape)\n",
    "print(test_sources_tensor.shape)\n",
    "print(test_targets_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, sources, targets):\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.sources[idx]\n",
    "        target = self.targets[idx]\n",
    "        return source, target\n",
    "    \n",
    "train_dataset = MyDataset(train_sources_tensor, train_targets_tensor)\n",
    "dev_dataset = MyDataset(dev_sources_tensor, dev_targets_tensor)\n",
    "test_dataset = MyDataset(test_sources_tensor, test_targets_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Seq2Seq Model with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seq2Seq with Attention\n",
    "\n",
    "## Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=512, hidden_size=512, num_layers=2, dropout=0.5,bidirectional=True,batch_first=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=batch_first)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embedding = self.dropout(self.embedding(x))                     # As per assignment, dropout is applied to embedding and not to inputs of hidden layer\n",
    "\n",
    "        # embedding shape: (batch_size, seq_length, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(embedding)\n",
    "\n",
    "        # print(\"Outputs (Enc)\",outputs.shape)\n",
    "\n",
    "        # outputs shape: (N, 500, 1024)\n",
    "        # hidden shape: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # cell shape: (num_layers*num_directions, batch_size, hidden_size)\n",
    "\n",
    "        # concatenate hiden states of last layer of bidrectional LSTM\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1).unsqueeze(0).expand(self.num_layers, -1, -1).contiguous()\n",
    "        # hidden shape: (2,N,1024)\n",
    "\n",
    "        return outputs, hidden, cell\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # self.linear = nn.Linear()\n",
    "\n",
    "    \n",
    "    def forward(self,decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden shape: (2, N, 1024)\n",
    "        # encoder_outputs shape: (N, 500, 1024)\n",
    "\n",
    "        # attention shape: (N,500)\n",
    "        hidden_last = decoder_hidden[-1,:,:].unsqueeze(0)\n",
    "        # print(\"Hidden last\", hidden_last.shape)\n",
    "                \n",
    "\n",
    "        # print(encoder_outputs.shape)\n",
    "        attention = torch.matmul(hidden_last.permute(1,0,2),encoder_outputs.permute(0,2,1))\n",
    "\n",
    "        attention = self.softmax(attention)\n",
    "\n",
    "        context = torch.matmul(attention,encoder_outputs)\n",
    "\n",
    "        # context shape: (N,1,1024)\n",
    "\n",
    "        return context, attention\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size=512, hidden_size=512, num_layers=2, dropout=0.5, batch_first=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size+hidden_size*2, hidden_size*2, num_layers, dropout=dropout, batch_first=batch_first)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        self.attention = Attention()\n",
    "    \n",
    "    def forward(self, x, hidden, encoder_outputs,teacher_forcing_ratio=1.0):\n",
    "        ### x : (N,500)\n",
    "        ### hidden : (2,N,1024)\n",
    "        ### encoder_outputs : (N,500,1024)\n",
    "        ### teacher_forcing_ratio : float\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        max_len = x.shape[1]\n",
    "        # print(\"Encoder outputs (Dec)\",encoder_outputs.shape)\n",
    "        vocab_size = self.output_size\n",
    "\n",
    "        target_embedding = self.dropout(self.embedding(x))\n",
    "        # target_embedding shape: (N,500,512)\n",
    "\n",
    "        initial_hidden = torch.randn(self.num_layers, batch_size, self.hidden_size*2).to(device)    # (2,N,1024)\n",
    "        initial_cell = torch.randn(self.num_layers, batch_size, self.hidden_size*2).to(device)      # (2,N,1024)\n",
    "\n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        cell_states = []\n",
    "        query = [initial_hidden]            # (2,N,1024)\n",
    "\n",
    "        for timestep in range(max_len):\n",
    "\n",
    "            if (timestep == 0):\n",
    "                context , attention = self.attention(query[-1],encoder_outputs)\n",
    "\n",
    "                if np.random.random() < teacher_forcing_ratio:\n",
    "                    input = target_embedding[:,timestep,:]      # (N,512)\n",
    "                    input = input.unsqueeze(1)          # (N,1,512)\n",
    "                else:\n",
    "                    input = torch.tensor([target_char_to_int['<SOS>']]*batch_size).unsqueeze(1).to(device)    # (N,1)\n",
    "                    input = self.embedding(input)       # (N,1,512)\n",
    "\n",
    "                    \n",
    "                \n",
    "                # print(\"Input shape\",input.shape)\n",
    "                # print(\"Context shape\",context.shape)\n",
    "                \n",
    "                input = torch.cat((input,context),dim=2)\n",
    "                # input shape: (N,1,1536)\n",
    "                # print(\"Input shape\",input.shape)\n",
    "                output, (hidden, cell) = self.lstm(input, (initial_hidden, initial_cell))    # (N,1,1024)\n",
    "                # print(\"Output shape\",output.shape)\n",
    "                # print(\"Hidden shape\",hidden.shape)\n",
    "                # print(\"Cell shape\",cell.shape)\n",
    "                # output shape: (N,1,1024)\n",
    "                # hidden shape: (2,N,1024)\n",
    "                # cell shape: (2,N,1024)\n",
    "            else:\n",
    "\n",
    "                context , attention = self.attention(query[-1],encoder_outputs)\n",
    "\n",
    "                if np.random.random() < teacher_forcing_ratio:\n",
    "                    input = target_embedding[:,timestep,:]\n",
    "                    input = input.unsqueeze(1)\n",
    "\n",
    "                else:\n",
    "                    # print(\"output\",output[-1].shape)\n",
    "                    input = output[-1]\n",
    "                    \n",
    "                    # input shape: (N,1,512)\n",
    "                \n",
    "                                \n",
    "                input = torch.cat((input,context),dim=2)\n",
    "                # input shape: (N,1,1536)\n",
    "\n",
    "                # print(\"Input shape\",input.shape)\n",
    "\n",
    "                output, (hidden, cell) = self.lstm(input, (hidden, cell))    # (N,1,1024)\n",
    "                # output shape: (N,1,1024)\n",
    "                # hidden shape: (2,N,1024)\n",
    "                # cell shape: (2,N,1024)\n",
    "            \n",
    "            # print(\"Output shape\",output.shape)\n",
    "            output = self.fc(output.squeeze(1))\n",
    "            # print(\"Output shape\",output.shape)\n",
    "            # output shape: (N, 46)\n",
    "            # output = F.softmax(output, dim=1)\n",
    "            # output = output.detach()\n",
    "            # hidden = hidden.detach()\n",
    "            # cell = cell.detach()    # Detach hidden and cell states to prevent backpropagation through time\n",
    "            # output shape: (N, 46)\n",
    "            outputs.append(output)\n",
    "            hidden_states.append(hidden)\n",
    "            cell_states.append(cell)\n",
    "            query.append(hidden)\n",
    "\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        hidden_states = torch.stack(hidden_states, dim=1)\n",
    "        cell_states = torch.stack(cell_states, dim=1)\n",
    "        \n",
    "        return outputs, hidden_states, cell_states\n",
    "\n",
    "    # def predict(self,hidden,encoder_outputs):\n",
    "    #     ### x : (N,500)\n",
    "    #     ### hidden : (2,N,1024)\n",
    "    #     ### encoder_outputs : (N,500,1024)\n",
    "\n",
    "    #     batch_size = encoder_outputs.shape[0]\n",
    "    #     max_len = encoder_outputs.shape[1]\n",
    "    #     # print(\"Encoder outputs (Dec)\",encoder_outputs.shape)\n",
    "    #     vocab_size = self.output_size\n",
    "\n",
    "    #     # target_embedding = self.dropout(self.embedding(x))\n",
    "    #     # target_embedding shape: (N,500,512)\n",
    "    #     initial_hidden = torch.randn(self.num_layers, batch_size, self.hidden_size*2).to(device)    # (2,N,1024)\n",
    "    #     initial_cell = torch.randn(self.num_layers, batch_size, self.hidden_size*2).to(device)      # (2,N,1024)\n",
    "\n",
    "    #     outputs = []\n",
    "    #     hidden_states = []\n",
    "    #     cell_states = []\n",
    "    #     query = [initial_hidden]            # (2,N,1024)\n",
    "\n",
    "\n",
    "    #     # input = []\n",
    "\n",
    "    #     input = torch.tensor([target_char_to_int['<SOS>']]*batch_size).unsqueeze(1).to(device)    # (N,1)\n",
    "\n",
    "    #     for timestep in range(max_len):\n",
    "\n",
    "    #         # output, hidden , cell = self.forward(input,hidden,encoder_outputs,teacher_forcing_ratio=1.0)\n",
    "    #         target_embedding = self.dropout(self.embedding(input))\n",
    "\n",
    "    #         if (timestep == 0):\n",
    "    #             context , attention = self.attention(initial_hidden,encoder_outputs)\n",
    "    #             x = target_embedding[:,timestep,:]      # (N,512)\n",
    "    #             x = x.unsqueeze(1)          # (N,1,512)\n",
    "    #             # print(\"Input shape\",input.shape)\n",
    "    #             # print(\"Context shape\",context.shape)\n",
    "                \n",
    "    #             x = torch.cat((x,context),dim=2)\n",
    "    #             # input shape: (N,1,1536)\n",
    "    #             # print(\"Input shape\",input.shape)\n",
    "    #             output, (dec_hidden, cell) = self.lstm(x, (initial_hidden, initial_cell))    # (N,1,1024)\n",
    "    #             # print(\"Output shape\",output.shape)\n",
    "    #             # print(\"Hidden shape\",hidden.shape)\n",
    "    #             # print(\"Cell shape\",cell.shape)\n",
    "    #             # output shape: (N,1,1024)\n",
    "    #             # hidden shape: (2,N,1024)\n",
    "    #             # cell shape: (2,N,1024)\n",
    "    #         else:\n",
    "\n",
    "    #             context , attention = self.attention(query[-1],encoder_outputs)\n",
    "\n",
    "                \n",
    "    #             x = target_embedding[:,timestep,:]\n",
    "    #             x = x.unsqueeze(1)\n",
    "\n",
    "    #             x = torch.cat((x,context),dim=2)\n",
    "    #             # input shape: (N,1,1536)\n",
    "\n",
    "    #             # print(\"Input shape\",input.shape)\n",
    "\n",
    "    #             output, (dec_hidden, cell) = self.lstm(x, (dec_hidden, cell))    # (N,1,1024)\n",
    "\n",
    "\n",
    "    #         # print(\"Output shape\",output.shape)\n",
    "    #         output = F.softmax(output, dim=-1)\n",
    "    #         # Output : (N,46)\n",
    "    #         output = torch.argmax(output,dim=-1)\n",
    "    #         # print(\"Outputs\",outputs.shape)\n",
    "\n",
    "    #         # Concatenate x with output\n",
    "    #         print(\"Output shape\",output.shape)\n",
    "    #         print(\"Input shape\",input.shape)\n",
    "\n",
    "    #         input = torch.cat((input,output),dim=1)\n",
    "\n",
    "    #         # output = self.fc(output.squeeze(1))\n",
    "    #         # output shape: (N, 46)\n",
    "    #         # output = F.softmax(output, dim=1)\n",
    "    #         # output shape: (N, 46)\n",
    "    #         output = output.detach()\n",
    "    #         outputs.append(output)\n",
    "    #         hidden_states.append(hidden)\n",
    "    #         cell_states.append(cell)\n",
    "    #         query.append(dec_hidden)\n",
    "        \n",
    "    #     outputs = torch.stack(outputs, dim=1)\n",
    "    #     hidden_states = torch.stack(hidden_states, dim=1)\n",
    "    #     cell_states = torch.stack(cell_states, dim=1)\n",
    "\n",
    "    #     return outputs, hidden_states, cell_states\n",
    "\n",
    "    def predict(self,hidden,encoder_outputs):\n",
    "\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        \n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        cell_states = []\n",
    "        query = [hidden]            # (2,N,1024)\n",
    "\n",
    "        input = torch.tensor([target_char_to_int['<SOS>']]*batch_size).unsqueeze(1).to(device)    # (N,1)\n",
    "\n",
    "        for timestep in range(500):\n",
    "            \n",
    "            output, hidden , cell = self.forward(input,hidden,encoder_outputs,teacher_forcing_ratio=1.0)            # (N,2)\n",
    "\n",
    "            output = F.softmax(output, dim=-1)\n",
    "\n",
    "            outputs.append(output)\n",
    "            \n",
    "            output = torch.argmax(output,dim=-1)\n",
    "\n",
    "            # Concatenate x with output\n",
    "            # print(\"Output shape\",output.shape)\n",
    "            input = output\n",
    "            # print(\"Input shape\",input.shape)\n",
    "\n",
    "            # output = output.detach()\n",
    "            hidden_states.append(hidden)\n",
    "            cell_states.append(cell)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        # print(\"Final outputs\",outputs.shape)\n",
    "        hidden_states = torch.stack(hidden_states, dim=1)\n",
    "        cell_states = torch.stack(cell_states, dim=1)\n",
    "\n",
    "        return outputs, hidden_states, cell_states\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing=1.0):\n",
    "        # source shape: (batch_size, seq_length)\n",
    "        # target shape: (batch_size, seq_length)\n",
    "\n",
    "        batch_size = source.shape[0]\n",
    "        seq_length = source.shape[1]\n",
    "\n",
    "        encoder_outputs, encoder_hidden, encoder_cell = self.encoder(source)\n",
    "        \n",
    "        # encoder_outputs shape: (batch_size, seq_length, hidden_size*num_directions)\n",
    "        # encoder_hidden shape: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # encoder_cell shape: (num_layers*num_directions, batch_size, hidden_size)\n",
    "\n",
    "        decoder_outputs, decoder_hidden, attentions = self.decoder(target,encoder_hidden,encoder_outputs,teacher_forcing)\n",
    "        # decoder_outputs shape: (batch_size, seq_length, hidden_size*num_directions*2)\n",
    "        # decoder_hidden shape: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # attentions shape: (batch_size, seq_length)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "    \n",
    "    # def predict(self, source, target, teacher_forcing=1.0):\n",
    "    #     # source shape: (batch_size, seq_length)\n",
    "    #     # target shape: (batch_size, seq_length)\n",
    "\n",
    "    #     batch_size = source.shape[0]\n",
    "    #     seq_length = source.shape[1]\n",
    "\n",
    "    #     encoder_outputs, encoder_hidden, encoder_cell = self.encoder(source)\n",
    "        \n",
    "    #     # encoder_outputs shape: (batch_size, seq_length, hidden_size*num_directions)\n",
    "    #     # encoder_hidden shape: (num_layers*num_directions, batch_size, hidden_size)\n",
    "    #     # encoder_cell shape: (num_layers*num_directions, batch_size, hidden_size)\n",
    "\n",
    "    #     decoder_outputs, decoder_hidden, attentions = self.decoder(target,encoder_hidden,encoder_outputs,teacher_forcing)\n",
    "    #     # decoder_outputs shape: (batch_size, seq_length, hidden_size*num_directions*2)\n",
    "    #     # decoder_hidden shape: (num_layers*num_directions, batch_size, hidden_size)\n",
    "    #     # attentions shape: (batch_size, seq_length)\n",
    "        \n",
    "\n",
    "    #     return decoder_outputs, decoder_hidden, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test,Train, Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train function\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time/60)\n",
    "    elapsed_secs = int(elapsed_time - elapsed_mins*60)\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, dev_loader, num_epochs):\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "\n",
    "    steps = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, (source, target) in enumerate(train_loader):\n",
    "            source = source.long().to(device)\n",
    "            target = target.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            encoder_outputs, encoder_hidden, encoder_cell = model.encoder(source)\n",
    "            # print(\"Encoder Outputs:\",encoder_outputs.shape)\n",
    "            # print(\"Encoder Hidden:\",encoder_hidden.shape)\n",
    "            # print(\"Encoder Cell:\",encoder_cell.shape)\n",
    "            \n",
    "            decoder_outputs, decoder_hidden, attention_weights = model.decoder(target,encoder_hidden, encoder_outputs,1.0)\n",
    "\n",
    "            # decoder_outputs shape: (batch_size, seq_length, hidden_size*num_directions*2)\n",
    "            # target shape: (batch_size, seq_length)\n",
    "            # decoder_outputs = decoder_outputs.view(-1, decoder_outputs.shape[-1])\n",
    "            # target = target.view(-1)\n",
    "            # print(\"Decoder Outputs:\",decoder_outputs.shape)\n",
    "            # print(\"Target:\",target.shape)\n",
    "\n",
    "            loss = criterion(decoder_outputs.view(-1,46), target.view(-1))\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            perplexity = np.exp(loss.item())\n",
    "\n",
    "            steps += 1\n",
    "            print(\"Batch:\",steps+1,\"Loss:\",loss.item(),\"Perplexity:\",perplexity)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        dev_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (source, target) in enumerate(dev_loader):\n",
    "                source = source.long().to(device)\n",
    "                target = target.long().to(device)\n",
    "\n",
    "                encoder_outputs, encoder_hidden, encoder_cell = model.encoder(source)\n",
    "                decoder_outputs, decoder_hidden, decoder_cell = model.decoder(target,encoder_hidden, encoder_outputs,1.0)\n",
    "\n",
    "                # decoder_outputs shape: (batch_size, seq_length, hidden_size*num_directions)\n",
    "                # target shape: (batch_size, seq_length)\n",
    "                # decoder_outputs = decoder_outputs.view(-1, decoder_outputs.shape[-1])\n",
    "                # target = target.view(-1)\n",
    "\n",
    "                decoder_outputs = decoder_outputs.view(-1,46)\n",
    "                target = target.view(-1)\n",
    "                \n",
    "                loss = criterion(decoder_outputs, target)\n",
    "                dev_loss += loss.item()\n",
    "\n",
    "        dev_loss /= len(dev_loader)\n",
    "        dev_losses.append(dev_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s',f'\\tTrain Loss: {train_loss}',f'\\t Train PPl. {np.exp(train_loss)}',f'\\t Val. Loss: {dev_loss}' , f'\\t Val. PPL: {np.exp(dev_loss)}')\n",
    "    \n",
    "        torch.save(model.state_dict(),\"model_{epoch}.pt\")\n",
    "\n",
    "\n",
    "    return train_losses, dev_losses\n",
    "\n",
    "\n",
    "### Test function\n",
    "def test(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (source, target) in enumerate(test_loader):\n",
    "            source = source.long().to(device)\n",
    "            target = target.long().to(device)\n",
    "\n",
    "            encoder_outputs, encoder_hidden, encoder_cell = model.encoder(source)\n",
    "            decoder_outputs, decoder_hidden, decoder_cell = model.decoder.predict(encoder_hidden, encoder_outputs)\n",
    "\n",
    "            # print(\"decoder outputs shape\",decoder_outputs.squeeze(2).shape)\n",
    "            # print(\"target shape\",target.shape)\n",
    "\n",
    "            # decoder_outputs shape: (batch_size, seq_length, hidden_size*num_directions)\n",
    "            # target shape: (batch_size, seq_length)\n",
    "            # decoder_outputs = decoder_outputs.view(-1, decoder_outputs.shape[-1])\n",
    "            # target = target.view(-1)\n",
    "\n",
    "            # loss = criterion(decoder_outputs.view(-1,46), target.view(-1))\n",
    "            # test_loss += loss.item()\n",
    "\n",
    "    # test_loss /= len(test_loader)\n",
    "    # print(f'Test Loss: {test_loss:.3f}')\n",
    "\n",
    "    return 0 , decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Training and Testing\n",
    "\n",
    "## Hyperparameters\n",
    "input_size = len(train_vocab)\n",
    "output_size = len(test_vocab)\n",
    "embedding_size = 512\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "batch_first = True\n",
    "teacher_forcing_ratio = 1.0\n",
    "num_epochs = 5\n",
    "\n",
    "encoder = Encoder(input_size, embedding_size, hidden_size, num_layers, dropout, bidirectional, batch_first).to(device)\n",
    "decoder = Decoder(output_size, embedding_size, hidden_size, num_layers, dropout, batch_first).to(device)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder,device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "# train_losses, dev_losses = train(model, criterion, optimizer, train_loader, dev_loader, num_epochs)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_e0b1600.pt\"))\n",
    "\n",
    "# test_loss = test(model, criterion, test_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Training and Testing\n",
    "\n",
    "# ## Hyperparameters\n",
    "# input_size = len(train_vocab)\n",
    "# output_size = len(test_vocab)\n",
    "# embedding_size = 512\n",
    "# hidden_size = 512\n",
    "# num_layers = 2\n",
    "# dropout = 0.5\n",
    "# bidirectional = True\n",
    "# batch_first = True\n",
    "# teacher_forcing_ratio = 1.0\n",
    "# num_epochs = 10\n",
    "\n",
    "# Encoder = Encoder(input_size, embedding_size, hidden_size, num_layers, dropout, bidirectional, batch_first).to(device)\n",
    "# Decoder = Decoder(output_size, embedding_size, hidden_size, num_layers, dropout, batch_first).to(device)\n",
    "\n",
    "# Model = Seq2Seq(Encoder, Decoder,device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(Model.parameters(),lr=1e-4)\n",
    "\n",
    "# Model.load_state_dict(torch.load('./model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss , _ = test(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diversity_penalty(new_sequence, existing_sequences):\n",
    "    \"\"\"\n",
    "    Calculate a diversity penalty based on the new sequence and existing sequences.\n",
    "    This is a simple example of diversity penalty calculation and can be customized.\n",
    "\n",
    "    Args:\n",
    "    - new_sequence (torch.Tensor): The new sequence to be penalized.\n",
    "    - existing_sequences (list of torch.Tensor): A list of existing sequences.\n",
    "\n",
    "    Returns:\n",
    "    - float: The diversity penalty score.\n",
    "    \"\"\"\n",
    "    penalty = 0.0\n",
    "    for seq in existing_sequences:\n",
    "        similarity = torch.sum(torch.eq(new_sequence, seq[0]).float()) / len(new_sequence)\n",
    "        penalty += similarity\n",
    "    return penalty\n",
    "\n",
    "def beam_search_decoder(probabilities, beam_width, max_length, diversity_penalty_weight=0.7):\n",
    "    \"\"\"\n",
    "    Beam search decoder for sequence generation.\n",
    "\n",
    "    Args:\n",
    "    - probabilities (torch.Tensor): A 2D tensor of shape (sequence_length, vocab_size)\n",
    "      containing the predicted probabilities for each token at each time step.\n",
    "    - beam_width (int): The number of sequences to consider at each decoding step.\n",
    "    - max_length (int): The maximum length of the generated sequence.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples, each containing (sequence, score), where:\n",
    "      - sequence (list): A list of token IDs representing the generated sequence.\n",
    "      - score (float): The log-likelihood score of the sequence.\n",
    "    \"\"\"\n",
    "    out  = torch.argmax(nn.Softmax(dim = 1)(probabilities), dim = 1)\n",
    "\n",
    "    # out = out.squeeze(0)\n",
    "    # out = out.squeeze(0)\n",
    "    # print(\"Output shape\",out.shape)\n",
    "    # print(out)\n",
    "    seq_len = 0\n",
    "    for char in out:\n",
    "        if(char == target_char_to_int[\"<EOS>\"]):\n",
    "            break\n",
    "        else:\n",
    "            seq_len += 1\n",
    "\n",
    "    # Get the sequence length and vocabulary size\n",
    "    sequence_length, vocab_size = probabilities.shape\n",
    "    sequence_length = seq_len\n",
    "    max_length = seq_len\n",
    "    print(seq_len)\n",
    "\n",
    "    # Initialize the beam with the empty sequence\n",
    "    beam = [(torch.tensor([], dtype=torch.long).to(device), 0.0)]\n",
    "\n",
    "    # Iterate through each time step\n",
    "    for t in range(max_length):\n",
    "        new_beam = []\n",
    "\n",
    "        # Expand the beam by considering the top 'beam_width' candidates at each step\n",
    "        for sequence, score in beam:\n",
    "            # If the sequence is already at the maximum length, keep it as is\n",
    "            if len(sequence) == max_length:\n",
    "                new_beam.append((sequence, score))\n",
    "                continue\n",
    "\n",
    "            # Get the probabilities for the next token\n",
    "            t_probs = probabilities[t]\n",
    "\n",
    "            # Get the top 'beam_width' token IDs and their corresponding log-likelihood scores\n",
    "            top_scores, top_tokens = torch.topk(t_probs, beam_width)\n",
    "\n",
    "            # Expand the current sequence with each of the top tokens\n",
    "            for token, token_score in zip(top_tokens, top_scores):\n",
    "                new_sequence = torch.cat([sequence, token.unsqueeze(0)], dim=0)\n",
    "                new_score = score + token_score.item()\n",
    "    \n",
    "                # Apply the diversity penalty\n",
    "                if len(new_sequence) > 1:\n",
    "                    # Calculate a penalty based on sequence diversity\n",
    "                    diversity_penalty = diversity_penalty_weight * calculate_diversity_penalty(new_sequence, new_beam)\n",
    "                    new_score -= diversity_penalty\n",
    "                    \n",
    "                new_beam.append((new_sequence, new_score))\n",
    "        print(t)\n",
    "\n",
    "        # Keep the top 'beam_width' candidates\n",
    "        new_beam.sort(key=lambda x: -x[1])\n",
    "        beam = new_beam[:beam_width]\n",
    "\n",
    "    # Return the top sequence and its score\n",
    "    return [(sequence.tolist(), score) for sequence, score in beam]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_char(seq):\n",
    "        vis = \"\"\n",
    "        for char in seq:\n",
    "            char = char\n",
    "            if(char == '<EOS>'):\n",
    "                return vis\n",
    "            vis += target_int_to_char[char]\n",
    "        \n",
    "        return vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Progression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Transform Progression data\n",
    "\n",
    "with open(\"./Data/A3 files/progression.txt\", \"r\") as f:\n",
    "    progression_dev = f.readlines()\n",
    "\n",
    "f.close()\n",
    "\n",
    "transform = {}\n",
    "transform[\"year\"] = \"num0\"\n",
    "transform[\"race\"] = \"str0\"\n",
    "transform[\"Time\"] = \"num1\"\n",
    "transform[\"Distance\"] = \"num2\"\n",
    "\n",
    "reverse_transform = {}\n",
    "reverse_transform[\"num0\"] = \"year\"\n",
    "reverse_transform[\"str0\"] = \"race\"\n",
    "reverse_transform[\"num1\"] = \"Time\"\n",
    "reverse_transform[\"num2\"] = \"Distance\"\n",
    "\n",
    "progression_dev = [line.strip('\\n') for line in progression_dev]\n",
    "\n",
    "data = \"[\" + progression_dev[1] + \"]\"\n",
    "\n",
    "\n",
    "# Create dataset using data\n",
    "\n",
    "data = encode_data_sources(data)\n",
    "\n",
    "data = [[source_char_to_int[\"<SOS>\"]]] + data\n",
    "data.append([source_char_to_int[\"<EOS>\"]])\n",
    "\n",
    "data = data + [[source_char_to_int['<pad>']]] * (500 - len(data))\n",
    "\n",
    "data = torch.Tensor(data).long().reshape(1,-1)\n",
    "\n",
    "progression_dataset = MyDataset(data,torch.Tensor([0]))\n",
    "\n",
    "progression_loader = DataLoader(progression_dataset, batch_size=1, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4, 39, 61, 13, 35, 70, 81, 23, 13, 64, 27, 13, 82, 23, 23,  9, 13, 44,\n",
      "        27, 13, 24, 60,  9, 23, 13, 64, 27, 23, 84, 82, 17, 48,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7, 44, 27, 13, 24, 60,  9, 82, 13, 64, 27,\n",
      "        23, 84, 82, 44, 27, 13, 24, 60,  9, 40, 13, 64, 27, 82, 72, 59, 17, 46,\n",
      "        51, 53, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "print(progression_loader.dataset.sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "predictions , probs = test(model, criterion, progression_loader)\n",
    "\n",
    "\n",
    "# print(\"1\",probs.shape)\n",
    "probs = probs.reshape(500,46)\n",
    "# print(\"2\",probs.shape)\n",
    "# probabilities = probs.squeeze(0)\n",
    "# print(probs.shape)\n",
    "\n",
    "seq_and_score = beam_search_decoder(probs, 15, 500, diversity_penalty_weight=0.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 12, 37, 0, 3, 4, 10, 36, 14, 4, 42, 4, 20, 26, 1, 1, 4, 4, 4, 1, 35, 0, 22, 10, 1, 18, 25, 25, 4, 21, 45, 18, 32, 10, 44, 21, 27, 4, 15, 0, 36, 36, 0, 28, 24, 28, 12, 4, 4, 26, 22, 4, 21, 0, 1, 21, 18, 4, 21, 1, 14, 1, 1, 4, 39, 0, 4, 4, 4, 3, 37, 10, 44, 21, 19, 22, 37, 32, 0, 36, 0, 4, 25, 24, 4, 36, 19, 42, 21, 1, 34, 10, 3, 20, 45, 42, 1, 21, 0, 12, 27, 27, 21, 14, 4, 11, 10, 24, 27, 25, 10, 45, 4, 44, 9, 45, 44, 44, 0, 4, 21, 36, 2, 1, 28, 41, 4, 4, 4, 21, 21, 28, 21, 18, 22, 10, 45, 45]\n",
      "13\n",
      "nft<SOS>mqndxqrq,gooqqqoz<SOS>}no277qeU2*nyecqs<SOS>dd<SOS>a8afqqg}qe<SOS>oe2qeoxooqv<SOS>qqqmtnyeT}t*<SOS>d<SOS>q78qdTreobnm,Uroe<SOS>fccexq n8c7nUqy0Uyy<SOS>qed3oalqqqeeae2}nUU\n"
     ]
    }
   ],
   "source": [
    "print(seq_and_score[0][0])\n",
    "\n",
    "\n",
    "print(target_char_to_int[\"<pad>\"])\n",
    "\n",
    "print(convert_to_char(seq_and_score[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
