{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:25:56.266935Z",
     "iopub.status.busy": "2023-11-13T04:25:56.266266Z",
     "iopub.status.idle": "2023-11-13T04:25:56.271913Z",
     "shell.execute_reply": "2023-11-13T04:25:56.271059Z",
     "shell.execute_reply.started": "2023-11-13T04:25:56.266900Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:02.305362Z",
     "iopub.status.busy": "2023-11-13T04:26:02.304515Z",
     "iopub.status.idle": "2023-11-13T04:26:02.311103Z",
     "shell.execute_reply": "2023-11-13T04:26:02.310049Z",
     "shell.execute_reply.started": "2023-11-13T04:26:02.305306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the GPU available? True\n"
     ]
    }
   ],
   "source": [
    "gpu_avail = torch.cuda.is_available()\n",
    "print(f\"Is the GPU available? {gpu_avail}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:04.181149Z",
     "iopub.status.busy": "2023-11-13T04:26:04.180304Z",
     "iopub.status.idle": "2023-11-13T04:26:04.186039Z",
     "shell.execute_reply": "2023-11-13T04:26:04.185136Z",
     "shell.execute_reply.started": "2023-11-13T04:26:04.181114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:06.495444Z",
     "iopub.status.busy": "2023-11-13T04:26:06.495048Z",
     "iopub.status.idle": "2023-11-13T04:26:06.503615Z",
     "shell.execute_reply": "2023-11-13T04:26:06.502495Z",
     "shell.execute_reply.started": "2023-11-13T04:26:06.495414Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with open(f\"{path}.sources\") as f:\n",
    "            self.sources = f.readlines()\n",
    "            self.vocab_src = set()\n",
    "            for lines in self.sources:\n",
    "                self.vocab_src.update(lines[1:-1])\n",
    "                \n",
    "        with open(f\"{path}.targets\") as f:\n",
    "            self.targets = f.readlines()\n",
    "            self.vocab_tgt = set()\n",
    "            for lines in self.targets:\n",
    "                self.vocab_tgt.update(lines)\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sources[idx][1:-1], self.targets[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:08.859028Z",
     "iopub.status.busy": "2023-11-13T04:26:08.858187Z",
     "iopub.status.idle": "2023-11-13T04:26:10.471213Z",
     "shell.execute_reply": "2023-11-13T04:26:10.470375Z",
     "shell.execute_reply.started": "2023-11-13T04:26:08.858994Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_src = set()\n",
    "vocab_tgt = set()\n",
    "train_dataset = CustomDataset('Data/A3 files/train')\n",
    "eval_dataset = CustomDataset('Data/A3 files/dev')\n",
    "test_dataset = CustomDataset('Data/A3 files/test')\n",
    "vocab_src.update(train_dataset.vocab_src)\n",
    "vocab_src.update(eval_dataset.vocab_src)\n",
    "vocab_src.update(test_dataset.vocab_src)\n",
    "vocab_tgt.update(train_dataset.vocab_tgt)\n",
    "vocab_tgt.update(eval_dataset.vocab_tgt)\n",
    "vocab_tgt.update(test_dataset.vocab_tgt)\n",
    "vocab_src = list(vocab_src)\n",
    "vocab_tgt = list(vocab_tgt)\n",
    "temp = {}\n",
    "for ind, key in enumerate(vocab_src):\n",
    "    temp[key] = ind\n",
    "vocab_src = temp\n",
    "temp = {}\n",
    "for ind, key in enumerate(vocab_tgt):\n",
    "    temp[key] = ind\n",
    "vocab_tgt = temp\n",
    "vocab_src[\"END\"] = 84\n",
    "vocab_src[\"PAD\"] = 85\n",
    "vocab_tgt[\"STR\"] = 44\n",
    "vocab_tgt[\"END\"] = 45\n",
    "vocab_tgt[\"PAD\"] = 46\n",
    "reverse_vocab_tgt = {}\n",
    "for key in vocab_tgt:\n",
    "    val = vocab_tgt[key]\n",
    "    reverse_vocab_tgt[val] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:13.612417Z",
     "iopub.status.busy": "2023-11-13T04:26:13.612066Z",
     "iopub.status.idle": "2023-11-13T04:26:13.640847Z",
     "shell.execute_reply": "2023-11-13T04:26:13.639734Z",
     "shell.execute_reply.started": "2023-11-13T04:26:13.612391Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size = 32, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:17.458618Z",
     "iopub.status.busy": "2023-11-13T04:26:17.458247Z",
     "iopub.status.idle": "2023-11-13T04:26:17.471097Z",
     "shell.execute_reply": "2023-11-13T04:26:17.469974Z",
     "shell.execute_reply.started": "2023-11-13T04:26:17.458589Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dims = 512, hidden_size = 512,num_layers = 2, max_src = 500, max_tgt = 500):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab_src),dims)\n",
    "        self.input_size = dims\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.max_src = max_src\n",
    "        self.max_tgt = max_tgt\n",
    "        self.drop = nn.Dropout(p = 0.5)\n",
    "        self.encoder = nn.LSTM(\n",
    "            dims, hidden_size, num_layers, batch_first = True,bidirectional=True, dropout = 0.5\n",
    "        )\n",
    "        \n",
    "    def encode_inp(self, x):\n",
    "        encoded_x = torch.zeros(len(x), self.max_src, dtype = int) + 85\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[i])):\n",
    "                encoded_x[i][j] = vocab_src[x[i][j]]\n",
    "            encoded_x[i][len(x[i])] = vocab_src[\"END\"]\n",
    "        return encoded_x.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded_x = self.encode_inp(x)\n",
    "        input_seq = self.drop(self.embedding(encoded_x))\n",
    "        hidden = torch.zeros(2*self.num_layers,input_seq.shape[0],self.hidden_size).to(device)\n",
    "        cell = torch.zeros(2*self.num_layers,input_seq.shape[0],self.hidden_size).to(device)\n",
    "        out, _ = self.encoder(input_seq,(hidden, cell))\n",
    "        return out\n",
    "\n",
    "    def inference(self, x):\n",
    "        self.max_src = len(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diversity_penalty(new_sequence, existing_sequences):\n",
    "    \"\"\"\n",
    "    Calculate a diversity penalty based on the new sequence and existing sequences.\n",
    "    This is a simple example of diversity penalty calculation and can be customized.\n",
    "\n",
    "    Args:\n",
    "    - new_sequence (torch.Tensor): The new sequence to be penalized.\n",
    "    - existing_sequences (list of torch.Tensor): A list of existing sequences.\n",
    "\n",
    "    Returns:\n",
    "    - float: The diversity penalty score.\n",
    "    \"\"\"\n",
    "    penalty = 0.0\n",
    "    for seq in existing_sequences:\n",
    "        similarity = torch.sum(torch.eq(new_sequence, seq[0]).float()) / len(new_sequence)\n",
    "        penalty += similarity\n",
    "    return penalty\n",
    "\n",
    "def beam_search_decoder(probabilities, beam_width, max_length, diversity_penalty_weight=5):\n",
    "    \"\"\"\n",
    "    Beam search decoder for sequence generation.\n",
    "\n",
    "    Args:\n",
    "    - probabilities (torch.Tensor): A 2D tensor of shape (sequence_length, vocab_size)\n",
    "      containing the predicted probabilities for each token at each time step.\n",
    "    - beam_width (int): The number of sequences to consider at each decoding step.\n",
    "    - max_length (int): The maximum length of the generated sequence.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples, each containing (sequence, score), where:\n",
    "      - sequence (list): A list of token IDs representing the generated sequence.\n",
    "      - score (float): The log-likelihood score of the sequence.\n",
    "    \"\"\"\n",
    "    out  = torch.argmax(nn.Softmax(dim = 1)(probabilities), dim = 1)\n",
    "    seq_len = 0\n",
    "    for char in out:\n",
    "        if(char == 45):\n",
    "            break\n",
    "        else:\n",
    "            seq_len += 1\n",
    "\n",
    "    # Get the sequence length and vocabulary size\n",
    "    sequence_length, vocab_size = probabilities.shape\n",
    "    sequence_length = seq_len\n",
    "    max_length = seq_len\n",
    "\n",
    "    # Initialize the beam with the empty sequence\n",
    "    beam = [(torch.tensor([], dtype=torch.long).to(device), 0.0)]\n",
    "\n",
    "    # Iterate through each time step\n",
    "    for t in range(max_length):\n",
    "        new_beam = []\n",
    "\n",
    "        # Expand the beam by considering the top 'beam_width' candidates at each step\n",
    "        for sequence, score in beam:\n",
    "            # If the sequence is already at the maximum length, keep it as is\n",
    "            if len(sequence) == max_length:\n",
    "                new_beam.append((sequence, score))\n",
    "                continue\n",
    "\n",
    "            # Get the probabilities for the next token\n",
    "            t_probs = probabilities[t]\n",
    "\n",
    "            # Get the top 'beam_width' token IDs and their corresponding log-likelihood scores\n",
    "            top_scores, top_tokens = torch.topk(t_probs, beam_width)\n",
    "\n",
    "            # Expand the current sequence with each of the top tokens\n",
    "            for token, token_score in zip(top_tokens, top_scores):\n",
    "                new_sequence = torch.cat([sequence, token.unsqueeze(0)], dim=0)\n",
    "                new_score = score + token_score.item()\n",
    "    \n",
    "                # Apply the diversity penalty\n",
    "                if len(new_sequence) > 1:\n",
    "                    # Calculate a penalty based on sequence diversity\n",
    "                    diversity_penalty = diversity_penalty_weight * calculate_diversity_penalty(new_sequence, new_beam)\n",
    "                    new_score -= diversity_penalty\n",
    "                    \n",
    "                new_beam.append((new_sequence, new_score))\n",
    "        print(t)\n",
    "\n",
    "        # Keep the top 'beam_width' candidates\n",
    "        new_beam.sort(key=lambda x: -x[1])\n",
    "        beam = new_beam[:beam_width]\n",
    "\n",
    "    # Return the top sequence and its score\n",
    "    return [(sequence.tolist(), score) for sequence, score in beam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:18.884154Z",
     "iopub.status.busy": "2023-11-13T04:26:18.883836Z",
     "iopub.status.idle": "2023-11-13T04:26:18.926870Z",
     "shell.execute_reply": "2023-11-13T04:26:18.925694Z",
     "shell.execute_reply.started": "2023-11-13T04:26:18.884130Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,dims = 512, hidden_size = 512,num_layers = 2, max_src = 500, max_tgt = 500):\n",
    "        super(Decoder, self).__init__()\n",
    "#         self.embedding = embedding\n",
    "        self.embedding = nn.Embedding(len(vocab_tgt),dims)\n",
    "        self.input_size = dims\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.max_src = max_src\n",
    "        self.max_tgt = max_tgt\n",
    "        self.drop = nn.Dropout(p = 0.5)\n",
    "        self.dec_cells = nn.ModuleList([nn.LSTMCell(2*hidden_size+dims, hidden_size), nn.LSTMCell(hidden_size, hidden_size)])\n",
    "        self.linear = nn.Linear(hidden_size,len(vocab_tgt)-1)\n",
    "    \n",
    "    def encode_inp(self, x):\n",
    "        encoded_x = torch.zeros(len(x), self.max_tgt+1, dtype = int) + 46\n",
    "        encoded_x_end = torch.zeros(len(x))\n",
    "        for i in range(len(x)):\n",
    "            encoded_x[i][0] = vocab_tgt[\"STR\"]\n",
    "            for j in range(len(x[i])):\n",
    "                encoded_x[i][j+1] = vocab_tgt[x[i][j]]\n",
    "            encoded_x[i][len(x[i])+1] = vocab_tgt[\"END\"]\n",
    "            encoded_x_end[i] = len(x[i])+1\n",
    "        return encoded_x.to(device), encoded_x_end\n",
    "    \n",
    "    def calcontext(self, timestep, query):\n",
    "        extended_query = torch.cat((query, query), dim = 1)\n",
    "        permuted_context = self.context.permute(1,0,2)\n",
    "#         for encoder_timestep in range(self.context.shape[1]):\n",
    "#             scores.append(torch.sum(self.context[:,encoder_timestep] * extended_query, dim = 1, keepdims=True))\n",
    "#         scores = torch.cat(scores, dim = 1)\n",
    "        scores = torch.sum(permuted_context * extended_query, dim = 2).permute(1,0)\n",
    "        weights = nn.Softmax(dim = 1)(scores).unsqueeze(2)\n",
    "        alignment = torch.sum(weights * self.context,  dim = 1)\n",
    "        \n",
    "        return alignment\n",
    "        \n",
    "    \n",
    "    def forward(self, context, target_,teacher_ratio):\n",
    "        self.context = context\n",
    "        encoded_x, encoded_x_end = self.encode_inp(target_)\n",
    "        target_seq = self.embedding(encoded_x)\n",
    "        \n",
    "        initial_hidden1 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        initial_cell1 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        initial_hidden2 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        initial_cell2 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        \n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        cell_states = []\n",
    "        query = [initial_hidden2]\n",
    "        for timestep in range(self.max_tgt+1):\n",
    "            if(timestep == 0):\n",
    "                (h_t1, c_t1) = self.dec_cells[0](self.drop(torch.cat((target_seq[:,timestep],self.calcontext(0,query[-1])),dim=1)), (initial_hidden1, initial_cell1))\n",
    "                (h_t2, c_t2) = self.dec_cells[1](self.drop(h_t1), (initial_hidden2, initial_cell2))\n",
    "            else:\n",
    "                input = []\n",
    "                if(torch.rand(1).item() < teacher_ratio):\n",
    "                    input = target_seq[:,timestep]\n",
    "                else:\n",
    "                    input = self.embedding(torch.argmax(nn.Softmax(dim = 1)(outputs[-1][:,0]),dim=1))\n",
    "                    \n",
    "                (h_t1, c_t1) = self.dec_cells[0](self.drop(torch.cat((input,self.calcontext(timestep,query[-1])),dim=1)), (hidden_states[-1][0], cell_states[-1][0]))\n",
    "                (h_t2, c_t2) = self.dec_cells[1](self.drop(h_t1), (hidden_states[-1][1], cell_states[-1][1]))\n",
    "            hidden_states.append([h_t1, h_t2])\n",
    "            cell_states.append([c_t1, c_t2])\n",
    "            query.append(h_t2)\n",
    "            out = self.linear(h_t2)\n",
    "            outputs.append(out.unsqueeze(1))\n",
    "    \n",
    "\n",
    "        output_prob = torch.cat(outputs,dim = 1)\n",
    "        \n",
    "        return nn.LogSoftmax(dim = 2)(output_prob), encoded_x\n",
    "    \n",
    "    def seq_to_vis(self, seq):\n",
    "        vis = \"\"\n",
    "        for char in seq:\n",
    "            char = char\n",
    "            if(char == 45):\n",
    "                return vis\n",
    "            vis += reverse_vocab_tgt[char]\n",
    "        \n",
    "        return vis\n",
    "\n",
    "\n",
    "    def inference(self, context):\n",
    "        self.context = context\n",
    "        start = torch.zeros(1,1, dtype = int).to(device) + 44\n",
    "        target_seq = self.embedding(start)\n",
    "        \n",
    "        initial_hidden1 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        initial_cell1 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        initial_hidden2 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        initial_cell2 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        \n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        cell_states = []\n",
    "        query = [initial_hidden2]\n",
    "        for timestep in range(self.max_tgt+1):\n",
    "            if(timestep == 0):\n",
    "                (h_t1, c_t1) = self.dec_cells[0](self.drop(torch.cat((target_seq[:,timestep],self.calcontext(0,query[-1])),dim=1)), (initial_hidden1, initial_cell1))\n",
    "                (h_t2, c_t2) = self.dec_cells[1](self.drop(h_t1), (initial_hidden2, initial_cell2))\n",
    "            else:\n",
    "                input = self.embedding(torch.argmax(nn.Softmax(dim = 1)(outputs[-1][:,0]),dim=1))\n",
    "                    \n",
    "                (h_t1, c_t1) = self.dec_cells[0](self.drop(torch.cat((input,self.calcontext(timestep,query[-1])),dim=1)), (hidden_states[-1][0], cell_states[-1][0]))\n",
    "                (h_t2, c_t2) = self.dec_cells[1](self.drop(h_t1), (hidden_states[-1][1], cell_states[-1][1]))\n",
    "            hidden_states.append([h_t1, h_t2])\n",
    "            cell_states.append([c_t1, c_t2])\n",
    "            query.append(h_t2)\n",
    "            out = self.linear(h_t2)\n",
    "            outputs.append(out.unsqueeze(1))\n",
    "    \n",
    "        output_prob = torch.cat(outputs,dim = 1)\n",
    "        out = torch.argmax(nn.Softmax(dim = 2)(output_prob), dim = 2)\n",
    "        \n",
    "#         Example usage:\n",
    "#         Replace 'probabilities', 'beam_width', and 'max_length' with your actual values\n",
    "#         probabilities = torch.tensor(...)  # Shape: (sequence_length, vocab_size)\n",
    "#         beam_width = 3\n",
    "#         max_length = 10\n",
    "        decoded_sequences = beam_search_decoder(output_prob[0], beam_width = 15, max_length = 500)\n",
    "        vis = []\n",
    "        for sequence, score in decoded_sequences:\n",
    "            vis.append(self.seq_to_vis(sequence))\n",
    "            # print(f\"Sequence: {sequence}, {self.seq_to_vis(sequence)}, Log-Likelihood Score: {score}\")\n",
    "\n",
    "        return vis\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-12T12:30:57.584467Z",
     "iopub.status.busy": "2023-11-12T12:30:57.583708Z",
     "iopub.status.idle": "2023-11-12T12:30:57.740710Z",
     "shell.execute_reply": "2023-11-12T12:30:57.739906Z",
     "shell.execute_reply.started": "2023-11-12T12:30:57.584433Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "optimizer_encoder = optim.Adam(encoder.parameters(), lr=0.0001)\n",
    "optimizer_decoder = optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "criterion = nn.NLLLoss(ignore_index = 46)\n",
    "\n",
    "# Set up early stopping parameters\n",
    "patience = 2000  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "loss_val = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-12T12:31:09.077395Z",
     "iopub.status.busy": "2023-11-12T12:31:09.076684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0], Batch[1], Train loss: 3.836839199066162\n",
      "Epoch[0], Val loss: 3.8171885013580322\n",
      "Epoch[0], Batch[2], Train loss: 3.8227381706237793\n",
      "Epoch[0], Val loss: 3.795388698577881\n",
      "Epoch[0], Batch[3], Train loss: 3.8073503971099854\n",
      "Epoch[0], Val loss: 3.775780439376831\n",
      "Epoch[0], Batch[4], Train loss: 3.791015863418579\n",
      "Epoch[0], Val loss: 3.7505481243133545\n",
      "Epoch[0], Batch[5], Train loss: 3.772714853286743\n",
      "Epoch[0], Val loss: 3.7165732383728027\n",
      "Epoch[0], Batch[6], Train loss: 3.752525806427002\n",
      "Epoch[0], Val loss: 3.679064989089966\n",
      "Epoch[0], Batch[7], Train loss: 3.7234628200531006\n",
      "Epoch[0], Val loss: 3.6283559799194336\n",
      "Epoch[0], Batch[8], Train loss: 3.6851515769958496\n",
      "Epoch[0], Val loss: 3.578216075897217\n",
      "Epoch[0], Batch[9], Train loss: 3.643625259399414\n",
      "Epoch[0], Val loss: 3.524170398712158\n",
      "Epoch[0], Batch[10], Train loss: 3.59562087059021\n",
      "Epoch[0], Val loss: 3.458437204360962\n",
      "Epoch[0], Batch[11], Train loss: 3.538316011428833\n",
      "Epoch[0], Val loss: 3.388319730758667\n",
      "Epoch[0], Batch[12], Train loss: 3.475210428237915\n",
      "Epoch[0], Val loss: 3.326045274734497\n",
      "Epoch[0], Batch[13], Train loss: 3.403271436691284\n",
      "Epoch[0], Val loss: 3.2578015327453613\n",
      "Epoch[0], Batch[14], Train loss: 3.3318233489990234\n",
      "Epoch[0], Val loss: 3.210559368133545\n",
      "Epoch[0], Batch[15], Train loss: 3.2627151012420654\n",
      "Epoch[0], Val loss: 3.1787362098693848\n",
      "Epoch[0], Batch[16], Train loss: 3.2071988582611084\n",
      "Epoch[0], Val loss: 3.1568171977996826\n",
      "Epoch[0], Batch[17], Train loss: 3.1708641052246094\n",
      "Epoch[0], Val loss: 3.1416139602661133\n",
      "Epoch[0], Batch[18], Train loss: 3.145772695541382\n",
      "Epoch[0], Val loss: 3.1211845874786377\n",
      "Epoch[0], Batch[19], Train loss: 3.1170318126678467\n",
      "Epoch[0], Val loss: 3.1016013622283936\n",
      "Epoch[0], Batch[20], Train loss: 3.0987548828125\n",
      "Epoch[0], Val loss: 3.085355043411255\n",
      "Epoch[0], Batch[21], Train loss: 3.080313205718994\n",
      "Epoch[0], Val loss: 3.061161518096924\n",
      "Epoch[0], Batch[22], Train loss: 3.061344623565674\n",
      "Epoch[0], Val loss: 3.0432605743408203\n",
      "Epoch[0], Batch[23], Train loss: 3.048034429550171\n",
      "Epoch[0], Val loss: 3.04305362701416\n",
      "Epoch[0], Batch[24], Train loss: 3.044102907180786\n",
      "Epoch[0], Val loss: 3.036151170730591\n",
      "Epoch[0], Batch[25], Train loss: 3.0403826236724854\n",
      "Epoch[0], Val loss: 3.018434524536133\n",
      "Epoch[0], Batch[26], Train loss: 3.0257585048675537\n",
      "Epoch[0], Val loss: 3.0093636512756348\n",
      "Epoch[0], Batch[27], Train loss: 3.01759672164917\n",
      "Epoch[0], Val loss: 2.9881372451782227\n",
      "Epoch[0], Batch[28], Train loss: 3.0014004707336426\n",
      "Epoch[0], Val loss: 2.9772307872772217\n",
      "Epoch[0], Batch[29], Train loss: 2.991401433944702\n",
      "Epoch[0], Val loss: 2.967029571533203\n",
      "Epoch[0], Batch[30], Train loss: 2.9798591136932373\n",
      "Epoch[0], Val loss: 2.9545633792877197\n",
      "Epoch[0], Batch[31], Train loss: 2.9683947563171387\n",
      "Epoch[0], Val loss: 2.949981212615967\n",
      "Epoch[0], Batch[32], Train loss: 2.96460223197937\n",
      "Epoch[0], Val loss: 2.9341607093811035\n",
      "Epoch[0], Batch[33], Train loss: 2.9492974281311035\n",
      "Epoch[0], Val loss: 2.922832489013672\n",
      "Epoch[0], Batch[34], Train loss: 2.9422364234924316\n",
      "Epoch[0], Val loss: 2.907947301864624\n",
      "Epoch[0], Batch[35], Train loss: 2.9291136264801025\n",
      "Epoch[0], Val loss: 2.8935706615448\n",
      "Epoch[0], Batch[36], Train loss: 2.9221978187561035\n",
      "Epoch[0], Val loss: 2.872046947479248\n",
      "Epoch[0], Batch[37], Train loss: 2.905264377593994\n",
      "Epoch[0], Val loss: 2.8559138774871826\n",
      "Epoch[0], Batch[38], Train loss: 2.8940677642822266\n",
      "Epoch[0], Val loss: 2.841269016265869\n",
      "Epoch[0], Batch[39], Train loss: 2.8917360305786133\n",
      "Epoch[0], Val loss: 2.81685471534729\n",
      "Epoch[0], Batch[40], Train loss: 2.874213457107544\n",
      "Epoch[0], Val loss: 2.8047916889190674\n",
      "Epoch[0], Batch[41], Train loss: 2.855959415435791\n",
      "Epoch[0], Val loss: 2.7845652103424072\n",
      "Epoch[0], Batch[42], Train loss: 2.843513011932373\n",
      "Epoch[0], Val loss: 2.7672994136810303\n",
      "Epoch[0], Batch[43], Train loss: 2.8280842304229736\n",
      "Epoch[0], Val loss: 2.7525875568389893\n",
      "Epoch[0], Batch[44], Train loss: 2.8096609115600586\n",
      "Epoch[0], Val loss: 2.7342753410339355\n",
      "Epoch[0], Batch[45], Train loss: 2.7963833808898926\n",
      "Epoch[0], Val loss: 2.7134175300598145\n",
      "Epoch[0], Batch[46], Train loss: 2.7757163047790527\n",
      "Epoch[0], Val loss: 2.6857430934906006\n",
      "Epoch[0], Batch[47], Train loss: 2.7554876804351807\n",
      "Epoch[0], Val loss: 2.662152051925659\n",
      "Epoch[0], Batch[48], Train loss: 2.7366037368774414\n",
      "Epoch[0], Val loss: 2.644970417022705\n",
      "Epoch[0], Batch[49], Train loss: 2.7244620323181152\n",
      "Epoch[0], Val loss: 2.6182234287261963\n",
      "Epoch[0], Batch[50], Train loss: 2.707028388977051\n",
      "Epoch[0], Val loss: 2.5954012870788574\n",
      "Epoch[0], Batch[51], Train loss: 2.679927349090576\n",
      "Epoch[0], Val loss: 2.5642690658569336\n",
      "Epoch[0], Batch[52], Train loss: 2.6661553382873535\n",
      "Epoch[0], Val loss: 2.5357167720794678\n",
      "Epoch[0], Batch[53], Train loss: 2.6386632919311523\n",
      "Epoch[0], Val loss: 2.509465456008911\n",
      "Epoch[0], Batch[54], Train loss: 2.6181530952453613\n",
      "Epoch[0], Val loss: 2.4933605194091797\n",
      "Epoch[0], Batch[55], Train loss: 2.597083330154419\n",
      "Epoch[0], Val loss: 2.467639207839966\n",
      "Epoch[0], Batch[56], Train loss: 2.5714690685272217\n",
      "Epoch[0], Val loss: 2.435943126678467\n",
      "Epoch[0], Batch[57], Train loss: 2.553356170654297\n",
      "Epoch[0], Val loss: 2.4163472652435303\n",
      "Epoch[0], Batch[58], Train loss: 2.520928382873535\n",
      "Epoch[0], Val loss: 2.382418632507324\n",
      "Epoch[0], Batch[59], Train loss: 2.4971187114715576\n",
      "Epoch[0], Val loss: 2.3652913570404053\n",
      "Epoch[0], Batch[60], Train loss: 2.475675582885742\n",
      "Epoch[0], Val loss: 2.3393285274505615\n",
      "Epoch[0], Batch[61], Train loss: 2.447413444519043\n",
      "Epoch[0], Val loss: 2.310349941253662\n",
      "Epoch[0], Batch[62], Train loss: 2.4326083660125732\n",
      "Epoch[0], Val loss: 2.286367416381836\n",
      "Epoch[0], Batch[63], Train loss: 2.400934934616089\n",
      "Epoch[0], Val loss: 2.2622649669647217\n",
      "Epoch[0], Batch[64], Train loss: 2.376199722290039\n",
      "Epoch[0], Val loss: 2.2381649017333984\n",
      "Epoch[0], Batch[65], Train loss: 2.3583297729492188\n",
      "Epoch[0], Val loss: 2.2096731662750244\n",
      "Epoch[0], Batch[66], Train loss: 2.334515333175659\n",
      "Epoch[0], Val loss: 2.1871395111083984\n",
      "Epoch[0], Batch[67], Train loss: 2.2940969467163086\n",
      "Epoch[0], Val loss: 2.166574239730835\n",
      "Epoch[0], Batch[68], Train loss: 2.2779295444488525\n",
      "Epoch[0], Val loss: 2.1415929794311523\n",
      "Epoch[0], Batch[69], Train loss: 2.25612735748291\n",
      "Epoch[0], Val loss: 2.1188135147094727\n",
      "Epoch[0], Batch[70], Train loss: 2.223578691482544\n",
      "Epoch[0], Val loss: 2.0975594520568848\n",
      "Epoch[0], Batch[71], Train loss: 2.206831693649292\n",
      "Epoch[0], Val loss: 2.064312696456909\n",
      "Epoch[0], Batch[72], Train loss: 2.183290481567383\n",
      "Epoch[0], Val loss: 2.0435287952423096\n",
      "Epoch[0], Batch[73], Train loss: 2.163926839828491\n",
      "Epoch[0], Val loss: 2.0176737308502197\n",
      "Epoch[0], Batch[74], Train loss: 2.136146306991577\n",
      "Epoch[0], Val loss: 2.000410318374634\n",
      "Epoch[0], Batch[75], Train loss: 2.1148767471313477\n",
      "Epoch[0], Val loss: 1.974666953086853\n",
      "Epoch[0], Batch[76], Train loss: 2.080742120742798\n",
      "Epoch[0], Val loss: 1.9521268606185913\n",
      "Epoch[0], Batch[77], Train loss: 2.0689609050750732\n",
      "Epoch[0], Val loss: 1.9267890453338623\n",
      "Epoch[0], Batch[78], Train loss: 2.049872636795044\n",
      "Epoch[0], Val loss: 1.8985936641693115\n",
      "Epoch[0], Batch[79], Train loss: 2.0352535247802734\n",
      "Epoch[0], Val loss: 1.8825260400772095\n",
      "Epoch[0], Batch[80], Train loss: 1.990086555480957\n",
      "Epoch[0], Val loss: 1.8666722774505615\n",
      "Epoch[0], Batch[81], Train loss: 1.9871381521224976\n",
      "Epoch[0], Val loss: 1.8389047384262085\n",
      "Epoch[0], Batch[82], Train loss: 1.967387318611145\n",
      "Epoch[0], Val loss: 1.8159469366073608\n",
      "Epoch[0], Batch[83], Train loss: 1.9372059106826782\n",
      "Epoch[0], Val loss: 1.8037734031677246\n",
      "Epoch[0], Batch[84], Train loss: 1.9135218858718872\n",
      "Epoch[0], Val loss: 1.7755125761032104\n",
      "Epoch[0], Batch[85], Train loss: 1.8968305587768555\n",
      "Epoch[0], Val loss: 1.7513223886489868\n",
      "Epoch[0], Batch[86], Train loss: 1.8732563257217407\n",
      "Epoch[0], Val loss: 1.726260781288147\n",
      "Epoch[0], Batch[87], Train loss: 1.8512144088745117\n",
      "Epoch[0], Val loss: 1.712129831314087\n",
      "Epoch[0], Batch[88], Train loss: 1.827113389968872\n",
      "Epoch[0], Val loss: 1.6931524276733398\n",
      "Epoch[0], Batch[89], Train loss: 1.814732551574707\n",
      "Epoch[0], Val loss: 1.6674429178237915\n",
      "Epoch[0], Batch[90], Train loss: 1.7975395917892456\n",
      "Epoch[0], Val loss: 1.6690765619277954\n",
      "Epoch[0], Batch[91], Train loss: 1.7738268375396729\n",
      "Epoch[0], Val loss: 1.6251494884490967\n",
      "Epoch[0], Batch[92], Train loss: 1.7566320896148682\n",
      "Epoch[0], Val loss: 1.5918083190917969\n",
      "Epoch[0], Batch[93], Train loss: 1.7510954141616821\n",
      "Epoch[0], Val loss: 1.574129581451416\n",
      "Epoch[0], Batch[94], Train loss: 1.7176905870437622\n",
      "Epoch[0], Val loss: 1.565573811531067\n",
      "Epoch[0], Batch[95], Train loss: 1.6929895877838135\n",
      "Epoch[0], Val loss: 1.5597872734069824\n",
      "Epoch[0], Batch[96], Train loss: 1.6690874099731445\n",
      "Epoch[0], Val loss: 1.5416475534439087\n",
      "Epoch[0], Batch[97], Train loss: 1.6734286546707153\n",
      "Epoch[0], Val loss: 1.4976345300674438\n",
      "Epoch[0], Batch[98], Train loss: 1.6346021890640259\n",
      "Epoch[0], Val loss: 1.4927595853805542\n",
      "Epoch[0], Batch[99], Train loss: 1.6177828311920166\n",
      "Epoch[0], Val loss: 1.4788910150527954\n",
      "Epoch[0], Batch[100], Train loss: 1.5914496183395386\n",
      "Epoch[0], Val loss: 1.4700440168380737\n",
      "Epoch[0], Batch[101], Train loss: 1.5992943048477173\n",
      "Epoch[0], Val loss: 1.4372022151947021\n",
      "Epoch[0], Batch[102], Train loss: 1.5630066394805908\n",
      "Epoch[0], Val loss: 1.4156244993209839\n",
      "Epoch[0], Batch[103], Train loss: 1.5459368228912354\n",
      "Epoch[0], Val loss: 1.3954999446868896\n",
      "Epoch[0], Batch[104], Train loss: 1.544629454612732\n",
      "Epoch[0], Val loss: 1.3796191215515137\n",
      "Epoch[0], Batch[105], Train loss: 1.5145556926727295\n",
      "Epoch[0], Val loss: 1.3610360622406006\n",
      "Epoch[0], Batch[106], Train loss: 1.482515811920166\n",
      "Epoch[0], Val loss: 1.3484091758728027\n",
      "Epoch[0], Batch[107], Train loss: 1.477959394454956\n",
      "Epoch[0], Val loss: 1.344449758529663\n",
      "Epoch[0], Batch[108], Train loss: 1.4610217809677124\n",
      "Epoch[0], Val loss: 1.3142213821411133\n",
      "Epoch[0], Batch[109], Train loss: 1.4449554681777954\n",
      "Epoch[0], Val loss: 1.307138204574585\n",
      "Epoch[0], Batch[110], Train loss: 1.4259344339370728\n",
      "Epoch[0], Val loss: 1.2708054780960083\n",
      "Epoch[0], Batch[111], Train loss: 1.4281450510025024\n",
      "Epoch[0], Val loss: 1.263844609260559\n",
      "Epoch[0], Batch[112], Train loss: 1.3875566720962524\n",
      "Epoch[0], Val loss: 1.2531110048294067\n",
      "Epoch[0], Batch[113], Train loss: 1.3724671602249146\n",
      "Epoch[0], Val loss: 1.2415273189544678\n",
      "Epoch[0], Batch[114], Train loss: 1.3596787452697754\n",
      "Epoch[0], Val loss: 1.2367019653320312\n",
      "Epoch[0], Batch[115], Train loss: 1.3406795263290405\n",
      "Epoch[0], Val loss: 1.174552321434021\n",
      "Epoch[0], Batch[116], Train loss: 1.32975435256958\n",
      "Epoch[0], Val loss: 1.2048962116241455\n",
      "Epoch[0], Batch[117], Train loss: 1.3201936483383179\n",
      "Epoch[0], Val loss: 1.171068549156189\n",
      "Epoch[0], Batch[118], Train loss: 1.3141614198684692\n",
      "Epoch[0], Val loss: 1.1359121799468994\n",
      "Epoch[0], Batch[119], Train loss: 1.290297031402588\n",
      "Epoch[0], Val loss: 1.1493957042694092\n",
      "Epoch[0], Batch[120], Train loss: 1.2734347581863403\n",
      "Epoch[0], Val loss: 1.1329772472381592\n",
      "Epoch[0], Batch[121], Train loss: 1.254254937171936\n",
      "Epoch[0], Val loss: 1.111486554145813\n",
      "Epoch[0], Batch[122], Train loss: 1.2410231828689575\n",
      "Epoch[0], Val loss: 1.085966944694519\n",
      "Epoch[0], Batch[123], Train loss: 1.2480584383010864\n",
      "Epoch[0], Val loss: 1.083823561668396\n",
      "Epoch[0], Batch[124], Train loss: 1.2142304182052612\n",
      "Epoch[0], Val loss: 1.0485811233520508\n",
      "Epoch[0], Batch[125], Train loss: 1.204615592956543\n",
      "Epoch[0], Val loss: 1.0504127740859985\n",
      "Epoch[0], Batch[126], Train loss: 1.1845365762710571\n",
      "Epoch[0], Val loss: 1.0394444465637207\n",
      "Epoch[0], Batch[127], Train loss: 1.178470492362976\n",
      "Epoch[0], Val loss: 1.0107102394104004\n",
      "Epoch[0], Batch[128], Train loss: 1.1535481214523315\n",
      "Epoch[0], Val loss: 1.0223503112792969\n",
      "Epoch[0], Batch[129], Train loss: 1.1557049751281738\n",
      "Epoch[0], Val loss: 1.0322507619857788\n",
      "Epoch[0], Batch[130], Train loss: 1.1219362020492554\n",
      "Epoch[0], Val loss: 0.9913272261619568\n",
      "Epoch[0], Batch[131], Train loss: 1.1214498281478882\n",
      "Epoch[0], Val loss: 0.9576466679573059\n",
      "Epoch[0], Batch[132], Train loss: 1.0979080200195312\n",
      "Epoch[0], Val loss: 0.9794044494628906\n",
      "Epoch[0], Batch[133], Train loss: 1.1032506227493286\n",
      "Epoch[0], Val loss: 0.9473708271980286\n",
      "Epoch[0], Batch[134], Train loss: 1.0816713571548462\n",
      "Epoch[0], Val loss: 0.9284168481826782\n",
      "Epoch[0], Batch[135], Train loss: 1.0457254648208618\n",
      "Epoch[0], Val loss: 0.9177823662757874\n",
      "Epoch[0], Batch[136], Train loss: 1.0541547536849976\n",
      "Epoch[0], Val loss: 0.910030722618103\n",
      "Epoch[0], Batch[137], Train loss: 1.0263460874557495\n",
      "Epoch[0], Val loss: 0.8901659250259399\n",
      "Epoch[0], Batch[138], Train loss: 1.0237023830413818\n",
      "Epoch[0], Val loss: 0.8693393468856812\n",
      "Epoch[0], Batch[139], Train loss: 1.021775722503662\n",
      "Epoch[0], Val loss: 0.8710903525352478\n",
      "Epoch[0], Batch[140], Train loss: 0.9925024509429932\n",
      "Epoch[0], Val loss: 0.877644956111908\n",
      "Epoch[0], Batch[141], Train loss: 0.9949250221252441\n",
      "Epoch[0], Val loss: 0.8261362910270691\n",
      "Epoch[0], Batch[142], Train loss: 0.9718044400215149\n",
      "Epoch[0], Val loss: 0.8325223922729492\n",
      "Epoch[0], Batch[143], Train loss: 0.9776904582977295\n",
      "Epoch[0], Val loss: 0.8178402185440063\n",
      "Epoch[0], Batch[144], Train loss: 0.9632044434547424\n",
      "Epoch[0], Val loss: 0.814324676990509\n",
      "Epoch[0], Batch[145], Train loss: 0.9371969103813171\n",
      "Epoch[0], Val loss: 0.8115811347961426\n",
      "Epoch[0], Batch[146], Train loss: 0.9395362734794617\n",
      "Epoch[0], Val loss: 0.8087841868400574\n",
      "Epoch[0], Batch[147], Train loss: 0.9316126108169556\n",
      "Epoch[0], Val loss: 0.775560200214386\n",
      "Epoch[0], Batch[148], Train loss: 0.9225777387619019\n",
      "Epoch[0], Val loss: 0.761532187461853\n",
      "Epoch[0], Batch[149], Train loss: 0.9059334993362427\n",
      "Epoch[0], Val loss: 0.7604455351829529\n",
      "Epoch[0], Batch[150], Train loss: 0.9074063301086426\n",
      "Epoch[0], Val loss: 0.7361056208610535\n",
      "Epoch[0], Batch[151], Train loss: 0.8676644563674927\n",
      "Epoch[0], Val loss: 0.7493354082107544\n",
      "Epoch[0], Batch[152], Train loss: 0.8644159436225891\n",
      "Epoch[0], Val loss: 0.7443346381187439\n",
      "Epoch[0], Batch[153], Train loss: 0.8780077695846558\n",
      "Epoch[0], Val loss: 0.7291382551193237\n",
      "Epoch[0], Batch[154], Train loss: 0.8536161184310913\n",
      "Epoch[0], Val loss: 0.6962664723396301\n",
      "Epoch[0], Batch[155], Train loss: 0.8300348520278931\n",
      "Epoch[0], Val loss: 0.7034609317779541\n",
      "Epoch[0], Batch[156], Train loss: 0.8286007642745972\n",
      "Epoch[0], Val loss: 0.6956981420516968\n",
      "Epoch[0], Batch[157], Train loss: 0.8207491636276245\n",
      "Epoch[0], Val loss: 0.7103519439697266\n",
      "Epoch[0], Batch[158], Train loss: 0.793380618095398\n",
      "Epoch[0], Val loss: 0.6826722025871277\n",
      "Epoch[0], Batch[159], Train loss: 0.7996890544891357\n",
      "Epoch[0], Val loss: 0.6729995608329773\n",
      "Epoch[0], Batch[160], Train loss: 0.7970332503318787\n",
      "Epoch[0], Val loss: 0.662540853023529\n",
      "Epoch[0], Batch[161], Train loss: 0.7750371098518372\n",
      "Epoch[0], Val loss: 0.6652324795722961\n",
      "Epoch[0], Batch[162], Train loss: 0.790941059589386\n",
      "Epoch[0], Val loss: 0.6539364457130432\n",
      "Epoch[0], Batch[163], Train loss: 0.7706555724143982\n",
      "Epoch[0], Val loss: 0.6295660734176636\n",
      "Epoch[0], Batch[164], Train loss: 0.7448763251304626\n",
      "Epoch[0], Val loss: 0.6414362788200378\n",
      "Epoch[0], Batch[165], Train loss: 0.7381328344345093\n",
      "Epoch[0], Val loss: 0.6148865222930908\n",
      "Epoch[0], Batch[166], Train loss: 0.7272089123725891\n",
      "Epoch[0], Val loss: 0.6356467604637146\n",
      "Epoch[0], Batch[167], Train loss: 0.7353774905204773\n",
      "Epoch[0], Val loss: 0.6213409304618835\n",
      "Epoch[0], Batch[168], Train loss: 0.7274835705757141\n",
      "Epoch[0], Val loss: 0.625373125076294\n",
      "Epoch[0], Batch[169], Train loss: 0.7122231125831604\n",
      "Epoch[0], Val loss: 0.5813648104667664\n",
      "Epoch[0], Batch[170], Train loss: 0.7033714652061462\n",
      "Epoch[0], Val loss: 0.5822816491127014\n",
      "Epoch[0], Batch[171], Train loss: 0.6947631239891052\n",
      "Epoch[0], Val loss: 0.5867623090744019\n",
      "Epoch[0], Batch[172], Train loss: 0.6995019316673279\n",
      "Epoch[0], Val loss: 0.540767252445221\n",
      "Epoch[0], Batch[173], Train loss: 0.6563771963119507\n",
      "Epoch[0], Val loss: 0.5733924508094788\n",
      "Epoch[0], Batch[174], Train loss: 0.6795478463172913\n",
      "Epoch[0], Val loss: 0.5598915815353394\n",
      "Epoch[0], Batch[175], Train loss: 0.6652691960334778\n",
      "Epoch[0], Val loss: 0.551726222038269\n",
      "Epoch[0], Batch[176], Train loss: 0.655335009098053\n",
      "Epoch[0], Val loss: 0.5513991117477417\n",
      "Epoch[0], Batch[177], Train loss: 0.6381639242172241\n",
      "Epoch[0], Val loss: 0.5347145795822144\n",
      "Epoch[0], Batch[178], Train loss: 0.6470510959625244\n",
      "Epoch[0], Val loss: 0.5157545804977417\n",
      "Epoch[0], Batch[179], Train loss: 0.6389963030815125\n",
      "Epoch[0], Val loss: 0.5356162786483765\n",
      "Epoch[0], Batch[180], Train loss: 0.6319668292999268\n",
      "Epoch[0], Val loss: 0.5159704089164734\n",
      "Epoch[0], Batch[181], Train loss: 0.6235565543174744\n",
      "Epoch[0], Val loss: 0.5035937428474426\n",
      "Epoch[0], Batch[182], Train loss: 0.6141204833984375\n",
      "Epoch[0], Val loss: 0.4918203055858612\n",
      "Epoch[0], Batch[183], Train loss: 0.5938965678215027\n",
      "Epoch[0], Val loss: 0.5026044249534607\n",
      "Epoch[0], Batch[184], Train loss: 0.6080456376075745\n",
      "Epoch[0], Val loss: 0.4985145628452301\n",
      "Epoch[0], Batch[185], Train loss: 0.5976864695549011\n",
      "Epoch[0], Val loss: 0.47699174284935\n",
      "Epoch[0], Batch[186], Train loss: 0.6081019043922424\n",
      "Epoch[0], Val loss: 0.4970485270023346\n",
      "Epoch[0], Batch[187], Train loss: 0.5805792212486267\n",
      "Epoch[0], Val loss: 0.46171438694000244\n",
      "Epoch[0], Batch[188], Train loss: 0.5683757066726685\n",
      "Epoch[0], Val loss: 0.45704683661460876\n",
      "Epoch[0], Batch[189], Train loss: 0.5758735537528992\n",
      "Epoch[0], Val loss: 0.4377928376197815\n",
      "Epoch[0], Batch[190], Train loss: 0.5535231828689575\n",
      "Epoch[0], Val loss: 0.463479220867157\n",
      "Epoch[0], Batch[191], Train loss: 0.5509437322616577\n",
      "Epoch[0], Val loss: 0.45468562841415405\n",
      "Epoch[0], Batch[192], Train loss: 0.5470548868179321\n",
      "Epoch[0], Val loss: 0.45036470890045166\n",
      "Epoch[0], Batch[193], Train loss: 0.5420864820480347\n",
      "Epoch[0], Val loss: 0.43732959032058716\n",
      "Epoch[0], Batch[194], Train loss: 0.5328220725059509\n",
      "Epoch[0], Val loss: 0.45914649963378906\n",
      "Epoch[0], Batch[195], Train loss: 0.5392611622810364\n",
      "Epoch[0], Val loss: 0.43932053446769714\n",
      "Epoch[0], Batch[196], Train loss: 0.5346022844314575\n",
      "Epoch[0], Val loss: 0.42258158326148987\n",
      "Epoch[0], Batch[197], Train loss: 0.5158028602600098\n",
      "Epoch[0], Val loss: 0.42463403940200806\n",
      "Epoch[0], Batch[198], Train loss: 0.5441335439682007\n",
      "Epoch[0], Val loss: 0.42963454127311707\n",
      "Epoch[0], Batch[199], Train loss: 0.5049477815628052\n",
      "Epoch[0], Val loss: 0.41374558210372925\n",
      "Epoch[0], Batch[200], Train loss: 0.5236364603042603\n",
      "Epoch[0], Val loss: 0.4042193591594696\n",
      "Epoch[0], Batch[201], Train loss: 0.5212231278419495\n",
      "Epoch[0], Val loss: 0.41865602135658264\n",
      "Epoch[0], Batch[202], Train loss: 0.4980941414833069\n",
      "Epoch[0], Val loss: 0.39487263560295105\n",
      "Epoch[0], Batch[203], Train loss: 0.4992254674434662\n",
      "Epoch[0], Val loss: 0.41167929768562317\n",
      "Epoch[0], Batch[204], Train loss: 0.515285074710846\n",
      "Epoch[0], Val loss: 0.3902197778224945\n",
      "Epoch[0], Batch[205], Train loss: 0.4742286801338196\n",
      "Epoch[0], Val loss: 0.38235288858413696\n",
      "Epoch[0], Batch[206], Train loss: 0.4908261001110077\n",
      "Epoch[0], Val loss: 0.40797364711761475\n",
      "Epoch[0], Batch[207], Train loss: 0.4875204265117645\n",
      "Epoch[0], Val loss: 0.3796471357345581\n",
      "Epoch[0], Batch[208], Train loss: 0.47659432888031006\n",
      "Epoch[0], Val loss: 0.364683598279953\n",
      "Epoch[0], Batch[209], Train loss: 0.4819452166557312\n",
      "Epoch[0], Val loss: 0.40633007884025574\n",
      "Epoch[0], Batch[210], Train loss: 0.48499199748039246\n",
      "Epoch[0], Val loss: 0.3816121220588684\n",
      "Epoch[0], Batch[211], Train loss: 0.4699079096317291\n",
      "Epoch[0], Val loss: 0.3697614073753357\n",
      "Epoch[0], Batch[212], Train loss: 0.4713759124279022\n",
      "Epoch[0], Val loss: 0.3659721612930298\n",
      "Epoch[0], Batch[213], Train loss: 0.470056414604187\n",
      "Epoch[0], Val loss: 0.33627691864967346\n",
      "Epoch[0], Batch[214], Train loss: 0.43271392583847046\n",
      "Epoch[0], Val loss: 0.35596540570259094\n",
      "Epoch[0], Batch[215], Train loss: 0.4416410028934479\n",
      "Epoch[0], Val loss: 0.36055830121040344\n",
      "Epoch[0], Batch[216], Train loss: 0.4245380461215973\n",
      "Epoch[0], Val loss: 0.353180468082428\n",
      "Epoch[0], Batch[217], Train loss: 0.4315541088581085\n",
      "Epoch[0], Val loss: 0.34546032547950745\n",
      "Epoch[0], Batch[218], Train loss: 0.4479418098926544\n",
      "Epoch[0], Val loss: 0.3446905016899109\n",
      "Epoch[0], Batch[219], Train loss: 0.456617146730423\n",
      "Epoch[0], Val loss: 0.32506364583969116\n",
      "Epoch[0], Batch[220], Train loss: 0.42326176166534424\n",
      "Epoch[0], Val loss: 0.31962472200393677\n",
      "Epoch[0], Batch[221], Train loss: 0.4127182960510254\n",
      "Epoch[0], Val loss: 0.3386918604373932\n",
      "Epoch[0], Batch[222], Train loss: 0.43408849835395813\n",
      "Epoch[0], Val loss: 0.3279075622558594\n",
      "Epoch[0], Batch[223], Train loss: 0.4066327214241028\n",
      "Epoch[0], Val loss: 0.30980420112609863\n",
      "Epoch[0], Batch[224], Train loss: 0.39175650477409363\n",
      "Epoch[0], Val loss: 0.32265645265579224\n",
      "Epoch[0], Batch[225], Train loss: 0.40631505846977234\n",
      "Epoch[0], Val loss: 0.34870266914367676\n",
      "Epoch[0], Batch[226], Train loss: 0.4318801164627075\n",
      "Epoch[0], Val loss: 0.3192737400531769\n",
      "Epoch[0], Batch[227], Train loss: 0.3758913278579712\n",
      "Epoch[0], Val loss: 0.30739259719848633\n",
      "Epoch[0], Batch[228], Train loss: 0.39039695262908936\n",
      "Epoch[0], Val loss: 0.32110166549682617\n",
      "Epoch[0], Batch[229], Train loss: 0.4075744152069092\n",
      "Epoch[0], Val loss: 0.33661937713623047\n",
      "Epoch[0], Batch[230], Train loss: 0.40351402759552\n",
      "Epoch[0], Val loss: 0.3103172779083252\n",
      "Epoch[0], Batch[231], Train loss: 0.3905472159385681\n",
      "Epoch[0], Val loss: 0.30194607377052307\n",
      "Epoch[0], Batch[232], Train loss: 0.3878587484359741\n",
      "Epoch[0], Val loss: 0.2999504506587982\n",
      "Epoch[0], Batch[233], Train loss: 0.359092652797699\n",
      "Epoch[0], Val loss: 0.3093930780887604\n",
      "Epoch[0], Batch[234], Train loss: 0.36321723461151123\n",
      "Epoch[0], Val loss: 0.2885274589061737\n",
      "Epoch[0], Batch[235], Train loss: 0.38380321860313416\n",
      "Epoch[0], Val loss: 0.3032660186290741\n",
      "Epoch[0], Batch[236], Train loss: 0.36120694875717163\n",
      "Epoch[0], Val loss: 0.3040044903755188\n",
      "Epoch[0], Batch[237], Train loss: 0.3637281656265259\n",
      "Epoch[0], Val loss: 0.27845749258995056\n",
      "Epoch[0], Batch[238], Train loss: 0.3738219738006592\n",
      "Epoch[0], Val loss: 0.29465916752815247\n",
      "Epoch[0], Batch[239], Train loss: 0.3604613244533539\n",
      "Epoch[0], Val loss: 0.2893187701702118\n",
      "Epoch[0], Batch[240], Train loss: 0.3645717203617096\n",
      "Epoch[0], Val loss: 0.2690587639808655\n",
      "Epoch[0], Batch[241], Train loss: 0.36417773365974426\n",
      "Epoch[0], Val loss: 0.2924743592739105\n",
      "Epoch[0], Batch[242], Train loss: 0.3522654175758362\n",
      "Epoch[0], Val loss: 0.2978583574295044\n",
      "Epoch[0], Batch[243], Train loss: 0.3540225028991699\n",
      "Epoch[0], Val loss: 0.28417178988456726\n",
      "Epoch[0], Batch[244], Train loss: 0.35080432891845703\n",
      "Epoch[0], Val loss: 0.28358137607574463\n",
      "Epoch[0], Batch[245], Train loss: 0.34100696444511414\n",
      "Epoch[0], Val loss: 0.28942686319351196\n",
      "Epoch[0], Batch[246], Train loss: 0.3280465602874756\n",
      "Epoch[0], Val loss: 0.2738783359527588\n",
      "Epoch[0], Batch[247], Train loss: 0.342692106962204\n",
      "Epoch[0], Val loss: 0.24864356219768524\n",
      "Epoch[0], Batch[248], Train loss: 0.3465769290924072\n",
      "Epoch[0], Val loss: 0.2837367355823517\n",
      "Epoch[0], Batch[249], Train loss: 0.3501318395137787\n",
      "Epoch[0], Val loss: 0.2582082152366638\n",
      "Epoch[0], Batch[250], Train loss: 0.33081528544425964\n",
      "Epoch[0], Val loss: 0.2705206871032715\n",
      "Epoch[0], Batch[251], Train loss: 0.3192727267742157\n",
      "Epoch[0], Val loss: 0.24727515876293182\n",
      "Epoch[0], Batch[252], Train loss: 0.30992940068244934\n",
      "Epoch[0], Val loss: 0.27693647146224976\n",
      "Epoch[0], Batch[253], Train loss: 0.33063194155693054\n",
      "Epoch[0], Val loss: 0.26208579540252686\n",
      "Epoch[0], Batch[254], Train loss: 0.3339877128601074\n",
      "Epoch[0], Val loss: 0.2530106008052826\n",
      "Epoch[0], Batch[255], Train loss: 0.333869069814682\n",
      "Epoch[0], Val loss: 0.2570474445819855\n",
      "Epoch[0], Batch[256], Train loss: 0.3122701048851013\n",
      "Epoch[0], Val loss: 0.23710615932941437\n",
      "Epoch[0], Batch[257], Train loss: 0.31755855679512024\n",
      "Epoch[0], Val loss: 0.2415091097354889\n",
      "Epoch[0], Batch[258], Train loss: 0.3065814673900604\n",
      "Epoch[0], Val loss: 0.23846712708473206\n",
      "Epoch[0], Batch[259], Train loss: 0.3050838112831116\n",
      "Epoch[0], Val loss: 0.24679343402385712\n",
      "Epoch[0], Batch[260], Train loss: 0.3316124677658081\n",
      "Epoch[0], Val loss: 0.24254180490970612\n",
      "Epoch[0], Batch[261], Train loss: 0.3185657858848572\n",
      "Epoch[0], Val loss: 0.24772176146507263\n",
      "Epoch[0], Batch[262], Train loss: 0.30606818199157715\n",
      "Epoch[0], Val loss: 0.24549449980258942\n",
      "Epoch[0], Batch[263], Train loss: 0.317777156829834\n",
      "Epoch[0], Val loss: 0.23922303318977356\n",
      "Epoch[0], Batch[264], Train loss: 0.29270246624946594\n",
      "Epoch[0], Val loss: 0.23328058421611786\n",
      "Epoch[0], Batch[265], Train loss: 0.3078543245792389\n",
      "Epoch[0], Val loss: 0.23689685761928558\n",
      "Epoch[0], Batch[266], Train loss: 0.2981100380420685\n",
      "Epoch[0], Val loss: 0.21840575337409973\n",
      "Epoch[0], Batch[267], Train loss: 0.2908116281032562\n",
      "Epoch[0], Val loss: 0.2161886841058731\n",
      "Epoch[0], Batch[268], Train loss: 0.29032886028289795\n",
      "Epoch[0], Val loss: 0.23181824386119843\n",
      "Epoch[0], Batch[269], Train loss: 0.28526437282562256\n",
      "Epoch[0], Val loss: 0.2142476886510849\n",
      "Epoch[0], Batch[270], Train loss: 0.3116208612918854\n",
      "Epoch[0], Val loss: 0.20734311640262604\n",
      "Epoch[0], Batch[271], Train loss: 0.3108612895011902\n",
      "Epoch[0], Val loss: 0.20529288053512573\n",
      "Epoch[0], Batch[272], Train loss: 0.2863403558731079\n",
      "Epoch[0], Val loss: 0.21980847418308258\n",
      "Epoch[0], Batch[273], Train loss: 0.2885604202747345\n",
      "Epoch[0], Val loss: 0.22780495882034302\n",
      "Epoch[0], Batch[274], Train loss: 0.2880234718322754\n",
      "Epoch[0], Val loss: 0.233476459980011\n",
      "Epoch[0], Batch[275], Train loss: 0.28514689207077026\n",
      "Epoch[0], Val loss: 0.21138446033000946\n",
      "Epoch[0], Batch[276], Train loss: 0.24918878078460693\n",
      "Epoch[0], Val loss: 0.2185775637626648\n",
      "Epoch[0], Batch[277], Train loss: 0.27079203724861145\n",
      "Epoch[0], Val loss: 0.2252708077430725\n",
      "Epoch[0], Batch[278], Train loss: 0.26802510023117065\n",
      "Epoch[0], Val loss: 0.20644155144691467\n",
      "Epoch[0], Batch[279], Train loss: 0.26749494671821594\n",
      "Epoch[0], Val loss: 0.22378647327423096\n",
      "Epoch[0], Batch[280], Train loss: 0.2690882384777069\n",
      "Epoch[0], Val loss: 0.2000550925731659\n",
      "Epoch[0], Batch[281], Train loss: 0.27091696858406067\n",
      "Epoch[0], Val loss: 0.20166578888893127\n",
      "Epoch[0], Batch[282], Train loss: 0.29418203234672546\n",
      "Epoch[0], Val loss: 0.1901572197675705\n",
      "Epoch[0], Batch[283], Train loss: 0.2543967068195343\n",
      "Epoch[0], Val loss: 0.21422332525253296\n",
      "Epoch[0], Batch[284], Train loss: 0.2547783851623535\n",
      "Epoch[0], Val loss: 0.2010350376367569\n",
      "Epoch[0], Batch[285], Train loss: 0.2558443248271942\n",
      "Epoch[0], Val loss: 0.20920392870903015\n",
      "Epoch[0], Batch[286], Train loss: 0.24669697880744934\n",
      "Epoch[0], Val loss: 0.19636522233486176\n",
      "Epoch[0], Batch[287], Train loss: 0.2615087032318115\n",
      "Epoch[0], Val loss: 0.19888851046562195\n",
      "Epoch[0], Batch[288], Train loss: 0.2566426694393158\n",
      "Epoch[0], Val loss: 0.1949702799320221\n",
      "Epoch[0], Batch[289], Train loss: 0.24976879358291626\n",
      "Epoch[0], Val loss: 0.1919330656528473\n",
      "Epoch[0], Batch[290], Train loss: 0.253018856048584\n",
      "Epoch[0], Val loss: 0.19869130849838257\n",
      "Epoch[0], Batch[291], Train loss: 0.2432592511177063\n",
      "Epoch[0], Val loss: 0.18293820321559906\n",
      "Epoch[0], Batch[292], Train loss: 0.2679237127304077\n",
      "Epoch[0], Val loss: 0.19653768837451935\n",
      "Epoch[0], Batch[293], Train loss: 0.25872132182121277\n",
      "Epoch[0], Val loss: 0.19861790537834167\n",
      "Epoch[0], Batch[294], Train loss: 0.2452925443649292\n",
      "Epoch[0], Val loss: 0.20540456473827362\n",
      "Epoch[0], Batch[295], Train loss: 0.25255829095840454\n",
      "Epoch[0], Val loss: 0.2012961506843567\n",
      "Epoch[0], Batch[296], Train loss: 0.2561086416244507\n",
      "Epoch[0], Val loss: 0.18118496239185333\n",
      "Epoch[0], Batch[297], Train loss: 0.2448836714029312\n",
      "Epoch[0], Val loss: 0.18323460221290588\n",
      "Epoch[0], Batch[298], Train loss: 0.2503325641155243\n",
      "Epoch[0], Val loss: 0.18155522644519806\n",
      "Epoch[0], Batch[299], Train loss: 0.22968971729278564\n",
      "Epoch[0], Val loss: 0.17556996643543243\n",
      "Epoch[0], Batch[300], Train loss: 0.24792242050170898\n",
      "Epoch[0], Val loss: 0.18418273329734802\n",
      "Epoch[0], Batch[301], Train loss: 0.25048792362213135\n",
      "Epoch[0], Val loss: 0.19563575088977814\n",
      "Epoch[0], Batch[302], Train loss: 0.23159334063529968\n",
      "Epoch[0], Val loss: 0.18075212836265564\n",
      "Epoch[0], Batch[303], Train loss: 0.22759892046451569\n",
      "Epoch[0], Val loss: 0.19711527228355408\n",
      "Epoch[0], Batch[304], Train loss: 0.21854691207408905\n",
      "Epoch[0], Val loss: 0.1921968162059784\n",
      "Epoch[0], Batch[305], Train loss: 0.2438579648733139\n",
      "Epoch[0], Val loss: 0.18383020162582397\n",
      "Epoch[0], Batch[306], Train loss: 0.23053206503391266\n",
      "Epoch[0], Val loss: 0.17754104733467102\n",
      "Epoch[0], Batch[307], Train loss: 0.21982339024543762\n",
      "Epoch[0], Val loss: 0.17937515676021576\n",
      "Epoch[0], Batch[308], Train loss: 0.232986181974411\n",
      "Epoch[0], Val loss: 0.17526882886886597\n",
      "Epoch[0], Batch[309], Train loss: 0.22375673055648804\n",
      "Epoch[0], Val loss: 0.17805376648902893\n",
      "Epoch[0], Batch[310], Train loss: 0.21578282117843628\n",
      "Epoch[0], Val loss: 0.18552665412425995\n",
      "Epoch[0], Batch[311], Train loss: 0.23416800796985626\n",
      "Epoch[0], Val loss: 0.1848442256450653\n",
      "Epoch[0], Batch[312], Train loss: 0.23755039274692535\n",
      "Epoch[0], Val loss: 0.15241345763206482\n",
      "Epoch[0], Batch[313], Train loss: 0.21691273152828217\n",
      "Epoch[0], Val loss: 0.1776258647441864\n",
      "Epoch[0], Batch[314], Train loss: 0.22562628984451294\n",
      "Epoch[0], Val loss: 0.1784002184867859\n",
      "Epoch[0], Batch[315], Train loss: 0.20702014863491058\n",
      "Epoch[0], Val loss: 0.15798616409301758\n",
      "Epoch[0], Batch[316], Train loss: 0.2148003876209259\n",
      "Epoch[0], Val loss: 0.16961930692195892\n",
      "Epoch[0], Batch[317], Train loss: 0.20366132259368896\n",
      "Epoch[0], Val loss: 0.15630578994750977\n",
      "Epoch[0], Batch[318], Train loss: 0.21007944643497467\n",
      "Epoch[0], Val loss: 0.16313835978507996\n",
      "Epoch[0], Batch[319], Train loss: 0.22830620408058167\n",
      "Epoch[0], Val loss: 0.16287119686603546\n",
      "Epoch[0], Batch[320], Train loss: 0.2141539454460144\n",
      "Epoch[0], Val loss: 0.1637575477361679\n",
      "Epoch[0], Batch[321], Train loss: 0.21781288087368011\n",
      "Epoch[0], Val loss: 0.16535501182079315\n",
      "Epoch[0], Batch[322], Train loss: 0.19643746316432953\n",
      "Epoch[0], Val loss: 0.1697034388780594\n",
      "Epoch[0], Batch[323], Train loss: 0.20374765992164612\n",
      "Epoch[0], Val loss: 0.1714993566274643\n",
      "Epoch[0], Batch[324], Train loss: 0.20091629028320312\n",
      "Epoch[0], Val loss: 0.14973224699497223\n",
      "Epoch[0], Batch[325], Train loss: 0.20664982497692108\n",
      "Epoch[0], Val loss: 0.1634817123413086\n",
      "Epoch[0], Batch[326], Train loss: 0.20243574678897858\n",
      "Epoch[0], Val loss: 0.15942062437534332\n",
      "Epoch[0], Batch[327], Train loss: 0.21282410621643066\n",
      "Epoch[0], Val loss: 0.1594441831111908\n",
      "Epoch[0], Batch[328], Train loss: 0.20185501873493195\n",
      "Epoch[0], Val loss: 0.14857535064220428\n",
      "Epoch[0], Batch[329], Train loss: 0.21752817928791046\n",
      "Epoch[0], Val loss: 0.1545257568359375\n",
      "Epoch[0], Batch[330], Train loss: 0.20080822706222534\n",
      "Epoch[0], Val loss: 0.15600283443927765\n",
      "Epoch[0], Batch[331], Train loss: 0.18665796518325806\n",
      "Epoch[0], Val loss: 0.16591469943523407\n",
      "Epoch[0], Batch[332], Train loss: 0.19617986679077148\n",
      "Epoch[0], Val loss: 0.15262529253959656\n",
      "Epoch[0], Batch[333], Train loss: 0.20870181918144226\n",
      "Epoch[0], Val loss: 0.15789872407913208\n",
      "Epoch[0], Batch[334], Train loss: 0.2016150802373886\n",
      "Epoch[0], Val loss: 0.1506405621767044\n",
      "Epoch[0], Batch[335], Train loss: 0.2048126608133316\n",
      "Epoch[0], Val loss: 0.150208979845047\n",
      "Epoch[0], Batch[336], Train loss: 0.20233336091041565\n",
      "Epoch[0], Val loss: 0.1436808854341507\n",
      "Epoch[0], Batch[337], Train loss: 0.1944841742515564\n",
      "Epoch[0], Val loss: 0.13906905055046082\n",
      "Epoch[0], Batch[338], Train loss: 0.186000257730484\n",
      "Epoch[0], Val loss: 0.14655110239982605\n",
      "Epoch[0], Batch[339], Train loss: 0.1937834918498993\n",
      "Epoch[0], Val loss: 0.16021355986595154\n",
      "Epoch[0], Batch[340], Train loss: 0.18208558857440948\n",
      "Epoch[0], Val loss: 0.15800081193447113\n",
      "Epoch[0], Batch[341], Train loss: 0.18522211909294128\n",
      "Epoch[0], Val loss: 0.14704813063144684\n",
      "Epoch[0], Batch[342], Train loss: 0.17500104010105133\n",
      "Epoch[0], Val loss: 0.15151333808898926\n",
      "Epoch[0], Batch[343], Train loss: 0.18604573607444763\n",
      "Epoch[0], Val loss: 0.14113272726535797\n",
      "Epoch[0], Batch[344], Train loss: 0.1953948587179184\n",
      "Epoch[0], Val loss: 0.15638624131679535\n",
      "Epoch[0], Batch[345], Train loss: 0.18700093030929565\n",
      "Epoch[0], Val loss: 0.14991949498653412\n",
      "Epoch[0], Batch[346], Train loss: 0.19141265749931335\n",
      "Epoch[0], Val loss: 0.14042189717292786\n",
      "Epoch[0], Batch[347], Train loss: 0.19207511842250824\n",
      "Epoch[0], Val loss: 0.14818723499774933\n",
      "Epoch[0], Batch[348], Train loss: 0.18776355683803558\n",
      "Epoch[0], Val loss: 0.14914685487747192\n",
      "Epoch[0], Batch[349], Train loss: 0.17982710897922516\n",
      "Epoch[0], Val loss: 0.13803592324256897\n",
      "Epoch[0], Batch[350], Train loss: 0.18358197808265686\n",
      "Epoch[0], Val loss: 0.13857780396938324\n",
      "Epoch[0], Batch[351], Train loss: 0.18093712627887726\n",
      "Epoch[0], Val loss: 0.1464441418647766\n",
      "Epoch[0], Batch[352], Train loss: 0.18689830601215363\n",
      "Epoch[0], Val loss: 0.13945795595645905\n",
      "Epoch[0], Batch[353], Train loss: 0.18772289156913757\n",
      "Epoch[0], Val loss: 0.13767588138580322\n",
      "Epoch[0], Batch[354], Train loss: 0.18324407935142517\n",
      "Epoch[0], Val loss: 0.13635726273059845\n",
      "Epoch[0], Batch[355], Train loss: 0.18587592244148254\n",
      "Epoch[0], Val loss: 0.1385444849729538\n",
      "Epoch[0], Batch[356], Train loss: 0.18080174922943115\n",
      "Epoch[0], Val loss: 0.13567331433296204\n",
      "Epoch[0], Batch[357], Train loss: 0.17686018347740173\n",
      "Epoch[0], Val loss: 0.13623102009296417\n",
      "Epoch[0], Batch[358], Train loss: 0.15876279771327972\n",
      "Epoch[0], Val loss: 0.13792474567890167\n",
      "Epoch[0], Batch[359], Train loss: 0.1730232685804367\n",
      "Epoch[0], Val loss: 0.14212468266487122\n",
      "Epoch[0], Batch[360], Train loss: 0.17963577806949615\n",
      "Epoch[0], Val loss: 0.1470979005098343\n",
      "Epoch[0], Batch[361], Train loss: 0.17900492250919342\n",
      "Epoch[0], Val loss: 0.14006218314170837\n",
      "Epoch[0], Batch[362], Train loss: 0.16283676028251648\n",
      "Epoch[0], Val loss: 0.12880869209766388\n",
      "Epoch[0], Batch[363], Train loss: 0.1780996024608612\n",
      "Epoch[0], Val loss: 0.13591714203357697\n",
      "Epoch[0], Batch[364], Train loss: 0.17116092145442963\n",
      "Epoch[0], Val loss: 0.14270423352718353\n",
      "Epoch[0], Batch[365], Train loss: 0.1734195053577423\n",
      "Epoch[0], Val loss: 0.1314680129289627\n",
      "Epoch[0], Batch[366], Train loss: 0.1599956601858139\n",
      "Epoch[0], Val loss: 0.13077892363071442\n",
      "Epoch[0], Batch[367], Train loss: 0.17326906323432922\n",
      "Epoch[0], Val loss: 0.14027270674705505\n",
      "Epoch[0], Batch[368], Train loss: 0.16360415518283844\n",
      "Epoch[0], Val loss: 0.13331018388271332\n",
      "Epoch[0], Batch[369], Train loss: 0.1632428765296936\n",
      "Epoch[0], Val loss: 0.12655891478061676\n",
      "Epoch[0], Batch[370], Train loss: 0.18016588687896729\n",
      "Epoch[0], Val loss: 0.13123202323913574\n",
      "Epoch[0], Batch[371], Train loss: 0.16440939903259277\n",
      "Epoch[0], Val loss: 0.13851702213287354\n",
      "Epoch[0], Batch[372], Train loss: 0.1672695279121399\n",
      "Epoch[0], Val loss: 0.13036251068115234\n",
      "Epoch[0], Batch[373], Train loss: 0.17084363102912903\n",
      "Epoch[0], Val loss: 0.1312578171491623\n",
      "Epoch[0], Batch[374], Train loss: 0.17188787460327148\n",
      "Epoch[0], Val loss: 0.12728512287139893\n",
      "Epoch[0], Batch[375], Train loss: 0.16860775649547577\n",
      "Epoch[0], Val loss: 0.1308508813381195\n",
      "Epoch[0], Batch[376], Train loss: 0.16878734529018402\n",
      "Epoch[0], Val loss: 0.1307234913110733\n",
      "Epoch[0], Batch[377], Train loss: 0.1602458357810974\n",
      "Epoch[0], Val loss: 0.13388817012310028\n",
      "Epoch[0], Batch[378], Train loss: 0.17045627534389496\n",
      "Epoch[0], Val loss: 0.13192209601402283\n",
      "Epoch[0], Batch[379], Train loss: 0.16715078055858612\n",
      "Epoch[0], Val loss: 0.12226535379886627\n",
      "Epoch[0], Batch[380], Train loss: 0.16238227486610413\n",
      "Epoch[0], Val loss: 0.13302519917488098\n",
      "Epoch[0], Batch[381], Train loss: 0.17521964013576508\n",
      "Epoch[0], Val loss: 0.11603580415248871\n",
      "Epoch[0], Batch[382], Train loss: 0.1639198511838913\n",
      "Epoch[0], Val loss: 0.1230793222784996\n",
      "Epoch[0], Batch[383], Train loss: 0.15339745581150055\n",
      "Epoch[0], Val loss: 0.11547024548053741\n",
      "Epoch[0], Batch[384], Train loss: 0.1568860113620758\n",
      "Epoch[0], Val loss: 0.1238575279712677\n",
      "Epoch[0], Batch[385], Train loss: 0.1537935435771942\n",
      "Epoch[0], Val loss: 0.12386824190616608\n",
      "Epoch[0], Batch[386], Train loss: 0.1587245613336563\n",
      "Epoch[0], Val loss: 0.12704172730445862\n",
      "Epoch[0], Batch[387], Train loss: 0.15050089359283447\n",
      "Epoch[0], Val loss: 0.11980914324522018\n",
      "Epoch[0], Batch[388], Train loss: 0.13824158906936646\n",
      "Epoch[0], Val loss: 0.11610317975282669\n",
      "Epoch[0], Batch[389], Train loss: 0.15150186419487\n",
      "Epoch[0], Val loss: 0.11787918210029602\n",
      "Epoch[0], Batch[390], Train loss: 0.1530548632144928\n",
      "Epoch[0], Val loss: 0.11397146433591843\n",
      "Epoch[0], Batch[391], Train loss: 0.157778799533844\n",
      "Epoch[0], Val loss: 0.11995939910411835\n",
      "Epoch[0], Batch[392], Train loss: 0.1459016352891922\n",
      "Epoch[0], Val loss: 0.12827780842781067\n",
      "Epoch[0], Batch[393], Train loss: 0.14903050661087036\n",
      "Epoch[0], Val loss: 0.11972423642873764\n",
      "Epoch[0], Batch[394], Train loss: 0.1463971883058548\n",
      "Epoch[0], Val loss: 0.118325375020504\n",
      "Epoch[0], Batch[395], Train loss: 0.14744624495506287\n",
      "Epoch[0], Val loss: 0.1171305850148201\n",
      "Epoch[0], Batch[396], Train loss: 0.15483801066875458\n",
      "Epoch[0], Val loss: 0.11161085218191147\n",
      "Epoch[0], Batch[397], Train loss: 0.14654649794101715\n",
      "Epoch[0], Val loss: 0.1167462095618248\n",
      "Epoch[0], Batch[398], Train loss: 0.15077029168605804\n",
      "Epoch[0], Val loss: 0.11107517033815384\n",
      "Epoch[0], Batch[399], Train loss: 0.15751446783542633\n",
      "Epoch[0], Val loss: 0.11617173254489899\n",
      "Epoch[0], Batch[400], Train loss: 0.15281865000724792\n",
      "Epoch[0], Val loss: 0.12083570659160614\n",
      "Epoch[0], Batch[401], Train loss: 0.14888599514961243\n",
      "Epoch[0], Val loss: 0.12090492248535156\n",
      "Epoch[0], Batch[402], Train loss: 0.14421924948692322\n",
      "Epoch[0], Val loss: 0.111027292907238\n",
      "Epoch[0], Batch[403], Train loss: 0.13909552991390228\n",
      "Epoch[0], Val loss: 0.1221076101064682\n",
      "Epoch[0], Batch[404], Train loss: 0.1486995369195938\n",
      "Epoch[0], Val loss: 0.11783657968044281\n",
      "Epoch[0], Batch[405], Train loss: 0.14315597712993622\n",
      "Epoch[0], Val loss: 0.11669231206178665\n",
      "Epoch[0], Batch[406], Train loss: 0.13892683386802673\n",
      "Epoch[0], Val loss: 0.11536587029695511\n",
      "Epoch[0], Batch[407], Train loss: 0.14040471613407135\n",
      "Epoch[0], Val loss: 0.115135557949543\n",
      "Epoch[0], Batch[408], Train loss: 0.146861270070076\n",
      "Epoch[0], Val loss: 0.10709871351718903\n",
      "Epoch[0], Batch[409], Train loss: 0.14188280701637268\n",
      "Epoch[0], Val loss: 0.1185157522559166\n",
      "Epoch[0], Batch[410], Train loss: 0.13780124485492706\n",
      "Epoch[0], Val loss: 0.11650305241346359\n",
      "Epoch[0], Batch[411], Train loss: 0.15008984506130219\n",
      "Epoch[0], Val loss: 0.10079074651002884\n",
      "Epoch[0], Batch[412], Train loss: 0.14810116589069366\n",
      "Epoch[0], Val loss: 0.11317501962184906\n",
      "Epoch[0], Batch[413], Train loss: 0.1286255270242691\n",
      "Epoch[0], Val loss: 0.10930828005075455\n",
      "Epoch[0], Batch[414], Train loss: 0.13011279702186584\n",
      "Epoch[0], Val loss: 0.11080444604158401\n",
      "Epoch[0], Batch[415], Train loss: 0.1458476483821869\n",
      "Epoch[0], Val loss: 0.10645968466997147\n",
      "Epoch[0], Batch[416], Train loss: 0.13850103318691254\n",
      "Epoch[0], Val loss: 0.105791375041008\n",
      "Epoch[0], Batch[417], Train loss: 0.13668295741081238\n",
      "Epoch[0], Val loss: 0.10830753296613693\n",
      "Epoch[0], Batch[418], Train loss: 0.13426880538463593\n",
      "Epoch[0], Val loss: 0.11294234544038773\n",
      "Epoch[0], Batch[419], Train loss: 0.13121776282787323\n",
      "Epoch[0], Val loss: 0.11303942650556564\n",
      "Epoch[0], Batch[420], Train loss: 0.13729549944400787\n",
      "Epoch[0], Val loss: 0.10734152793884277\n",
      "Epoch[0], Batch[421], Train loss: 0.15054301917552948\n",
      "Epoch[0], Val loss: 0.11601628363132477\n",
      "Epoch[0], Batch[422], Train loss: 0.13115277886390686\n",
      "Epoch[0], Val loss: 0.11623702198266983\n",
      "Epoch[0], Batch[423], Train loss: 0.14557847380638123\n",
      "Epoch[0], Val loss: 0.1011049896478653\n",
      "Epoch[0], Batch[424], Train loss: 0.13304565846920013\n",
      "Epoch[0], Val loss: 0.10343513637781143\n",
      "Epoch[0], Batch[425], Train loss: 0.13928574323654175\n",
      "Epoch[0], Val loss: 0.10686852037906647\n",
      "Epoch[0], Batch[426], Train loss: 0.13855665922164917\n",
      "Epoch[0], Val loss: 0.10182105749845505\n",
      "Epoch[0], Batch[427], Train loss: 0.1367010623216629\n",
      "Epoch[0], Val loss: 0.12129329890012741\n",
      "Epoch[0], Batch[428], Train loss: 0.14183712005615234\n",
      "Epoch[0], Val loss: 0.10434193909168243\n",
      "Epoch[0], Batch[429], Train loss: 0.13525882363319397\n",
      "Epoch[0], Val loss: 0.10530799627304077\n",
      "Epoch[0], Batch[430], Train loss: 0.13568878173828125\n",
      "Epoch[0], Val loss: 0.10381220281124115\n",
      "Epoch[0], Batch[431], Train loss: 0.1333301067352295\n",
      "Epoch[0], Val loss: 0.09656723588705063\n",
      "Epoch[0], Batch[432], Train loss: 0.13500584661960602\n",
      "Epoch[0], Val loss: 0.10399799048900604\n",
      "Epoch[0], Batch[433], Train loss: 0.13589882850646973\n",
      "Epoch[0], Val loss: 0.10573408007621765\n",
      "Epoch[0], Batch[434], Train loss: 0.13295984268188477\n",
      "Epoch[0], Val loss: 0.10323415696620941\n",
      "Epoch[0], Batch[435], Train loss: 0.13480262458324432\n",
      "Epoch[0], Val loss: 0.10616660863161087\n",
      "Epoch[0], Batch[436], Train loss: 0.12928660213947296\n",
      "Epoch[0], Val loss: 0.1007346510887146\n",
      "Epoch[0], Batch[437], Train loss: 0.1346421241760254\n",
      "Epoch[0], Val loss: 0.10820457339286804\n",
      "Epoch[0], Batch[438], Train loss: 0.13093651831150055\n",
      "Epoch[0], Val loss: 0.10535378754138947\n",
      "Epoch[0], Batch[439], Train loss: 0.12682583928108215\n",
      "Epoch[0], Val loss: 0.10193352401256561\n",
      "Epoch[0], Batch[440], Train loss: 0.12648457288742065\n",
      "Epoch[0], Val loss: 0.10441458225250244\n",
      "Epoch[0], Batch[441], Train loss: 0.1235610619187355\n",
      "Epoch[0], Val loss: 0.0945054218173027\n",
      "Epoch[0], Batch[442], Train loss: 0.13614189624786377\n",
      "Epoch[0], Val loss: 0.09585560858249664\n",
      "Epoch[0], Batch[443], Train loss: 0.129246324300766\n",
      "Epoch[0], Val loss: 0.09663138538599014\n",
      "Epoch[0], Batch[444], Train loss: 0.12753033638000488\n",
      "Epoch[0], Val loss: 0.1003049910068512\n",
      "Epoch[0], Batch[445], Train loss: 0.13020582497119904\n",
      "Epoch[0], Val loss: 0.10342949628829956\n",
      "Epoch[0], Batch[446], Train loss: 0.12930545210838318\n",
      "Epoch[0], Val loss: 0.10308131575584412\n",
      "Epoch[0], Batch[447], Train loss: 0.1178312599658966\n",
      "Epoch[0], Val loss: 0.09161781519651413\n",
      "Epoch[0], Batch[448], Train loss: 0.12109300494194031\n",
      "Epoch[0], Val loss: 0.10390545427799225\n",
      "Epoch[0], Batch[449], Train loss: 0.1280541867017746\n",
      "Epoch[0], Val loss: 0.09504317492246628\n",
      "Epoch[0], Batch[450], Train loss: 0.13437607884407043\n",
      "Epoch[0], Val loss: 0.09423985332250595\n",
      "Epoch[0], Batch[451], Train loss: 0.1314513087272644\n",
      "Epoch[0], Val loss: 0.09982266277074814\n",
      "Epoch[0], Batch[452], Train loss: 0.12564560770988464\n",
      "Epoch[0], Val loss: 0.09766004979610443\n",
      "Epoch[0], Batch[453], Train loss: 0.12622898817062378\n",
      "Epoch[0], Val loss: 0.09534658491611481\n",
      "Epoch[0], Batch[454], Train loss: 0.12645074725151062\n",
      "Epoch[0], Val loss: 0.09372733533382416\n",
      "Epoch[0], Batch[455], Train loss: 0.12353923916816711\n",
      "Epoch[0], Val loss: 0.10144900530576706\n",
      "Epoch[0], Batch[456], Train loss: 0.12514963746070862\n",
      "Epoch[0], Val loss: 0.10083343833684921\n",
      "Epoch[0], Batch[457], Train loss: 0.12291748821735382\n",
      "Epoch[0], Val loss: 0.0938292145729065\n",
      "Epoch[0], Batch[458], Train loss: 0.1161922737956047\n",
      "Epoch[0], Val loss: 0.10405231267213821\n",
      "Epoch[0], Batch[459], Train loss: 0.1278664767742157\n",
      "Epoch[0], Val loss: 0.09340927749872208\n",
      "Epoch[0], Batch[460], Train loss: 0.1320837140083313\n",
      "Epoch[0], Val loss: 0.0952894389629364\n",
      "Epoch[0], Batch[461], Train loss: 0.11980560421943665\n",
      "Epoch[0], Val loss: 0.0959768295288086\n",
      "Epoch[0], Batch[462], Train loss: 0.11921601742506027\n",
      "Epoch[0], Val loss: 0.09641041606664658\n",
      "Epoch[0], Batch[463], Train loss: 0.117030568420887\n",
      "Epoch[0], Val loss: 0.09440936893224716\n",
      "Epoch[0], Batch[464], Train loss: 0.12206856906414032\n",
      "Epoch[0], Val loss: 0.09400089085102081\n",
      "Epoch[0], Batch[465], Train loss: 0.1211116686463356\n",
      "Epoch[0], Val loss: 0.09651941806077957\n",
      "Epoch[0], Batch[466], Train loss: 0.12015718221664429\n",
      "Epoch[0], Val loss: 0.09603795409202576\n",
      "Epoch[0], Batch[467], Train loss: 0.11435633897781372\n",
      "Epoch[0], Val loss: 0.09536241739988327\n",
      "Epoch[0], Batch[468], Train loss: 0.11871535331010818\n",
      "Epoch[0], Val loss: 0.09414014220237732\n",
      "Epoch[0], Batch[469], Train loss: 0.11981537938117981\n",
      "Epoch[0], Val loss: 0.0981963649392128\n",
      "Epoch[0], Batch[470], Train loss: 0.11525354534387589\n",
      "Epoch[0], Val loss: 0.08882642537355423\n",
      "Epoch[0], Batch[471], Train loss: 0.11357017606496811\n",
      "Epoch[0], Val loss: 0.09127345681190491\n",
      "Epoch[0], Batch[472], Train loss: 0.1157013475894928\n",
      "Epoch[0], Val loss: 0.09562115371227264\n",
      "Epoch[0], Batch[473], Train loss: 0.11544200032949448\n",
      "Epoch[0], Val loss: 0.09018559753894806\n",
      "Epoch[0], Batch[474], Train loss: 0.12537071108818054\n",
      "Epoch[0], Val loss: 0.09137828648090363\n",
      "Epoch[0], Batch[475], Train loss: 0.11679636687040329\n",
      "Epoch[0], Val loss: 0.09806884080171585\n",
      "Epoch[0], Batch[476], Train loss: 0.1128489300608635\n",
      "Epoch[0], Val loss: 0.08880000561475754\n",
      "Epoch[0], Batch[477], Train loss: 0.12141703814268112\n",
      "Epoch[0], Val loss: 0.08859462291002274\n",
      "Epoch[0], Batch[478], Train loss: 0.11617173999547958\n",
      "Epoch[0], Val loss: 0.0953219011425972\n",
      "Epoch[0], Batch[479], Train loss: 0.12066753953695297\n",
      "Epoch[0], Val loss: 0.08704980462789536\n",
      "Epoch[0], Batch[480], Train loss: 0.11950749158859253\n",
      "Epoch[0], Val loss: 0.09895109385251999\n",
      "Epoch[0], Batch[481], Train loss: 0.11596651375293732\n",
      "Epoch[0], Val loss: 0.09076369553804398\n",
      "Epoch[0], Batch[482], Train loss: 0.11545989662408829\n",
      "Epoch[0], Val loss: 0.0980185717344284\n",
      "Epoch[0], Batch[483], Train loss: 0.10810195654630661\n",
      "Epoch[0], Val loss: 0.0898674726486206\n",
      "Epoch[0], Batch[484], Train loss: 0.11352338641881943\n",
      "Epoch[0], Val loss: 0.08884842693805695\n",
      "Epoch[0], Batch[485], Train loss: 0.10969480872154236\n",
      "Epoch[0], Val loss: 0.09615689516067505\n",
      "Epoch[0], Batch[486], Train loss: 0.11703909188508987\n",
      "Epoch[0], Val loss: 0.09327917546033859\n",
      "Epoch[0], Batch[487], Train loss: 0.11907155811786652\n",
      "Epoch[0], Val loss: 0.08814950287342072\n",
      "Epoch[0], Batch[488], Train loss: 0.10945259034633636\n",
      "Epoch[0], Val loss: 0.09499074518680573\n",
      "Epoch[0], Batch[489], Train loss: 0.12093402445316315\n",
      "Epoch[0], Val loss: 0.0886060819029808\n",
      "Epoch[0], Batch[490], Train loss: 0.12257508188486099\n",
      "Epoch[0], Val loss: 0.08362580090761185\n",
      "Epoch[0], Batch[491], Train loss: 0.11275171488523483\n",
      "Epoch[0], Val loss: 0.09184993803501129\n",
      "Epoch[0], Batch[492], Train loss: 0.1111992746591568\n",
      "Epoch[0], Val loss: 0.08449889719486237\n",
      "Epoch[0], Batch[493], Train loss: 0.11546551436185837\n",
      "Epoch[0], Val loss: 0.08624586462974548\n",
      "Epoch[0], Batch[494], Train loss: 0.10260337591171265\n",
      "Epoch[0], Val loss: 0.08714374154806137\n",
      "Epoch[0], Batch[495], Train loss: 0.10776960849761963\n",
      "Epoch[0], Val loss: 0.08131776005029678\n",
      "Epoch[0], Batch[496], Train loss: 0.10643686354160309\n",
      "Epoch[0], Val loss: 0.08883220702409744\n",
      "Epoch[0], Batch[497], Train loss: 0.10543139278888702\n",
      "Epoch[0], Val loss: 0.09324733167886734\n",
      "Epoch[0], Batch[498], Train loss: 0.11405894160270691\n",
      "Epoch[0], Val loss: 0.08683624118566513\n",
      "Epoch[0], Batch[499], Train loss: 0.11309488117694855\n",
      "Epoch[0], Val loss: 0.08942851424217224\n",
      "Epoch[0], Batch[500], Train loss: 0.11293429881334305\n",
      "Epoch[0], Val loss: 0.0800551250576973\n",
      "Epoch[0], Batch[501], Train loss: 0.1034485325217247\n",
      "Epoch[0], Val loss: 0.08780518919229507\n",
      "Epoch[0], Batch[502], Train loss: 0.10863395780324936\n",
      "Epoch[0], Val loss: 0.08804845809936523\n",
      "Epoch[0], Batch[503], Train loss: 0.11520504206418991\n",
      "Epoch[0], Val loss: 0.08676657825708389\n",
      "Epoch[0], Batch[504], Train loss: 0.11804410070180893\n",
      "Epoch[0], Val loss: 0.08020643144845963\n",
      "Epoch[0], Batch[505], Train loss: 0.10598520189523697\n",
      "Epoch[0], Val loss: 0.09254848212003708\n",
      "Epoch[0], Batch[506], Train loss: 0.10318483412265778\n",
      "Epoch[0], Val loss: 0.08813183754682541\n",
      "Epoch[0], Batch[507], Train loss: 0.10742692649364471\n",
      "Epoch[0], Val loss: 0.08513421565294266\n",
      "Epoch[0], Batch[508], Train loss: 0.10968850553035736\n",
      "Epoch[0], Val loss: 0.0829978883266449\n",
      "Epoch[0], Batch[509], Train loss: 0.1047666147351265\n",
      "Epoch[0], Val loss: 0.08431092649698257\n",
      "Epoch[0], Batch[510], Train loss: 0.10824795812368393\n",
      "Epoch[0], Val loss: 0.09447944909334183\n",
      "Epoch[0], Batch[511], Train loss: 0.11164278537034988\n",
      "Epoch[0], Val loss: 0.08245185017585754\n",
      "Epoch[0], Batch[512], Train loss: 0.1026846170425415\n",
      "Epoch[0], Val loss: 0.08871390670537949\n",
      "Epoch[0], Batch[513], Train loss: 0.11441052705049515\n",
      "Epoch[0], Val loss: 0.08649254590272903\n",
      "Epoch[0], Batch[514], Train loss: 0.10573851317167282\n",
      "Epoch[0], Val loss: 0.08145678788423538\n",
      "Epoch[0], Batch[515], Train loss: 0.10149126499891281\n",
      "Epoch[0], Val loss: 0.07840579003095627\n",
      "Epoch[0], Batch[516], Train loss: 0.11065779626369476\n",
      "Epoch[0], Val loss: 0.08449708670377731\n",
      "Epoch[0], Batch[517], Train loss: 0.10615444928407669\n",
      "Epoch[0], Val loss: 0.08430091291666031\n",
      "Epoch[0], Batch[518], Train loss: 0.09821953624486923\n",
      "Epoch[0], Val loss: 0.09104189276695251\n",
      "Epoch[0], Batch[519], Train loss: 0.0973963588476181\n",
      "Epoch[0], Val loss: 0.07910940796136856\n",
      "Epoch[0], Batch[520], Train loss: 0.10261289030313492\n",
      "Epoch[0], Val loss: 0.08685330301523209\n",
      "Epoch[0], Batch[521], Train loss: 0.10741639882326126\n",
      "Epoch[0], Val loss: 0.07769613713026047\n",
      "Epoch[0], Batch[522], Train loss: 0.10985718667507172\n",
      "Epoch[0], Val loss: 0.08623360097408295\n",
      "Epoch[0], Batch[523], Train loss: 0.09805259853601456\n",
      "Epoch[0], Val loss: 0.08711603283882141\n",
      "Epoch[0], Batch[524], Train loss: 0.10617759078741074\n",
      "Epoch[0], Val loss: 0.0859360620379448\n",
      "Epoch[0], Batch[525], Train loss: 0.10927023738622665\n",
      "Epoch[0], Val loss: 0.08942358940839767\n",
      "Epoch[0], Batch[526], Train loss: 0.09782037138938904\n",
      "Epoch[0], Val loss: 0.08601757884025574\n",
      "Epoch[0], Batch[527], Train loss: 0.09903529286384583\n",
      "Epoch[0], Val loss: 0.08379904925823212\n",
      "Epoch[0], Batch[528], Train loss: 0.10742762684822083\n",
      "Epoch[0], Val loss: 0.08050891757011414\n",
      "Epoch[0], Batch[529], Train loss: 0.10406417399644852\n",
      "Epoch[0], Val loss: 0.07940548658370972\n",
      "Epoch[0], Batch[530], Train loss: 0.10164220631122589\n",
      "Epoch[0], Val loss: 0.07937628030776978\n",
      "Epoch[0], Batch[531], Train loss: 0.10027138888835907\n",
      "Epoch[0], Val loss: 0.0796532854437828\n",
      "Epoch[0], Batch[532], Train loss: 0.1069800928235054\n",
      "Epoch[0], Val loss: 0.07615441828966141\n",
      "Epoch[0], Batch[533], Train loss: 0.10024281591176987\n",
      "Epoch[0], Val loss: 0.0761827901005745\n",
      "Epoch[0], Batch[534], Train loss: 0.09804873913526535\n",
      "Epoch[0], Val loss: 0.08057893067598343\n",
      "Epoch[0], Batch[535], Train loss: 0.11070170253515244\n",
      "Epoch[0], Val loss: 0.08206867426633835\n",
      "Epoch[0], Batch[536], Train loss: 0.09767243266105652\n",
      "Epoch[0], Val loss: 0.08113430440425873\n",
      "Epoch[0], Batch[537], Train loss: 0.09379290044307709\n",
      "Epoch[0], Val loss: 0.07652437686920166\n",
      "Epoch[0], Batch[538], Train loss: 0.1034020483493805\n",
      "Epoch[0], Val loss: 0.08361589163541794\n",
      "Epoch[0], Batch[539], Train loss: 0.10530304163694382\n",
      "Epoch[0], Val loss: 0.08286930620670319\n",
      "Epoch[0], Batch[540], Train loss: 0.10237085074186325\n",
      "Epoch[0], Val loss: 0.08031358569860458\n",
      "Epoch[0], Batch[541], Train loss: 0.10481465607881546\n",
      "Epoch[0], Val loss: 0.08375800400972366\n",
      "Epoch[0], Batch[542], Train loss: 0.09597574174404144\n",
      "Epoch[0], Val loss: 0.0833207368850708\n",
      "Epoch[0], Batch[543], Train loss: 0.09408403187990189\n",
      "Epoch[0], Val loss: 0.0768001526594162\n",
      "Epoch[0], Batch[544], Train loss: 0.0965120866894722\n",
      "Epoch[0], Val loss: 0.0793721005320549\n",
      "Epoch[0], Batch[545], Train loss: 0.09659606218338013\n",
      "Epoch[0], Val loss: 0.08144287765026093\n",
      "Epoch[0], Batch[546], Train loss: 0.09715403616428375\n",
      "Epoch[0], Val loss: 0.0793602392077446\n",
      "Epoch[0], Batch[547], Train loss: 0.09717465937137604\n",
      "Epoch[0], Val loss: 0.08164514601230621\n",
      "Epoch[0], Batch[548], Train loss: 0.0949854627251625\n",
      "Epoch[0], Val loss: 0.08160057663917542\n",
      "Epoch[0], Batch[549], Train loss: 0.08978374302387238\n",
      "Epoch[0], Val loss: 0.07385803759098053\n",
      "Epoch[0], Batch[550], Train loss: 0.09893965721130371\n",
      "Epoch[0], Val loss: 0.07668868452310562\n",
      "Epoch[0], Batch[551], Train loss: 0.09920093417167664\n",
      "Epoch[0], Val loss: 0.08330627530813217\n",
      "Epoch[0], Batch[552], Train loss: 0.09322783350944519\n",
      "Epoch[0], Val loss: 0.0793861597776413\n",
      "Epoch[0], Batch[553], Train loss: 0.11113584786653519\n",
      "Epoch[0], Val loss: 0.08838111162185669\n",
      "Epoch[0], Batch[554], Train loss: 0.09547057747840881\n",
      "Epoch[0], Val loss: 0.07703519612550735\n",
      "Epoch[0], Batch[555], Train loss: 0.09471051394939423\n",
      "Epoch[0], Val loss: 0.08118196576833725\n",
      "Epoch[0], Batch[556], Train loss: 0.09134777635335922\n",
      "Epoch[0], Val loss: 0.07408560812473297\n",
      "Epoch[0], Batch[557], Train loss: 0.09751666337251663\n",
      "Epoch[0], Val loss: 0.07335584610700607\n",
      "Epoch[0], Batch[558], Train loss: 0.10149408876895905\n",
      "Epoch[0], Val loss: 0.07811681926250458\n",
      "Epoch[0], Batch[559], Train loss: 0.10431624203920364\n",
      "Epoch[0], Val loss: 0.08719056099653244\n",
      "Epoch[0], Batch[560], Train loss: 0.09506237506866455\n",
      "Epoch[0], Val loss: 0.08138647675514221\n",
      "Epoch[0], Batch[561], Train loss: 0.09716786444187164\n",
      "Epoch[0], Val loss: 0.07826726883649826\n",
      "Epoch[0], Batch[562], Train loss: 0.088676318526268\n",
      "Epoch[0], Val loss: 0.07384009659290314\n",
      "Epoch[0], Batch[563], Train loss: 0.09354481101036072\n",
      "Epoch[0], Val loss: 0.08146287500858307\n",
      "Epoch[0], Batch[564], Train loss: 0.10041498392820358\n",
      "Epoch[0], Val loss: 0.08167225122451782\n",
      "Epoch[0], Batch[565], Train loss: 0.09081574529409409\n",
      "Epoch[0], Val loss: 0.08110020309686661\n",
      "Epoch[0], Batch[566], Train loss: 0.09600131213665009\n",
      "Epoch[0], Val loss: 0.08061334490776062\n",
      "Epoch[0], Batch[567], Train loss: 0.09383683651685715\n",
      "Epoch[0], Val loss: 0.07787264138460159\n",
      "Epoch[0], Batch[568], Train loss: 0.09530318528413773\n",
      "Epoch[0], Val loss: 0.07860039919614792\n",
      "Epoch[0], Batch[569], Train loss: 0.09273366630077362\n",
      "Epoch[0], Val loss: 0.07303633540868759\n",
      "Epoch[0], Batch[570], Train loss: 0.09614085406064987\n",
      "Epoch[0], Val loss: 0.07469861954450607\n",
      "Epoch[0], Batch[571], Train loss: 0.09320586919784546\n",
      "Epoch[0], Val loss: 0.08013167232275009\n",
      "Epoch[0], Batch[572], Train loss: 0.10084246844053268\n",
      "Epoch[0], Val loss: 0.07771250605583191\n",
      "Epoch[0], Batch[573], Train loss: 0.09122334420681\n",
      "Epoch[0], Val loss: 0.07807769626379013\n",
      "Epoch[0], Batch[574], Train loss: 0.08497879654169083\n",
      "Epoch[0], Val loss: 0.07785917818546295\n",
      "Epoch[0], Batch[575], Train loss: 0.09322396665811539\n",
      "Epoch[0], Val loss: 0.0806097686290741\n",
      "Epoch[0], Batch[576], Train loss: 0.09230488538742065\n",
      "Epoch[0], Val loss: 0.07712335884571075\n",
      "Epoch[0], Batch[577], Train loss: 0.09627332538366318\n",
      "Epoch[0], Val loss: 0.07515949010848999\n",
      "Epoch[0], Batch[578], Train loss: 0.09983806312084198\n",
      "Epoch[0], Val loss: 0.07584130764007568\n",
      "Epoch[0], Batch[579], Train loss: 0.09291031956672668\n",
      "Epoch[0], Val loss: 0.07276710122823715\n",
      "Epoch[0], Batch[580], Train loss: 0.08837190270423889\n",
      "Epoch[0], Val loss: 0.07803492248058319\n",
      "Epoch[0], Batch[581], Train loss: 0.09485422819852829\n",
      "Epoch[0], Val loss: 0.07318002730607986\n",
      "Epoch[0], Batch[582], Train loss: 0.09379959851503372\n",
      "Epoch[0], Val loss: 0.07743620127439499\n",
      "Epoch[0], Batch[583], Train loss: 0.0986299216747284\n",
      "Epoch[0], Val loss: 0.07363609224557877\n",
      "Epoch[0], Batch[584], Train loss: 0.08813884109258652\n",
      "Epoch[0], Val loss: 0.07536058127880096\n",
      "Epoch[0], Batch[585], Train loss: 0.08878473192453384\n",
      "Epoch[0], Val loss: 0.0752553790807724\n",
      "Epoch[0], Batch[586], Train loss: 0.08887813240289688\n",
      "Epoch[0], Val loss: 0.07316143065690994\n",
      "Epoch[0], Batch[587], Train loss: 0.09190605580806732\n",
      "Epoch[0], Val loss: 0.07388481497764587\n",
      "Epoch[0], Batch[588], Train loss: 0.09053216129541397\n",
      "Epoch[0], Val loss: 0.07841233909130096\n",
      "Epoch[0], Batch[589], Train loss: 0.09157304465770721\n",
      "Epoch[0], Val loss: 0.08078792691230774\n",
      "Epoch[0], Batch[590], Train loss: 0.08936356008052826\n",
      "Epoch[0], Val loss: 0.07035981118679047\n",
      "Epoch[0], Batch[591], Train loss: 0.08881158381700516\n",
      "Epoch[0], Val loss: 0.07066483050584793\n",
      "Epoch[0], Batch[592], Train loss: 0.08495225757360458\n",
      "Epoch[0], Val loss: 0.07101947069168091\n",
      "Epoch[0], Batch[593], Train loss: 0.08785103261470795\n",
      "Epoch[0], Val loss: 0.07818858325481415\n",
      "Epoch[0], Batch[594], Train loss: 0.09714020043611526\n",
      "Epoch[0], Val loss: 0.08113481849431992\n",
      "Epoch[0], Batch[595], Train loss: 0.09334876388311386\n",
      "Epoch[0], Val loss: 0.07058997452259064\n",
      "Epoch[0], Batch[596], Train loss: 0.09252075850963593\n",
      "Epoch[0], Val loss: 0.0769522413611412\n",
      "Epoch[0], Batch[597], Train loss: 0.1002064198255539\n",
      "Epoch[0], Val loss: 0.07083170861005783\n",
      "Epoch[0], Batch[598], Train loss: 0.08428671956062317\n",
      "Epoch[0], Val loss: 0.07100126892328262\n",
      "Epoch[0], Batch[599], Train loss: 0.08980265259742737\n",
      "Epoch[0], Val loss: 0.07342606037855148\n",
      "Epoch[0], Batch[600], Train loss: 0.08793959766626358\n",
      "Epoch[0], Val loss: 0.07554896175861359\n",
      "Epoch[0], Batch[601], Train loss: 0.08749052882194519\n",
      "Epoch[0], Val loss: 0.07076507061719894\n",
      "Epoch[0], Batch[602], Train loss: 0.09019831568002701\n",
      "Epoch[0], Val loss: 0.07321794331073761\n",
      "Epoch[0], Batch[603], Train loss: 0.09317555278539658\n",
      "Epoch[0], Val loss: 0.07553261518478394\n",
      "Epoch[0], Batch[604], Train loss: 0.08485592156648636\n",
      "Epoch[0], Val loss: 0.07603748887777328\n",
      "Epoch[0], Batch[605], Train loss: 0.0917416661977768\n",
      "Epoch[0], Val loss: 0.07212946563959122\n",
      "Epoch[0], Batch[606], Train loss: 0.08904536068439484\n",
      "Epoch[0], Val loss: 0.07624560594558716\n",
      "Epoch[0], Batch[607], Train loss: 0.08854053169488907\n",
      "Epoch[0], Val loss: 0.07111255824565887\n",
      "Epoch[0], Batch[608], Train loss: 0.09540897607803345\n",
      "Epoch[0], Val loss: 0.07720665633678436\n",
      "Epoch[0], Batch[609], Train loss: 0.09134748578071594\n",
      "Epoch[0], Val loss: 0.06887660920619965\n",
      "Epoch[0], Batch[610], Train loss: 0.08503022789955139\n",
      "Epoch[0], Val loss: 0.07325480878353119\n",
      "Epoch[0], Batch[611], Train loss: 0.08747022598981857\n",
      "Epoch[0], Val loss: 0.07086537033319473\n",
      "Epoch[0], Batch[612], Train loss: 0.0916864201426506\n",
      "Epoch[0], Val loss: 0.0690651535987854\n",
      "Epoch[0], Batch[613], Train loss: 0.08147146552801132\n",
      "Epoch[0], Val loss: 0.06977688521146774\n",
      "Epoch[0], Batch[614], Train loss: 0.09297791868448257\n",
      "Epoch[0], Val loss: 0.0727083832025528\n",
      "Epoch[0], Batch[615], Train loss: 0.0834226906299591\n",
      "Epoch[0], Val loss: 0.07027286291122437\n",
      "Epoch[0], Batch[616], Train loss: 0.0874423161149025\n",
      "Epoch[0], Val loss: 0.07087390124797821\n",
      "Epoch[0], Batch[617], Train loss: 0.09482655674219131\n",
      "Epoch[0], Val loss: 0.07344929128885269\n",
      "Epoch[0], Batch[618], Train loss: 0.08315746486186981\n",
      "Epoch[0], Val loss: 0.07121313363313675\n",
      "Epoch[0], Batch[619], Train loss: 0.08752429485321045\n",
      "Epoch[0], Val loss: 0.0759032815694809\n",
      "Epoch[0], Batch[620], Train loss: 0.08926014602184296\n",
      "Epoch[0], Val loss: 0.07337422668933868\n",
      "Epoch[0], Batch[621], Train loss: 0.08625634759664536\n",
      "Epoch[0], Val loss: 0.07243938744068146\n",
      "Epoch[0], Batch[622], Train loss: 0.08820091187953949\n",
      "Epoch[0], Val loss: 0.0689375102519989\n",
      "Epoch[0], Batch[623], Train loss: 0.08926001936197281\n",
      "Epoch[0], Val loss: 0.07028462737798691\n",
      "Epoch[0], Batch[624], Train loss: 0.08214440196752548\n",
      "Epoch[0], Val loss: 0.06925715506076813\n",
      "Epoch[0], Batch[625], Train loss: 0.08550701290369034\n",
      "Epoch[0], Val loss: 0.07134328037500381\n",
      "Epoch[0], Batch[626], Train loss: 0.07739701122045517\n",
      "Epoch[0], Val loss: 0.06666598469018936\n",
      "Epoch[0], Batch[627], Train loss: 0.08297046273946762\n",
      "Epoch[0], Val loss: 0.0668276697397232\n",
      "Epoch[0], Batch[628], Train loss: 0.08557526022195816\n",
      "Epoch[0], Val loss: 0.06416726857423782\n",
      "Epoch[0], Batch[629], Train loss: 0.08066312223672867\n",
      "Epoch[0], Val loss: 0.0710315853357315\n",
      "Epoch[0], Batch[630], Train loss: 0.08457252383232117\n",
      "Epoch[0], Val loss: 0.06851620972156525\n",
      "Epoch[0], Batch[631], Train loss: 0.08427803218364716\n",
      "Epoch[0], Val loss: 0.06731892377138138\n",
      "Epoch[0], Batch[632], Train loss: 0.09026891738176346\n",
      "Epoch[0], Val loss: 0.06508545577526093\n",
      "Epoch[0], Batch[633], Train loss: 0.08073464781045914\n",
      "Epoch[0], Val loss: 0.07389681786298752\n",
      "Epoch[0], Batch[634], Train loss: 0.0822339877486229\n",
      "Epoch[0], Val loss: 0.07351312786340714\n",
      "Epoch[0], Batch[635], Train loss: 0.08294040709733963\n",
      "Epoch[0], Val loss: 0.07081805169582367\n",
      "Epoch[0], Batch[636], Train loss: 0.08999525010585785\n",
      "Epoch[0], Val loss: 0.07024182379245758\n",
      "Epoch[0], Batch[637], Train loss: 0.07551124691963196\n",
      "Epoch[0], Val loss: 0.07179753482341766\n",
      "Epoch[0], Batch[638], Train loss: 0.08355967700481415\n",
      "Epoch[0], Val loss: 0.07378438860177994\n",
      "Epoch[0], Batch[639], Train loss: 0.07762005180120468\n",
      "Epoch[0], Val loss: 0.06796139478683472\n",
      "Epoch[0], Batch[640], Train loss: 0.08001308888196945\n",
      "Epoch[0], Val loss: 0.061045486479997635\n",
      "Epoch[0], Batch[641], Train loss: 0.0824194848537445\n",
      "Epoch[0], Val loss: 0.0706033855676651\n",
      "Epoch[0], Batch[642], Train loss: 0.07628947496414185\n",
      "Epoch[0], Val loss: 0.06970594078302383\n",
      "Epoch[0], Batch[643], Train loss: 0.08564691990613937\n",
      "Epoch[0], Val loss: 0.07126917690038681\n",
      "Epoch[0], Batch[644], Train loss: 0.08429356664419174\n",
      "Epoch[0], Val loss: 0.06841372698545456\n",
      "Epoch[0], Batch[645], Train loss: 0.07984276115894318\n",
      "Epoch[0], Val loss: 0.07085012644529343\n",
      "Epoch[0], Batch[646], Train loss: 0.08129849284887314\n",
      "Epoch[0], Val loss: 0.07026737928390503\n",
      "Epoch[0], Batch[647], Train loss: 0.08094897121191025\n",
      "Epoch[0], Val loss: 0.06863296777009964\n",
      "Epoch[0], Batch[648], Train loss: 0.08207374811172485\n",
      "Epoch[0], Val loss: 0.07192161679267883\n",
      "Epoch[0], Batch[649], Train loss: 0.08334530889987946\n",
      "Epoch[0], Val loss: 0.07247081398963928\n",
      "Epoch[0], Batch[650], Train loss: 0.07883335649967194\n",
      "Epoch[0], Val loss: 0.0700819343328476\n",
      "Epoch[0], Batch[651], Train loss: 0.07834304124116898\n",
      "Epoch[0], Val loss: 0.0691891759634018\n",
      "Epoch[0], Batch[652], Train loss: 0.08650586754083633\n",
      "Epoch[0], Val loss: 0.06683491170406342\n",
      "Epoch[0], Batch[653], Train loss: 0.07464706152677536\n",
      "Epoch[0], Val loss: 0.06819095462560654\n",
      "Epoch[0], Batch[654], Train loss: 0.07830864191055298\n",
      "Epoch[0], Val loss: 0.06502103805541992\n",
      "Epoch[0], Batch[655], Train loss: 0.08755772560834885\n",
      "Epoch[0], Val loss: 0.06869915872812271\n",
      "Epoch[0], Batch[656], Train loss: 0.08331137895584106\n",
      "Epoch[0], Val loss: 0.07213296741247177\n",
      "Epoch[0], Batch[657], Train loss: 0.08151814341545105\n",
      "Epoch[0], Val loss: 0.060798317193984985\n",
      "Epoch[0], Batch[658], Train loss: 0.086595319211483\n",
      "Epoch[0], Val loss: 0.0659153014421463\n",
      "Epoch[0], Batch[659], Train loss: 0.07909739017486572\n",
      "Epoch[0], Val loss: 0.06935994327068329\n",
      "Epoch[0], Batch[660], Train loss: 0.07941339164972305\n",
      "Epoch[0], Val loss: 0.07094766199588776\n",
      "Epoch[0], Batch[661], Train loss: 0.08683207631111145\n",
      "Epoch[0], Val loss: 0.06838928908109665\n",
      "Epoch[0], Batch[662], Train loss: 0.08439073711633682\n",
      "Epoch[0], Val loss: 0.06362014263868332\n",
      "Epoch[0], Batch[663], Train loss: 0.07868997752666473\n",
      "Epoch[0], Val loss: 0.06835372000932693\n",
      "Epoch[0], Batch[664], Train loss: 0.08314809203147888\n",
      "Epoch[0], Val loss: 0.06518198549747467\n",
      "Epoch[0], Batch[665], Train loss: 0.07803540676832199\n",
      "Epoch[0], Val loss: 0.06852859258651733\n",
      "Epoch[0], Batch[666], Train loss: 0.08106902241706848\n",
      "Epoch[0], Val loss: 0.06979458034038544\n",
      "Epoch[0], Batch[667], Train loss: 0.07461933046579361\n",
      "Epoch[0], Val loss: 0.06683658808469772\n",
      "Epoch[0], Batch[668], Train loss: 0.08040191978216171\n",
      "Epoch[0], Val loss: 0.06730440258979797\n",
      "Epoch[0], Batch[669], Train loss: 0.07739907503128052\n",
      "Epoch[0], Val loss: 0.06403826922178268\n",
      "Epoch[0], Batch[670], Train loss: 0.08075503259897232\n",
      "Epoch[0], Val loss: 0.06554011255502701\n",
      "Epoch[0], Batch[671], Train loss: 0.07855350524187088\n",
      "Epoch[0], Val loss: 0.06682704389095306\n",
      "Epoch[0], Batch[672], Train loss: 0.07592392712831497\n",
      "Epoch[0], Val loss: 0.07683365046977997\n",
      "Epoch[0], Batch[673], Train loss: 0.0786634236574173\n",
      "Epoch[0], Val loss: 0.0688931941986084\n",
      "Epoch[0], Batch[674], Train loss: 0.07565866410732269\n",
      "Epoch[0], Val loss: 0.0691981092095375\n",
      "Epoch[0], Batch[675], Train loss: 0.08336681872606277\n",
      "Epoch[0], Val loss: 0.06653518229722977\n",
      "Epoch[0], Batch[676], Train loss: 0.0815008282661438\n",
      "Epoch[0], Val loss: 0.06480694562196732\n",
      "Epoch[0], Batch[677], Train loss: 0.08748577535152435\n",
      "Epoch[0], Val loss: 0.06364671140909195\n",
      "Epoch[0], Batch[678], Train loss: 0.08252669125795364\n",
      "Epoch[0], Val loss: 0.06816080957651138\n",
      "Epoch[0], Batch[679], Train loss: 0.07918863743543625\n",
      "Epoch[0], Val loss: 0.06772574782371521\n",
      "Epoch[0], Batch[680], Train loss: 0.07764917612075806\n",
      "Epoch[0], Val loss: 0.06778240948915482\n",
      "Epoch[0], Batch[681], Train loss: 0.07741179317235947\n",
      "Epoch[0], Val loss: 0.07230184227228165\n",
      "Epoch[0], Batch[682], Train loss: 0.08300237357616425\n",
      "Epoch[0], Val loss: 0.07051194459199905\n",
      "Epoch[0], Batch[683], Train loss: 0.07191132754087448\n",
      "Epoch[0], Val loss: 0.06782151758670807\n",
      "Epoch[0], Batch[684], Train loss: 0.07756120711565018\n",
      "Epoch[0], Val loss: 0.06594575941562653\n",
      "Epoch[0], Batch[685], Train loss: 0.07698054611682892\n",
      "Epoch[0], Val loss: 0.07248090952634811\n",
      "Epoch[0], Batch[686], Train loss: 0.0790148675441742\n",
      "Epoch[0], Val loss: 0.06907594203948975\n",
      "Epoch[0], Batch[687], Train loss: 0.0782218724489212\n",
      "Epoch[0], Val loss: 0.06745261698961258\n",
      "Epoch[0], Batch[688], Train loss: 0.08318977057933807\n",
      "Epoch[0], Val loss: 0.06047096103429794\n",
      "Epoch[0], Batch[689], Train loss: 0.08166693896055222\n",
      "Epoch[0], Val loss: 0.06609095633029938\n",
      "Epoch[0], Batch[690], Train loss: 0.07488930970430374\n",
      "Epoch[0], Val loss: 0.060878291726112366\n",
      "Epoch[0], Batch[691], Train loss: 0.08422736823558807\n",
      "Epoch[0], Val loss: 0.06525060534477234\n",
      "Epoch[0], Batch[692], Train loss: 0.07320128381252289\n",
      "Epoch[0], Val loss: 0.06649309396743774\n",
      "Epoch[0], Batch[693], Train loss: 0.07713861763477325\n",
      "Epoch[0], Val loss: 0.06351921707391739\n",
      "Epoch[0], Batch[694], Train loss: 0.07624045014381409\n",
      "Epoch[0], Val loss: 0.0625254437327385\n",
      "Epoch[0], Batch[695], Train loss: 0.0777093917131424\n",
      "Epoch[0], Val loss: 0.06618845462799072\n",
      "Epoch[0], Batch[696], Train loss: 0.0754743292927742\n",
      "Epoch[0], Val loss: 0.06433188170194626\n",
      "Epoch[0], Batch[697], Train loss: 0.07466854155063629\n",
      "Epoch[0], Val loss: 0.0624995082616806\n",
      "Epoch[0], Batch[698], Train loss: 0.07620222121477127\n",
      "Epoch[0], Val loss: 0.06246928498148918\n",
      "Epoch[0], Batch[699], Train loss: 0.07675709575414658\n",
      "Epoch[0], Val loss: 0.06517227739095688\n",
      "Epoch[0], Batch[700], Train loss: 0.07491108030080795\n",
      "Epoch[0], Val loss: 0.06380993127822876\n",
      "Epoch[0], Batch[701], Train loss: 0.07498910278081894\n",
      "Epoch[0], Val loss: 0.06834226846694946\n",
      "Epoch[0], Batch[702], Train loss: 0.08343133330345154\n",
      "Epoch[0], Val loss: 0.06570330262184143\n",
      "Epoch[0], Batch[703], Train loss: 0.07200773805379868\n",
      "Epoch[0], Val loss: 0.06946445256471634\n",
      "Epoch[0], Batch[704], Train loss: 0.07569967955350876\n",
      "Epoch[0], Val loss: 0.061884522438049316\n",
      "Epoch[0], Batch[705], Train loss: 0.07303378731012344\n",
      "Epoch[0], Val loss: 0.0716848149895668\n",
      "Epoch[0], Batch[706], Train loss: 0.08180895447731018\n",
      "Epoch[0], Val loss: 0.06175107881426811\n",
      "Epoch[0], Batch[707], Train loss: 0.07805231213569641\n",
      "Epoch[0], Val loss: 0.065116286277771\n",
      "Epoch[0], Batch[708], Train loss: 0.0858222097158432\n",
      "Epoch[0], Val loss: 0.06348438560962677\n",
      "Epoch[0], Batch[709], Train loss: 0.0796358734369278\n",
      "Epoch[0], Val loss: 0.06183484196662903\n",
      "Epoch[0], Batch[710], Train loss: 0.0702817365527153\n",
      "Epoch[0], Val loss: 0.0720382109284401\n",
      "Epoch[0], Batch[711], Train loss: 0.07401813566684723\n",
      "Epoch[0], Val loss: 0.06378123909235\n",
      "Epoch[0], Batch[712], Train loss: 0.07769732922315598\n",
      "Epoch[0], Val loss: 0.06346689909696579\n",
      "Epoch[0], Batch[713], Train loss: 0.08114258199930191\n",
      "Epoch[0], Val loss: 0.06326117366552353\n",
      "Epoch[0], Batch[714], Train loss: 0.07693076133728027\n",
      "Epoch[0], Val loss: 0.06924882531166077\n",
      "Epoch[0], Batch[715], Train loss: 0.0757763609290123\n",
      "Epoch[0], Val loss: 0.061141666024923325\n",
      "Epoch[0], Batch[716], Train loss: 0.07568871974945068\n",
      "Epoch[0], Val loss: 0.0633067861199379\n",
      "Epoch[0], Batch[717], Train loss: 0.08072460442781448\n",
      "Epoch[0], Val loss: 0.06448055058717728\n",
      "Epoch[0], Batch[718], Train loss: 0.0762510821223259\n",
      "Epoch[0], Val loss: 0.06442133337259293\n",
      "Epoch[0], Batch[719], Train loss: 0.0775039941072464\n",
      "Epoch[0], Val loss: 0.05963876098394394\n",
      "Epoch[0], Batch[720], Train loss: 0.0789995864033699\n",
      "Epoch[0], Val loss: 0.059879858046770096\n",
      "Epoch[0], Batch[721], Train loss: 0.07675083726644516\n",
      "Epoch[0], Val loss: 0.0644931048154831\n",
      "Epoch[0], Batch[722], Train loss: 0.07359838485717773\n",
      "Epoch[0], Val loss: 0.06568295508623123\n",
      "Epoch[0], Batch[723], Train loss: 0.07235985994338989\n",
      "Epoch[0], Val loss: 0.05966848134994507\n",
      "Epoch[0], Batch[724], Train loss: 0.0735275074839592\n",
      "Epoch[0], Val loss: 0.06656207889318466\n",
      "Epoch[0], Batch[725], Train loss: 0.07664158195257187\n",
      "Epoch[0], Val loss: 0.059720948338508606\n",
      "Epoch[0], Batch[726], Train loss: 0.07843766361474991\n",
      "Epoch[0], Val loss: 0.06238704174757004\n",
      "Epoch[0], Batch[727], Train loss: 0.07162730395793915\n",
      "Epoch[0], Val loss: 0.0624704584479332\n",
      "Epoch[0], Batch[728], Train loss: 0.07447188347578049\n",
      "Epoch[0], Val loss: 0.06409753113985062\n",
      "Epoch[0], Batch[729], Train loss: 0.07635807245969772\n",
      "Epoch[0], Val loss: 0.06256312876939774\n",
      "Epoch[0], Batch[730], Train loss: 0.07455327361822128\n",
      "Epoch[0], Val loss: 0.06416673213243484\n",
      "Epoch[0], Batch[731], Train loss: 0.07238712906837463\n",
      "Epoch[0], Val loss: 0.06776443868875504\n",
      "Epoch[0], Batch[732], Train loss: 0.07446952164173126\n",
      "Epoch[0], Val loss: 0.06369410455226898\n",
      "Epoch[0], Batch[733], Train loss: 0.07263753563165665\n",
      "Epoch[0], Val loss: 0.06278947740793228\n",
      "Epoch[0], Batch[734], Train loss: 0.07606303691864014\n",
      "Epoch[0], Val loss: 0.05781813710927963\n",
      "Epoch[0], Batch[735], Train loss: 0.07218237221240997\n",
      "Epoch[0], Val loss: 0.06636485457420349\n",
      "Epoch[0], Batch[736], Train loss: 0.07451338320970535\n",
      "Epoch[0], Val loss: 0.059684306383132935\n",
      "Epoch[0], Batch[737], Train loss: 0.07262548059225082\n",
      "Epoch[0], Val loss: 0.06691139936447144\n",
      "Epoch[0], Batch[738], Train loss: 0.07748376578092575\n",
      "Epoch[0], Val loss: 0.06055202707648277\n",
      "Epoch[0], Batch[739], Train loss: 0.07044106721878052\n",
      "Epoch[0], Val loss: 0.06444456428289413\n",
      "Epoch[0], Batch[740], Train loss: 0.07642822712659836\n",
      "Epoch[0], Val loss: 0.0643443912267685\n",
      "Epoch[0], Batch[741], Train loss: 0.07369651645421982\n",
      "Epoch[0], Val loss: 0.06355828046798706\n",
      "Epoch[0], Batch[742], Train loss: 0.07299728691577911\n",
      "Epoch[0], Val loss: 0.06042994186282158\n",
      "Epoch[0], Batch[743], Train loss: 0.07396511733531952\n",
      "Epoch[0], Val loss: 0.06264535337686539\n",
      "Epoch[0], Batch[744], Train loss: 0.07250259816646576\n",
      "Epoch[0], Val loss: 0.06793230026960373\n",
      "Epoch[0], Batch[745], Train loss: 0.07599899917840958\n",
      "Epoch[0], Val loss: 0.0655394047498703\n",
      "Epoch[0], Batch[746], Train loss: 0.07149948924779892\n",
      "Epoch[0], Val loss: 0.060864515602588654\n",
      "Epoch[0], Batch[747], Train loss: 0.07154335081577301\n",
      "Epoch[0], Val loss: 0.06478680670261383\n",
      "Epoch[0], Batch[748], Train loss: 0.07390274107456207\n",
      "Epoch[0], Val loss: 0.06915850192308426\n",
      "Epoch[0], Batch[749], Train loss: 0.06982384622097015\n",
      "Epoch[0], Val loss: 0.06172776222229004\n",
      "Epoch[0], Batch[750], Train loss: 0.07660949230194092\n",
      "Epoch[0], Val loss: 0.06229182705283165\n",
      "Epoch[0], Batch[751], Train loss: 0.07547423243522644\n",
      "Epoch[0], Val loss: 0.06539620459079742\n",
      "Epoch[0], Batch[752], Train loss: 0.06891244649887085\n",
      "Epoch[0], Val loss: 0.06100289896130562\n",
      "Epoch[0], Batch[753], Train loss: 0.07171182334423065\n",
      "Epoch[0], Val loss: 0.059763554483652115\n",
      "Epoch[0], Batch[754], Train loss: 0.07221054285764694\n",
      "Epoch[0], Val loss: 0.06245357170701027\n",
      "Epoch[0], Batch[755], Train loss: 0.07553991675376892\n",
      "Epoch[0], Val loss: 0.061571963131427765\n",
      "Epoch[0], Batch[756], Train loss: 0.07411761581897736\n",
      "Epoch[0], Val loss: 0.06039818748831749\n",
      "Epoch[0], Batch[757], Train loss: 0.07096819579601288\n",
      "Epoch[0], Val loss: 0.06860276311635971\n",
      "Epoch[0], Batch[758], Train loss: 0.07505545020103455\n",
      "Epoch[0], Val loss: 0.0591844841837883\n",
      "Epoch[0], Batch[759], Train loss: 0.07301463186740875\n",
      "Epoch[0], Val loss: 0.0631532296538353\n",
      "Epoch[0], Batch[760], Train loss: 0.07293706387281418\n",
      "Epoch[0], Val loss: 0.06681263446807861\n",
      "Epoch[0], Batch[761], Train loss: 0.0760967954993248\n",
      "Epoch[0], Val loss: 0.059890393167734146\n",
      "Epoch[0], Batch[762], Train loss: 0.07050487399101257\n",
      "Epoch[0], Val loss: 0.06377529352903366\n",
      "Epoch[0], Batch[763], Train loss: 0.07098342478275299\n",
      "Epoch[0], Val loss: 0.0640602558851242\n",
      "Epoch[0], Batch[764], Train loss: 0.07249636948108673\n",
      "Epoch[0], Val loss: 0.06012400612235069\n",
      "Epoch[0], Batch[765], Train loss: 0.07881533354520798\n",
      "Epoch[0], Val loss: 0.061432525515556335\n",
      "Epoch[0], Batch[766], Train loss: 0.07045706361532211\n",
      "Epoch[0], Val loss: 0.05775781720876694\n",
      "Epoch[0], Batch[767], Train loss: 0.0722382664680481\n",
      "Epoch[0], Val loss: 0.06424396485090256\n",
      "Epoch[0], Batch[768], Train loss: 0.07143914699554443\n",
      "Epoch[0], Val loss: 0.06023848056793213\n",
      "Epoch[0], Batch[769], Train loss: 0.07061228156089783\n",
      "Epoch[0], Val loss: 0.0601501390337944\n",
      "Epoch[0], Batch[770], Train loss: 0.070691779255867\n",
      "Epoch[0], Val loss: 0.05968014895915985\n",
      "Epoch[0], Batch[771], Train loss: 0.07424308359622955\n",
      "Epoch[0], Val loss: 0.0643828958272934\n",
      "Epoch[0], Batch[772], Train loss: 0.0706494078040123\n",
      "Epoch[0], Val loss: 0.06167808175086975\n",
      "Epoch[0], Batch[773], Train loss: 0.06953402608633041\n",
      "Epoch[0], Val loss: 0.06267067790031433\n",
      "Epoch[0], Batch[774], Train loss: 0.06961776316165924\n",
      "Epoch[0], Val loss: 0.060780733823776245\n",
      "Epoch[0], Batch[775], Train loss: 0.07083607465028763\n",
      "Epoch[0], Val loss: 0.06377390027046204\n",
      "Epoch[0], Batch[776], Train loss: 0.0710870698094368\n",
      "Epoch[0], Val loss: 0.060230255126953125\n",
      "Epoch[0], Batch[777], Train loss: 0.07278038561344147\n",
      "Epoch[0], Val loss: 0.06341101229190826\n",
      "Epoch[0], Batch[778], Train loss: 0.0741925835609436\n",
      "Epoch[0], Val loss: 0.06214714050292969\n",
      "Epoch[0], Batch[779], Train loss: 0.07383354753255844\n",
      "Epoch[0], Val loss: 0.0578441396355629\n",
      "Epoch[0], Batch[780], Train loss: 0.07233558595180511\n",
      "Epoch[0], Val loss: 0.06029270961880684\n",
      "Epoch[0], Batch[781], Train loss: 0.07473307847976685\n",
      "Epoch[0], Val loss: 0.05980924889445305\n",
      "Epoch[0], Batch[782], Train loss: 0.06855577230453491\n",
      "Epoch[0], Val loss: 0.05794646590948105\n",
      "Epoch[0], Batch[783], Train loss: 0.06914549320936203\n",
      "Epoch[0], Val loss: 0.0641467496752739\n",
      "Epoch[0], Batch[784], Train loss: 0.06909250468015671\n",
      "Epoch[0], Val loss: 0.0609159991145134\n",
      "Epoch[0], Batch[785], Train loss: 0.0719083696603775\n",
      "Epoch[0], Val loss: 0.06266327947378159\n",
      "Epoch[0], Batch[786], Train loss: 0.06931111961603165\n",
      "Epoch[0], Val loss: 0.06365753710269928\n",
      "Epoch[0], Batch[787], Train loss: 0.07274188846349716\n",
      "Epoch[0], Val loss: 0.06629247218370438\n",
      "Epoch[0], Batch[788], Train loss: 0.06627984344959259\n",
      "Epoch[0], Val loss: 0.057043589651584625\n",
      "Epoch[0], Batch[789], Train loss: 0.06976909935474396\n",
      "Epoch[0], Val loss: 0.06369134038686752\n",
      "Epoch[0], Batch[790], Train loss: 0.07050570100545883\n",
      "Epoch[0], Val loss: 0.06275700032711029\n",
      "Epoch[0], Batch[791], Train loss: 0.07869439572095871\n",
      "Epoch[0], Val loss: 0.05751265585422516\n",
      "Epoch[0], Batch[792], Train loss: 0.06619863212108612\n",
      "Epoch[0], Val loss: 0.06196685507893562\n",
      "Epoch[0], Batch[793], Train loss: 0.06878909468650818\n",
      "Epoch[0], Val loss: 0.05793805047869682\n",
      "Epoch[0], Batch[794], Train loss: 0.07032981514930725\n",
      "Epoch[0], Val loss: 0.0589725524187088\n",
      "Epoch[0], Batch[795], Train loss: 0.0744386613368988\n",
      "Epoch[0], Val loss: 0.06538517028093338\n",
      "Epoch[0], Batch[796], Train loss: 0.07004271447658539\n",
      "Epoch[0], Val loss: 0.06469940394163132\n",
      "Epoch[0], Batch[797], Train loss: 0.06813069432973862\n",
      "Epoch[0], Val loss: 0.06013210117816925\n",
      "Epoch[0], Batch[798], Train loss: 0.06735905259847641\n",
      "Epoch[0], Val loss: 0.06176528334617615\n",
      "Epoch[0], Batch[799], Train loss: 0.070715993642807\n",
      "Epoch[0], Val loss: 0.059562429785728455\n",
      "Epoch[0], Batch[800], Train loss: 0.07007116079330444\n",
      "Epoch[0], Val loss: 0.06166606768965721\n",
      "Epoch[0], Batch[801], Train loss: 0.07090767472982407\n",
      "Epoch[0], Val loss: 0.06513117998838425\n",
      "Epoch[0], Batch[802], Train loss: 0.069314144551754\n",
      "Epoch[0], Val loss: 0.06201007962226868\n",
      "Epoch[0], Batch[803], Train loss: 0.0741235613822937\n",
      "Epoch[0], Val loss: 0.06278759986162186\n",
      "Epoch[0], Batch[804], Train loss: 0.06771892309188843\n",
      "Epoch[0], Val loss: 0.060923196375370026\n",
      "Epoch[0], Batch[805], Train loss: 0.0690179169178009\n",
      "Epoch[0], Val loss: 0.05865969881415367\n",
      "Epoch[0], Batch[806], Train loss: 0.06611587852239609\n",
      "Epoch[0], Val loss: 0.05898835510015488\n",
      "Epoch[0], Batch[807], Train loss: 0.0728745311498642\n",
      "Epoch[0], Val loss: 0.06437332183122635\n",
      "Epoch[0], Batch[808], Train loss: 0.06904728710651398\n",
      "Epoch[0], Val loss: 0.06323395669460297\n",
      "Epoch[0], Batch[809], Train loss: 0.0687512680888176\n",
      "Epoch[0], Val loss: 0.06013079360127449\n",
      "Epoch[0], Batch[810], Train loss: 0.06729526817798615\n",
      "Epoch[0], Val loss: 0.06392920762300491\n",
      "Epoch[0], Batch[811], Train loss: 0.06717629730701447\n",
      "Epoch[0], Val loss: 0.06500861793756485\n",
      "Epoch[0], Batch[812], Train loss: 0.07022770494222641\n",
      "Epoch[0], Val loss: 0.059320468455553055\n",
      "Epoch[0], Batch[813], Train loss: 0.06815205514431\n",
      "Epoch[0], Val loss: 0.06412529200315475\n",
      "Epoch[0], Batch[814], Train loss: 0.07134849578142166\n",
      "Epoch[0], Val loss: 0.06053726375102997\n",
      "Epoch[0], Batch[815], Train loss: 0.07082716375589371\n",
      "Epoch[0], Val loss: 0.05593102052807808\n",
      "Epoch[0], Batch[816], Train loss: 0.06824842095375061\n",
      "Epoch[0], Val loss: 0.06155960261821747\n",
      "Epoch[0], Batch[817], Train loss: 0.06664756685495377\n",
      "Epoch[0], Val loss: 0.05981728062033653\n",
      "Epoch[0], Batch[818], Train loss: 0.069993756711483\n",
      "Epoch[0], Val loss: 0.058109793812036514\n",
      "Epoch[0], Batch[819], Train loss: 0.06875075399875641\n",
      "Epoch[0], Val loss: 0.06106632202863693\n",
      "Epoch[0], Batch[820], Train loss: 0.06739695370197296\n",
      "Epoch[0], Val loss: 0.058886054903268814\n",
      "Epoch[0], Batch[821], Train loss: 0.0691513642668724\n",
      "Epoch[0], Val loss: 0.060715388506650925\n",
      "Epoch[0], Batch[822], Train loss: 0.06321464478969574\n",
      "Epoch[0], Val loss: 0.0637941062450409\n",
      "Epoch[0], Batch[823], Train loss: 0.06877107173204422\n",
      "Epoch[0], Val loss: 0.05953795090317726\n",
      "Epoch[0], Batch[824], Train loss: 0.06844696402549744\n",
      "Epoch[0], Val loss: 0.06095433607697487\n",
      "Epoch[0], Batch[825], Train loss: 0.07028168439865112\n",
      "Epoch[0], Val loss: 0.06353157758712769\n",
      "Epoch[0], Batch[826], Train loss: 0.0700325220823288\n",
      "Epoch[0], Val loss: 0.06096930429339409\n",
      "Epoch[0], Batch[827], Train loss: 0.06755869090557098\n",
      "Epoch[0], Val loss: 0.06219300255179405\n",
      "Epoch[0], Batch[828], Train loss: 0.06921274214982986\n",
      "Epoch[0], Val loss: 0.05796012282371521\n",
      "Epoch[0], Batch[829], Train loss: 0.07101263105869293\n",
      "Epoch[0], Val loss: 0.060923315584659576\n",
      "Epoch[0], Batch[830], Train loss: 0.06573519110679626\n",
      "Epoch[0], Val loss: 0.06302787363529205\n",
      "Epoch[0], Batch[831], Train loss: 0.07286619395017624\n",
      "Epoch[0], Val loss: 0.058945074677467346\n",
      "Epoch[0], Batch[832], Train loss: 0.0666629895567894\n",
      "Epoch[0], Val loss: 0.0678994357585907\n",
      "Epoch[0], Batch[833], Train loss: 0.0713878944516182\n",
      "Epoch[0], Val loss: 0.06618848443031311\n",
      "Epoch[0], Batch[834], Train loss: 0.06436504423618317\n",
      "Epoch[0], Val loss: 0.061130695044994354\n",
      "Epoch[0], Batch[835], Train loss: 0.06634867936372757\n",
      "Epoch[0], Val loss: 0.062196165323257446\n",
      "Epoch[0], Batch[836], Train loss: 0.06655550748109818\n",
      "Epoch[0], Val loss: 0.060194242745637894\n",
      "Epoch[0], Batch[837], Train loss: 0.061678607016801834\n",
      "Epoch[0], Val loss: 0.06164797022938728\n",
      "Epoch[0], Batch[838], Train loss: 0.06883449107408524\n",
      "Epoch[0], Val loss: 0.06214311346411705\n",
      "Epoch[0], Batch[839], Train loss: 0.07202935218811035\n",
      "Epoch[0], Val loss: 0.056816715747117996\n",
      "Epoch[0], Batch[840], Train loss: 0.06955191493034363\n",
      "Epoch[0], Val loss: 0.05796527862548828\n",
      "Epoch[0], Batch[841], Train loss: 0.06489824503660202\n",
      "Epoch[0], Val loss: 0.061328157782554626\n",
      "Epoch[0], Batch[842], Train loss: 0.06601810455322266\n",
      "Epoch[0], Val loss: 0.061287324875593185\n",
      "Epoch[0], Batch[843], Train loss: 0.06743579357862473\n",
      "Epoch[0], Val loss: 0.06075238436460495\n",
      "Epoch[0], Batch[844], Train loss: 0.06778208166360855\n",
      "Epoch[0], Val loss: 0.0579909048974514\n",
      "Epoch[0], Batch[845], Train loss: 0.06720104068517685\n",
      "Epoch[0], Val loss: 0.05820793658494949\n",
      "Epoch[0], Batch[846], Train loss: 0.07563940435647964\n",
      "Epoch[0], Val loss: 0.0619363933801651\n",
      "Epoch[0], Batch[847], Train loss: 0.06559424102306366\n",
      "Epoch[0], Val loss: 0.05862400680780411\n",
      "Epoch[0], Batch[848], Train loss: 0.06590691208839417\n",
      "Epoch[0], Val loss: 0.06123083829879761\n",
      "Epoch[0], Batch[849], Train loss: 0.06943592429161072\n",
      "Epoch[0], Val loss: 0.0590648353099823\n",
      "Epoch[0], Batch[850], Train loss: 0.07213744521141052\n",
      "Epoch[0], Val loss: 0.05901153385639191\n",
      "Epoch[0], Batch[851], Train loss: 0.06507020443677902\n",
      "Epoch[0], Val loss: 0.05668213963508606\n",
      "Epoch[0], Batch[852], Train loss: 0.06837877631187439\n",
      "Epoch[0], Val loss: 0.05697742849588394\n",
      "Epoch[0], Batch[853], Train loss: 0.06880859285593033\n",
      "Epoch[0], Val loss: 0.05927971750497818\n",
      "Epoch[0], Batch[854], Train loss: 0.06818640977144241\n",
      "Epoch[0], Val loss: 0.06013331934809685\n",
      "Epoch[0], Batch[855], Train loss: 0.06738787889480591\n",
      "Epoch[0], Val loss: 0.06500595808029175\n",
      "Epoch[0], Batch[856], Train loss: 0.06345627456903458\n",
      "Epoch[0], Val loss: 0.057426817715168\n",
      "Epoch[0], Batch[857], Train loss: 0.06746376305818558\n",
      "Epoch[0], Val loss: 0.05659061297774315\n",
      "Epoch[0], Batch[858], Train loss: 0.06425949931144714\n",
      "Epoch[0], Val loss: 0.05589346960186958\n",
      "Epoch[0], Batch[859], Train loss: 0.07250487059354782\n",
      "Epoch[0], Val loss: 0.05989793315529823\n",
      "Epoch[0], Batch[860], Train loss: 0.06754801422357559\n",
      "Epoch[0], Val loss: 0.058613695204257965\n",
      "Epoch[0], Batch[861], Train loss: 0.06254265457391739\n",
      "Epoch[0], Val loss: 0.05904562771320343\n",
      "Epoch[0], Batch[862], Train loss: 0.06307614594697952\n",
      "Epoch[0], Val loss: 0.05605605989694595\n",
      "Epoch[0], Batch[863], Train loss: 0.06757988780736923\n",
      "Epoch[0], Val loss: 0.061410628259181976\n",
      "Epoch[0], Batch[864], Train loss: 0.06751231104135513\n",
      "Epoch[0], Val loss: 0.06198906525969505\n",
      "Epoch[0], Batch[865], Train loss: 0.06528385728597641\n",
      "Epoch[0], Val loss: 0.05599086359143257\n",
      "Epoch[0], Batch[866], Train loss: 0.07213661819696426\n",
      "Epoch[0], Val loss: 0.061644770205020905\n",
      "Epoch[0], Batch[867], Train loss: 0.06818307936191559\n",
      "Epoch[0], Val loss: 0.057751383632421494\n",
      "Epoch[0], Batch[868], Train loss: 0.06946565210819244\n",
      "Epoch[0], Val loss: 0.059642184525728226\n",
      "Epoch[0], Batch[869], Train loss: 0.06643950939178467\n",
      "Epoch[0], Val loss: 0.06065693125128746\n",
      "Epoch[0], Batch[870], Train loss: 0.06368593871593475\n",
      "Epoch[0], Val loss: 0.06011376902461052\n",
      "Epoch[0], Batch[871], Train loss: 0.06744007766246796\n",
      "Epoch[0], Val loss: 0.06058187782764435\n",
      "Epoch[0], Batch[872], Train loss: 0.065376877784729\n",
      "Epoch[0], Val loss: 0.05536549538373947\n",
      "Epoch[0], Batch[873], Train loss: 0.06694000214338303\n",
      "Epoch[0], Val loss: 0.06051115691661835\n",
      "Epoch[0], Batch[874], Train loss: 0.06864816695451736\n",
      "Epoch[0], Val loss: 0.0582612119615078\n",
      "Epoch[0], Batch[875], Train loss: 0.06794985383749008\n",
      "Epoch[0], Val loss: 0.06557795405387878\n",
      "Epoch[0], Batch[876], Train loss: 0.06560079008340836\n",
      "Epoch[0], Val loss: 0.05353628471493721\n",
      "Epoch[0], Batch[877], Train loss: 0.07459180057048798\n",
      "Epoch[0], Val loss: 0.05854608863592148\n",
      "Epoch[0], Batch[878], Train loss: 0.06381116807460785\n",
      "Epoch[0], Val loss: 0.06258144229650497\n",
      "Epoch[0], Batch[879], Train loss: 0.06922801584005356\n",
      "Epoch[0], Val loss: 0.06378167867660522\n",
      "Epoch[0], Batch[880], Train loss: 0.06294865906238556\n",
      "Epoch[0], Val loss: 0.05867074429988861\n",
      "Epoch[0], Batch[881], Train loss: 0.06766270846128464\n",
      "Epoch[0], Val loss: 0.05814677104353905\n",
      "Epoch[0], Batch[882], Train loss: 0.06484415382146835\n",
      "Epoch[0], Val loss: 0.05341772735118866\n",
      "Epoch[0], Batch[883], Train loss: 0.07089642435312271\n",
      "Epoch[0], Val loss: 0.05773373693227768\n",
      "Epoch[0], Batch[884], Train loss: 0.06508083641529083\n",
      "Epoch[0], Val loss: 0.06009243428707123\n",
      "Epoch[0], Batch[885], Train loss: 0.06462395191192627\n",
      "Epoch[0], Val loss: 0.057771991938352585\n",
      "Epoch[0], Batch[886], Train loss: 0.06768786162137985\n",
      "Epoch[0], Val loss: 0.06558506935834885\n",
      "Epoch[0], Batch[887], Train loss: 0.06750226020812988\n",
      "Epoch[0], Val loss: 0.05359895899891853\n",
      "Epoch[0], Batch[888], Train loss: 0.06864982843399048\n",
      "Epoch[0], Val loss: 0.05721466615796089\n",
      "Epoch[0], Batch[889], Train loss: 0.06777147948741913\n",
      "Epoch[0], Val loss: 0.05803127586841583\n",
      "Epoch[0], Batch[890], Train loss: 0.07191849499940872\n",
      "Epoch[0], Val loss: 0.06021985784173012\n",
      "Epoch[0], Batch[891], Train loss: 0.061532456427812576\n",
      "Epoch[0], Val loss: 0.05849779024720192\n",
      "Epoch[0], Batch[892], Train loss: 0.06549465656280518\n",
      "Epoch[0], Val loss: 0.06240637227892876\n",
      "Epoch[0], Batch[893], Train loss: 0.07098160684108734\n",
      "Epoch[0], Val loss: 0.06158065423369408\n",
      "Epoch[0], Batch[894], Train loss: 0.06680243462324142\n",
      "Epoch[0], Val loss: 0.06025690957903862\n",
      "Epoch[0], Batch[895], Train loss: 0.06732580065727234\n",
      "Epoch[0], Val loss: 0.057558491826057434\n",
      "Epoch[0], Batch[896], Train loss: 0.06558002531528473\n",
      "Epoch[0], Val loss: 0.058619190007448196\n",
      "Epoch[0], Batch[897], Train loss: 0.06968877464532852\n",
      "Epoch[0], Val loss: 0.05838016793131828\n",
      "Epoch[0], Batch[898], Train loss: 0.06911063194274902\n",
      "Epoch[0], Val loss: 0.06141750514507294\n",
      "Epoch[0], Batch[899], Train loss: 0.06527023762464523\n",
      "Epoch[0], Val loss: 0.05628253519535065\n",
      "Epoch[0], Batch[900], Train loss: 0.06536620110273361\n",
      "Epoch[0], Val loss: 0.059135958552360535\n",
      "Epoch[0], Batch[901], Train loss: 0.06774953752756119\n",
      "Epoch[0], Val loss: 0.05536944046616554\n",
      "Epoch[0], Batch[902], Train loss: 0.06870236992835999\n",
      "Epoch[0], Val loss: 0.05708586424589157\n",
      "Epoch[0], Batch[903], Train loss: 0.06803187727928162\n",
      "Epoch[0], Val loss: 0.0567324161529541\n",
      "Epoch[0], Batch[904], Train loss: 0.06299339234828949\n",
      "Epoch[0], Val loss: 0.06207650154829025\n",
      "Epoch[0], Batch[905], Train loss: 0.06498738378286362\n",
      "Epoch[0], Val loss: 0.060108643025159836\n",
      "Epoch[0], Batch[906], Train loss: 0.06329640746116638\n",
      "Epoch[0], Val loss: 0.058459848165512085\n",
      "Epoch[0], Batch[907], Train loss: 0.06279312819242477\n",
      "Epoch[0], Val loss: 0.05906226858496666\n",
      "Epoch[0], Batch[908], Train loss: 0.06545458734035492\n",
      "Epoch[0], Val loss: 0.05509518086910248\n",
      "Epoch[0], Batch[909], Train loss: 0.0617927722632885\n",
      "Epoch[0], Val loss: 0.05691440775990486\n",
      "Epoch[0], Batch[910], Train loss: 0.0657806545495987\n",
      "Epoch[0], Val loss: 0.060037825256586075\n",
      "Epoch[0], Batch[911], Train loss: 0.0626939982175827\n",
      "Epoch[0], Val loss: 0.05509945750236511\n",
      "Epoch[0], Batch[912], Train loss: 0.06747569143772125\n",
      "Epoch[0], Val loss: 0.055061280727386475\n",
      "Epoch[0], Batch[913], Train loss: 0.06800112873315811\n",
      "Epoch[0], Val loss: 0.05584069713950157\n",
      "Epoch[0], Batch[914], Train loss: 0.06967779248952866\n",
      "Epoch[0], Val loss: 0.05580488592386246\n",
      "Epoch[0], Batch[915], Train loss: 0.06320858001708984\n",
      "Epoch[0], Val loss: 0.056590110063552856\n",
      "Epoch[0], Batch[916], Train loss: 0.06364937871694565\n",
      "Epoch[0], Val loss: 0.058363523334264755\n",
      "Epoch[0], Batch[917], Train loss: 0.06586860865354538\n",
      "Epoch[0], Val loss: 0.056583698838949203\n",
      "Epoch[0], Batch[918], Train loss: 0.06574124097824097\n",
      "Epoch[0], Val loss: 0.05552879348397255\n",
      "Epoch[0], Batch[919], Train loss: 0.06929776817560196\n",
      "Epoch[0], Val loss: 0.05778361111879349\n",
      "Epoch[0], Batch[920], Train loss: 0.061013203114271164\n",
      "Epoch[0], Val loss: 0.05635080114006996\n",
      "Epoch[0], Batch[921], Train loss: 0.06664755195379257\n",
      "Epoch[0], Val loss: 0.05766497179865837\n",
      "Epoch[0], Batch[922], Train loss: 0.06344090402126312\n",
      "Epoch[0], Val loss: 0.0561545304954052\n",
      "Epoch[0], Batch[923], Train loss: 0.06363432109355927\n",
      "Epoch[0], Val loss: 0.05944695696234703\n",
      "Epoch[0], Batch[924], Train loss: 0.06718098372220993\n",
      "Epoch[0], Val loss: 0.05605809763073921\n",
      "Epoch[0], Batch[925], Train loss: 0.06828433275222778\n",
      "Epoch[0], Val loss: 0.055499203503131866\n",
      "Epoch[0], Batch[926], Train loss: 0.06579207628965378\n",
      "Epoch[0], Val loss: 0.06000123545527458\n",
      "Epoch[0], Batch[927], Train loss: 0.06480253487825394\n",
      "Epoch[0], Val loss: 0.05483943223953247\n",
      "Epoch[0], Batch[928], Train loss: 0.061407510191202164\n",
      "Epoch[0], Val loss: 0.0518716424703598\n",
      "Epoch[0], Batch[929], Train loss: 0.06786631792783737\n",
      "Epoch[0], Val loss: 0.05714201182126999\n",
      "Epoch[0], Batch[930], Train loss: 0.06693415343761444\n",
      "Epoch[0], Val loss: 0.0595320425927639\n",
      "Epoch[0], Batch[931], Train loss: 0.0633731335401535\n",
      "Epoch[0], Val loss: 0.052742745727300644\n",
      "Epoch[0], Batch[932], Train loss: 0.06494273245334625\n",
      "Epoch[0], Val loss: 0.05591049790382385\n",
      "Epoch[0], Batch[933], Train loss: 0.06259693950414658\n",
      "Epoch[0], Val loss: 0.055465057492256165\n",
      "Epoch[0], Batch[934], Train loss: 0.0617951974272728\n",
      "Epoch[0], Val loss: 0.057663463056087494\n",
      "Epoch[0], Batch[935], Train loss: 0.062461309134960175\n",
      "Epoch[0], Val loss: 0.05917578563094139\n",
      "Epoch[0], Batch[936], Train loss: 0.06592655926942825\n",
      "Epoch[0], Val loss: 0.05678286403417587\n",
      "Epoch[0], Batch[937], Train loss: 0.06497427076101303\n",
      "Epoch[0], Val loss: 0.05677541717886925\n",
      "Epoch[0], Batch[938], Train loss: 0.0641891285777092\n",
      "Epoch[0], Val loss: 0.05487622320652008\n",
      "Epoch[0], Batch[939], Train loss: 0.06592678278684616\n",
      "Epoch[0], Val loss: 0.054471731185913086\n",
      "Epoch[0], Batch[940], Train loss: 0.066578209400177\n",
      "Epoch[0], Val loss: 0.05495123937726021\n",
      "Epoch[0], Batch[941], Train loss: 0.06573894619941711\n",
      "Epoch[0], Val loss: 0.056820813566446304\n",
      "Epoch[0], Batch[942], Train loss: 0.06058817729353905\n",
      "Epoch[0], Val loss: 0.05606739595532417\n",
      "Epoch[0], Batch[943], Train loss: 0.06667561084032059\n",
      "Epoch[0], Val loss: 0.057371415197849274\n",
      "Epoch[0], Batch[944], Train loss: 0.06566324084997177\n",
      "Epoch[0], Val loss: 0.056099191308021545\n",
      "Epoch[0], Batch[945], Train loss: 0.06630479544401169\n",
      "Epoch[0], Val loss: 0.05619535595178604\n",
      "Epoch[0], Batch[946], Train loss: 0.06037180870771408\n",
      "Epoch[0], Val loss: 0.05907869711518288\n",
      "Epoch[0], Batch[947], Train loss: 0.06859837472438812\n",
      "Epoch[0], Val loss: 0.05287439748644829\n",
      "Epoch[0], Batch[948], Train loss: 0.062320299446582794\n",
      "Epoch[0], Val loss: 0.05678397789597511\n",
      "Epoch[0], Batch[949], Train loss: 0.06488171219825745\n",
      "Epoch[0], Val loss: 0.05503880977630615\n",
      "Epoch[0], Batch[950], Train loss: 0.06295756995677948\n",
      "Epoch[0], Val loss: 0.05445718392729759\n",
      "Epoch[0], Batch[951], Train loss: 0.06175004318356514\n",
      "Epoch[0], Val loss: 0.05212686210870743\n",
      "Epoch[0], Batch[952], Train loss: 0.0656203106045723\n",
      "Epoch[0], Val loss: 0.05542493984103203\n",
      "Epoch[0], Batch[953], Train loss: 0.05817092955112457\n",
      "Epoch[0], Val loss: 0.053850144147872925\n",
      "Epoch[0], Batch[954], Train loss: 0.06783892214298248\n",
      "Epoch[0], Val loss: 0.056310199201107025\n",
      "Epoch[0], Batch[955], Train loss: 0.06641726940870285\n",
      "Epoch[0], Val loss: 0.05472581833600998\n",
      "Epoch[0], Batch[956], Train loss: 0.061769306659698486\n",
      "Epoch[0], Val loss: 0.05784693360328674\n",
      "Epoch[0], Batch[957], Train loss: 0.0627477765083313\n",
      "Epoch[0], Val loss: 0.05548159033060074\n",
      "Epoch[0], Batch[958], Train loss: 0.06621984392404556\n",
      "Epoch[0], Val loss: 0.0578494556248188\n",
      "Epoch[0], Batch[959], Train loss: 0.06372024118900299\n",
      "Epoch[0], Val loss: 0.0561940036714077\n",
      "Epoch[0], Batch[960], Train loss: 0.06293687224388123\n",
      "Epoch[0], Val loss: 0.05951973423361778\n",
      "Epoch[0], Batch[961], Train loss: 0.06494331359863281\n",
      "Epoch[0], Val loss: 0.05533093586564064\n",
      "Epoch[0], Batch[962], Train loss: 0.0604068897664547\n",
      "Epoch[0], Val loss: 0.06032351404428482\n",
      "Epoch[0], Batch[963], Train loss: 0.06717555224895477\n",
      "Epoch[0], Val loss: 0.05689454451203346\n",
      "Epoch[0], Batch[964], Train loss: 0.06368589401245117\n",
      "Epoch[0], Val loss: 0.05525225028395653\n",
      "Epoch[0], Batch[965], Train loss: 0.0649113580584526\n",
      "Epoch[0], Val loss: 0.057111259549856186\n",
      "Epoch[0], Batch[966], Train loss: 0.06427199393510818\n",
      "Epoch[0], Val loss: 0.05593497306108475\n",
      "Epoch[0], Batch[967], Train loss: 0.06935494393110275\n",
      "Epoch[0], Val loss: 0.05834416672587395\n",
      "Epoch[0], Batch[968], Train loss: 0.06581145524978638\n",
      "Epoch[0], Val loss: 0.055218540132045746\n",
      "Epoch[0], Batch[969], Train loss: 0.061832066625356674\n",
      "Epoch[0], Val loss: 0.058294422924518585\n",
      "Epoch[0], Batch[970], Train loss: 0.0625523030757904\n",
      "Epoch[0], Val loss: 0.05827006325125694\n",
      "Epoch[0], Batch[971], Train loss: 0.06372910737991333\n",
      "Epoch[0], Val loss: 0.055041633546352386\n",
      "Epoch[0], Batch[972], Train loss: 0.06688634306192398\n",
      "Epoch[0], Val loss: 0.0634547770023346\n",
      "Epoch[0], Batch[973], Train loss: 0.0640166848897934\n",
      "Epoch[0], Val loss: 0.05741589888930321\n",
      "Epoch[0], Batch[974], Train loss: 0.06328083574771881\n",
      "Epoch[0], Val loss: 0.05594209209084511\n",
      "Epoch[0], Batch[975], Train loss: 0.059450503438711166\n",
      "Epoch[0], Val loss: 0.054480358958244324\n",
      "Epoch[0], Batch[976], Train loss: 0.06659016758203506\n",
      "Epoch[0], Val loss: 0.05320896953344345\n",
      "Epoch[0], Batch[977], Train loss: 0.06332328170537949\n",
      "Epoch[0], Val loss: 0.0529530830681324\n",
      "Epoch[0], Batch[978], Train loss: 0.06861887127161026\n",
      "Epoch[0], Val loss: 0.05561446025967598\n",
      "Epoch[0], Batch[979], Train loss: 0.057690732181072235\n",
      "Epoch[0], Val loss: 0.05321972072124481\n",
      "Epoch[0], Batch[980], Train loss: 0.06808225810527802\n",
      "Epoch[0], Val loss: 0.0626758262515068\n",
      "Epoch[0], Batch[981], Train loss: 0.0650855228304863\n",
      "Epoch[0], Val loss: 0.05629073455929756\n",
      "Epoch[0], Batch[982], Train loss: 0.0612153597176075\n",
      "Epoch[0], Val loss: 0.05251343548297882\n",
      "Epoch[0], Batch[983], Train loss: 0.06570935249328613\n",
      "Epoch[0], Val loss: 0.054161250591278076\n",
      "Epoch[0], Batch[984], Train loss: 0.06309395283460617\n",
      "Epoch[0], Val loss: 0.06040266528725624\n",
      "Epoch[0], Batch[985], Train loss: 0.0620243176817894\n",
      "Epoch[0], Val loss: 0.057366978377103806\n",
      "Epoch[0], Batch[986], Train loss: 0.0596543624997139\n",
      "Epoch[0], Val loss: 0.059846069663763046\n",
      "Epoch[0], Batch[987], Train loss: 0.06182241067290306\n",
      "Epoch[0], Val loss: 0.0581030510365963\n",
      "Epoch[0], Batch[988], Train loss: 0.06498946249485016\n",
      "Epoch[0], Val loss: 0.061815205961465836\n",
      "Epoch[0], Batch[989], Train loss: 0.06039866805076599\n",
      "Epoch[0], Val loss: 0.05995611101388931\n",
      "Epoch[0], Batch[990], Train loss: 0.060359373688697815\n",
      "Epoch[0], Val loss: 0.06174609437584877\n",
      "Epoch[0], Batch[991], Train loss: 0.06093035265803337\n",
      "Epoch[0], Val loss: 0.057913053780794144\n",
      "Epoch[0], Batch[992], Train loss: 0.06120207905769348\n",
      "Epoch[0], Val loss: 0.055239614099264145\n",
      "Epoch[0], Batch[993], Train loss: 0.06388761103153229\n",
      "Epoch[0], Val loss: 0.059211064130067825\n",
      "Epoch[0], Batch[994], Train loss: 0.05791674554347992\n",
      "Epoch[0], Val loss: 0.060394950211048126\n",
      "Epoch[0], Batch[995], Train loss: 0.06090257316827774\n",
      "Epoch[0], Val loss: 0.05437413975596428\n",
      "Epoch[0], Batch[996], Train loss: 0.06021479144692421\n",
      "Epoch[0], Val loss: 0.056867800652980804\n",
      "Epoch[0], Batch[997], Train loss: 0.059777453541755676\n",
      "Epoch[0], Val loss: 0.05511682480573654\n",
      "Epoch[0], Batch[998], Train loss: 0.06287674605846405\n",
      "Epoch[0], Val loss: 0.0587863028049469\n",
      "Epoch[0], Batch[999], Train loss: 0.06202273443341255\n",
      "Epoch[0], Val loss: 0.054140910506248474\n",
      "Epoch[0], Batch[1000], Train loss: 0.062256984412670135\n",
      "Epoch[0], Val loss: 0.056608546525239944\n",
      "Epoch[0], Batch[1001], Train loss: 0.06540178507566452\n",
      "Epoch[0], Val loss: 0.0581434965133667\n",
      "Epoch[0], Batch[1002], Train loss: 0.06052789092063904\n",
      "Epoch[0], Val loss: 0.054642967879772186\n",
      "Epoch[0], Batch[1003], Train loss: 0.06380459666252136\n",
      "Epoch[0], Val loss: 0.05535131320357323\n",
      "Epoch[0], Batch[1004], Train loss: 0.06114431098103523\n",
      "Epoch[0], Val loss: 0.06027708202600479\n",
      "Epoch[0], Batch[1005], Train loss: 0.062237728387117386\n",
      "Epoch[0], Val loss: 0.056921541690826416\n",
      "Epoch[0], Batch[1006], Train loss: 0.06563552469015121\n",
      "Epoch[0], Val loss: 0.05604599416255951\n",
      "Epoch[0], Batch[1007], Train loss: 0.05758408457040787\n",
      "Epoch[0], Val loss: 0.056543413549661636\n",
      "Epoch[0], Batch[1008], Train loss: 0.06259071081876755\n",
      "Epoch[0], Val loss: 0.05385545268654823\n",
      "Epoch[0], Batch[1009], Train loss: 0.06571082770824432\n",
      "Epoch[0], Val loss: 0.05599638447165489\n",
      "Epoch[0], Batch[1010], Train loss: 0.06261994689702988\n",
      "Epoch[0], Val loss: 0.055562131106853485\n",
      "Epoch[0], Batch[1011], Train loss: 0.06242760270833969\n",
      "Epoch[0], Val loss: 0.05407906696200371\n",
      "Epoch[0], Batch[1012], Train loss: 0.06563180685043335\n",
      "Epoch[0], Val loss: 0.053361836820840836\n",
      "Epoch[0], Batch[1013], Train loss: 0.06721168011426926\n",
      "Epoch[0], Val loss: 0.055924661457538605\n",
      "Epoch[0], Batch[1014], Train loss: 0.06047540530562401\n",
      "Epoch[0], Val loss: 0.05078817158937454\n",
      "Epoch[0], Batch[1015], Train loss: 0.059663984924554825\n",
      "Epoch[0], Val loss: 0.05281371250748634\n",
      "Epoch[0], Batch[1016], Train loss: 0.06381219625473022\n",
      "Epoch[0], Val loss: 0.05633952468633652\n",
      "Epoch[0], Batch[1017], Train loss: 0.06307770311832428\n",
      "Epoch[0], Val loss: 0.05329476669430733\n",
      "Epoch[0], Batch[1018], Train loss: 0.06150856614112854\n",
      "Epoch[0], Val loss: 0.054689567536115646\n",
      "Epoch[0], Batch[1019], Train loss: 0.056154515594244\n",
      "Epoch[0], Val loss: 0.054784879088401794\n",
      "Epoch[0], Batch[1020], Train loss: 0.06259024143218994\n",
      "Epoch[0], Val loss: 0.05630122125148773\n",
      "Epoch[0], Batch[1021], Train loss: 0.06209058314561844\n",
      "Epoch[0], Val loss: 0.052181944251060486\n",
      "Epoch[0], Batch[1022], Train loss: 0.06263430416584015\n",
      "Epoch[0], Val loss: 0.05740252509713173\n",
      "Epoch[0], Batch[1023], Train loss: 0.06179548427462578\n",
      "Epoch[0], Val loss: 0.05705596134066582\n",
      "Epoch[0], Batch[1024], Train loss: 0.0660557821393013\n",
      "Epoch[0], Val loss: 0.05569194629788399\n",
      "Epoch[0], Batch[1025], Train loss: 0.06697985529899597\n",
      "Epoch[0], Val loss: 0.05404966324567795\n",
      "Epoch[0], Batch[1026], Train loss: 0.06046884506940842\n",
      "Epoch[0], Val loss: 0.05297901853919029\n",
      "Epoch[0], Batch[1027], Train loss: 0.06109239161014557\n",
      "Epoch[0], Val loss: 0.060847342014312744\n",
      "Epoch[0], Batch[1028], Train loss: 0.06031080707907677\n",
      "Epoch[0], Val loss: 0.05701909959316254\n",
      "Epoch[0], Batch[1029], Train loss: 0.059515852481126785\n",
      "Epoch[0], Val loss: 0.05399617552757263\n",
      "Epoch[0], Batch[1030], Train loss: 0.06083996221423149\n",
      "Epoch[0], Val loss: 0.05582248046994209\n",
      "Epoch[0], Batch[1031], Train loss: 0.059229131788015366\n",
      "Epoch[0], Val loss: 0.05600983649492264\n",
      "Epoch[0], Batch[1032], Train loss: 0.06376887112855911\n",
      "Epoch[0], Val loss: 0.05381283909082413\n",
      "Epoch[0], Batch[1033], Train loss: 0.0588783323764801\n",
      "Epoch[0], Val loss: 0.053390730172395706\n",
      "Epoch[0], Batch[1034], Train loss: 0.059153106063604355\n",
      "Epoch[0], Val loss: 0.05561884120106697\n",
      "Epoch[0], Batch[1035], Train loss: 0.06335432082414627\n",
      "Epoch[0], Val loss: 0.05759240686893463\n",
      "Epoch[0], Batch[1036], Train loss: 0.06173662841320038\n",
      "Epoch[0], Val loss: 0.05329460650682449\n",
      "Epoch[0], Batch[1037], Train loss: 0.06209872290492058\n",
      "Epoch[0], Val loss: 0.05808393284678459\n",
      "Epoch[0], Batch[1038], Train loss: 0.06475551426410675\n",
      "Epoch[0], Val loss: 0.05444982647895813\n",
      "Epoch[0], Batch[1039], Train loss: 0.05853049457073212\n",
      "Epoch[0], Val loss: 0.05326372757554054\n",
      "Epoch[0], Batch[1040], Train loss: 0.06214601546525955\n",
      "Epoch[0], Val loss: 0.05512825772166252\n",
      "Epoch[0], Batch[1041], Train loss: 0.058621931821107864\n",
      "Epoch[0], Val loss: 0.052163250744342804\n",
      "Epoch[0], Batch[1042], Train loss: 0.061550673097372055\n",
      "Epoch[0], Val loss: 0.056371163576841354\n",
      "Epoch[0], Batch[1043], Train loss: 0.06820645928382874\n",
      "Epoch[0], Val loss: 0.05613456293940544\n",
      "Epoch[0], Batch[1044], Train loss: 0.05819457024335861\n",
      "Epoch[0], Val loss: 0.05561821162700653\n",
      "Epoch[0], Batch[1045], Train loss: 0.05904000252485275\n",
      "Epoch[0], Val loss: 0.06024644151329994\n",
      "Epoch[0], Batch[1046], Train loss: 0.06034740060567856\n",
      "Epoch[0], Val loss: 0.0552542544901371\n",
      "Epoch[0], Batch[1047], Train loss: 0.06274684518575668\n",
      "Epoch[0], Val loss: 0.059371430426836014\n",
      "Epoch[0], Batch[1048], Train loss: 0.0652448832988739\n",
      "Epoch[0], Val loss: 0.05421546846628189\n",
      "Epoch[0], Batch[1049], Train loss: 0.060644835233688354\n",
      "Epoch[0], Val loss: 0.059019871056079865\n",
      "Epoch[0], Batch[1050], Train loss: 0.060407642275094986\n",
      "Epoch[0], Val loss: 0.05628170818090439\n",
      "Epoch[0], Batch[1051], Train loss: 0.055646687746047974\n",
      "Epoch[0], Val loss: 0.05741061270236969\n",
      "Epoch[0], Batch[1052], Train loss: 0.05966566875576973\n",
      "Epoch[0], Val loss: 0.055026303976774216\n",
      "Epoch[0], Batch[1053], Train loss: 0.06627420336008072\n",
      "Epoch[0], Val loss: 0.05301464721560478\n",
      "Epoch[0], Batch[1054], Train loss: 0.06306034326553345\n",
      "Epoch[0], Val loss: 0.05257367715239525\n",
      "Epoch[0], Batch[1055], Train loss: 0.05962777137756348\n",
      "Epoch[0], Val loss: 0.05378194525837898\n",
      "Epoch[0], Batch[1056], Train loss: 0.061543483287096024\n",
      "Epoch[0], Val loss: 0.0556788444519043\n",
      "Epoch[0], Batch[1057], Train loss: 0.06175332888960838\n",
      "Epoch[0], Val loss: 0.051942043006420135\n",
      "Epoch[0], Batch[1058], Train loss: 0.06120854243636131\n",
      "Epoch[0], Val loss: 0.056153297424316406\n",
      "Epoch[0], Batch[1059], Train loss: 0.05837031081318855\n",
      "Epoch[0], Val loss: 0.055015381425619125\n",
      "Epoch[0], Batch[1060], Train loss: 0.06410954147577286\n",
      "Epoch[0], Val loss: 0.05537106469273567\n",
      "Epoch[0], Batch[1061], Train loss: 0.06210147216916084\n",
      "Epoch[0], Val loss: 0.05467992275953293\n",
      "Epoch[0], Batch[1062], Train loss: 0.06842072308063507\n",
      "Epoch[0], Val loss: 0.05484472215175629\n",
      "Epoch[0], Batch[1063], Train loss: 0.05761577561497688\n",
      "Epoch[0], Val loss: 0.057440899312496185\n",
      "Epoch[0], Batch[1064], Train loss: 0.061503808945417404\n",
      "Epoch[0], Val loss: 0.05279872566461563\n",
      "Epoch[0], Batch[1065], Train loss: 0.056632377207279205\n",
      "Epoch[0], Val loss: 0.05651450157165527\n",
      "Epoch[0], Batch[1066], Train loss: 0.06105377525091171\n",
      "Epoch[0], Val loss: 0.05317576229572296\n",
      "Epoch[0], Batch[1067], Train loss: 0.059491973370313644\n",
      "Epoch[0], Val loss: 0.056769970804452896\n",
      "Epoch[0], Batch[1068], Train loss: 0.06428559124469757\n",
      "Epoch[0], Val loss: 0.05182618275284767\n",
      "Epoch[0], Batch[1069], Train loss: 0.05855855718255043\n",
      "Epoch[0], Val loss: 0.0561816431581974\n",
      "Epoch[0], Batch[1070], Train loss: 0.06028206646442413\n",
      "Epoch[0], Val loss: 0.05717213451862335\n",
      "Epoch[0], Batch[1071], Train loss: 0.06129603460431099\n",
      "Epoch[0], Val loss: 0.05378812924027443\n",
      "Epoch[0], Batch[1072], Train loss: 0.06075533851981163\n",
      "Epoch[0], Val loss: 0.05700640380382538\n",
      "Epoch[0], Batch[1073], Train loss: 0.06274279206991196\n",
      "Epoch[0], Val loss: 0.05529748275876045\n",
      "Epoch[0], Batch[1074], Train loss: 0.05857996642589569\n",
      "Epoch[0], Val loss: 0.054928094148635864\n",
      "Epoch[0], Batch[1075], Train loss: 0.05822371318936348\n",
      "Epoch[0], Val loss: 0.051373548805713654\n",
      "Epoch[0], Batch[1076], Train loss: 0.054625242948532104\n",
      "Epoch[0], Val loss: 0.05300052836537361\n",
      "Epoch[0], Batch[1077], Train loss: 0.060836002230644226\n",
      "Epoch[0], Val loss: 0.052938587963581085\n",
      "Epoch[0], Batch[1078], Train loss: 0.0627441555261612\n",
      "Epoch[0], Val loss: 0.053375959396362305\n",
      "Epoch[0], Batch[1079], Train loss: 0.05953357741236687\n",
      "Epoch[0], Val loss: 0.05874163657426834\n",
      "Epoch[0], Batch[1080], Train loss: 0.05732651799917221\n",
      "Epoch[0], Val loss: 0.05264287814497948\n",
      "Epoch[0], Batch[1081], Train loss: 0.06252794712781906\n",
      "Epoch[0], Val loss: 0.05316225066781044\n",
      "Epoch[0], Batch[1082], Train loss: 0.0636710599064827\n",
      "Epoch[0], Val loss: 0.05691713094711304\n",
      "Epoch[0], Batch[1083], Train loss: 0.06175714358687401\n",
      "Epoch[0], Val loss: 0.05364150181412697\n",
      "Epoch[0], Batch[1084], Train loss: 0.06098267063498497\n",
      "Epoch[0], Val loss: 0.048688653856515884\n",
      "Epoch[0], Batch[1085], Train loss: 0.06283082813024521\n",
      "Epoch[0], Val loss: 0.05093276500701904\n",
      "Epoch[0], Batch[1086], Train loss: 0.0552876815199852\n",
      "Epoch[0], Val loss: 0.054319825023412704\n",
      "Epoch[0], Batch[1087], Train loss: 0.0632636621594429\n",
      "Epoch[0], Val loss: 0.05296811833977699\n",
      "Epoch[0], Batch[1088], Train loss: 0.0545787550508976\n",
      "Epoch[0], Val loss: 0.04937344416975975\n",
      "Epoch[0], Batch[1089], Train loss: 0.06158421188592911\n",
      "Epoch[0], Val loss: 0.05220567435026169\n",
      "Epoch[0], Batch[1090], Train loss: 0.05966212972998619\n",
      "Epoch[0], Val loss: 0.05453033372759819\n",
      "Epoch[0], Batch[1091], Train loss: 0.05920557305216789\n",
      "Epoch[0], Val loss: 0.056140221655368805\n",
      "Epoch[0], Batch[1092], Train loss: 0.058870065957307816\n",
      "Epoch[0], Val loss: 0.05402204766869545\n",
      "Epoch[0], Batch[1093], Train loss: 0.05931878462433815\n",
      "Epoch[0], Val loss: 0.05649653822183609\n",
      "Epoch[0], Batch[1094], Train loss: 0.05964894965291023\n",
      "Epoch[0], Val loss: 0.050487302243709564\n",
      "Epoch[0], Batch[1095], Train loss: 0.06151757016777992\n",
      "Epoch[0], Val loss: 0.05271591246128082\n",
      "Epoch[0], Batch[1096], Train loss: 0.058524638414382935\n",
      "Epoch[0], Val loss: 0.05431053414940834\n",
      "Epoch[0], Batch[1097], Train loss: 0.05724947899580002\n",
      "Epoch[0], Val loss: 0.05704363062977791\n",
      "Epoch[0], Batch[1098], Train loss: 0.057170312851667404\n",
      "Epoch[0], Val loss: 0.05356208235025406\n",
      "Epoch[0], Batch[1099], Train loss: 0.056175749748945236\n",
      "Epoch[0], Val loss: 0.05538784712553024\n",
      "Epoch[0], Batch[1100], Train loss: 0.058492712676525116\n",
      "Epoch[0], Val loss: 0.054020319133996964\n",
      "Epoch[0], Batch[1101], Train loss: 0.060801852494478226\n",
      "Epoch[0], Val loss: 0.054241031408309937\n",
      "Epoch[0], Batch[1102], Train loss: 0.0642768144607544\n",
      "Epoch[0], Val loss: 0.05481546372175217\n",
      "Epoch[0], Batch[1103], Train loss: 0.05707312747836113\n",
      "Epoch[0], Val loss: 0.053011439740657806\n",
      "Epoch[0], Batch[1104], Train loss: 0.05746173486113548\n",
      "Epoch[0], Val loss: 0.05382285267114639\n",
      "Epoch[0], Batch[1105], Train loss: 0.05974895507097244\n",
      "Epoch[0], Val loss: 0.05251588299870491\n",
      "Epoch[0], Batch[1106], Train loss: 0.061100881546735764\n",
      "Epoch[0], Val loss: 0.057594623416662216\n",
      "Epoch[0], Batch[1107], Train loss: 0.0641583800315857\n",
      "Epoch[0], Val loss: 0.05432305112481117\n",
      "Epoch[0], Batch[1108], Train loss: 0.05923609063029289\n",
      "Epoch[0], Val loss: 0.05597277730703354\n",
      "Epoch[0], Batch[1109], Train loss: 0.05796507000923157\n",
      "Epoch[0], Val loss: 0.052769217640161514\n",
      "Epoch[0], Batch[1110], Train loss: 0.06163468956947327\n",
      "Epoch[0], Val loss: 0.0539281852543354\n",
      "Epoch[0], Batch[1111], Train loss: 0.06146210432052612\n",
      "Epoch[0], Val loss: 0.054350655525922775\n",
      "Epoch[0], Batch[1112], Train loss: 0.05959704518318176\n",
      "Epoch[0], Val loss: 0.05808768793940544\n",
      "Epoch[0], Batch[1113], Train loss: 0.06156136095523834\n",
      "Epoch[0], Val loss: 0.053359728306531906\n",
      "Epoch[0], Batch[1114], Train loss: 0.05902658775448799\n",
      "Epoch[0], Val loss: 0.050909098237752914\n",
      "Epoch[0], Batch[1115], Train loss: 0.0577375665307045\n",
      "Epoch[0], Val loss: 0.05187280848622322\n",
      "Epoch[0], Batch[1116], Train loss: 0.05713571980595589\n",
      "Epoch[0], Val loss: 0.057144831866025925\n",
      "Epoch[0], Batch[1117], Train loss: 0.06354857236146927\n",
      "Epoch[0], Val loss: 0.05590469390153885\n",
      "Epoch[0], Batch[1118], Train loss: 0.060454193502664566\n",
      "Epoch[0], Val loss: 0.055844616144895554\n",
      "Epoch[0], Batch[1119], Train loss: 0.05970960110425949\n",
      "Epoch[0], Val loss: 0.05555710941553116\n",
      "Epoch[0], Batch[1120], Train loss: 0.059383638203144073\n",
      "Epoch[0], Val loss: 0.05346987396478653\n",
      "Epoch[0], Batch[1121], Train loss: 0.05917639285326004\n",
      "Epoch[0], Val loss: 0.054520849138498306\n",
      "Epoch[0], Batch[1122], Train loss: 0.060808926820755005\n",
      "Epoch[0], Val loss: 0.053682588040828705\n",
      "Epoch[0], Batch[1123], Train loss: 0.05823657289147377\n",
      "Epoch[0], Val loss: 0.049312882125377655\n",
      "Epoch[0], Batch[1124], Train loss: 0.05568935349583626\n",
      "Epoch[0], Val loss: 0.05583340674638748\n",
      "Epoch[0], Batch[1125], Train loss: 0.06096614897251129\n",
      "Epoch[0], Val loss: 0.05656338855624199\n",
      "Epoch[0], Batch[1126], Train loss: 0.062040138989686966\n",
      "Epoch[0], Val loss: 0.05382530018687248\n",
      "Epoch[0], Batch[1127], Train loss: 0.061779092997312546\n",
      "Epoch[0], Val loss: 0.05415981635451317\n",
      "Epoch[0], Batch[1128], Train loss: 0.056974925100803375\n",
      "Epoch[0], Val loss: 0.05306940898299217\n",
      "Epoch[0], Batch[1129], Train loss: 0.06057524308562279\n",
      "Epoch[0], Val loss: 0.05395366623997688\n",
      "Epoch[0], Batch[1130], Train loss: 0.06109895557165146\n",
      "Epoch[0], Val loss: 0.055911559611558914\n",
      "Epoch[0], Batch[1131], Train loss: 0.060521066188812256\n",
      "Epoch[0], Val loss: 0.056171685457229614\n",
      "Epoch[0], Batch[1132], Train loss: 0.060873840004205704\n",
      "Epoch[0], Val loss: 0.05604273080825806\n",
      "Epoch[0], Batch[1133], Train loss: 0.05678641423583031\n",
      "Epoch[0], Val loss: 0.05430504307150841\n",
      "Epoch[0], Batch[1134], Train loss: 0.056024447083473206\n",
      "Epoch[0], Val loss: 0.05291253700852394\n",
      "Epoch[0], Batch[1135], Train loss: 0.05942412093281746\n",
      "Epoch[0], Val loss: 0.054953865706920624\n",
      "Epoch[0], Batch[1136], Train loss: 0.0556856207549572\n",
      "Epoch[0], Val loss: 0.05654269829392433\n",
      "Epoch[0], Batch[1137], Train loss: 0.05662810429930687\n",
      "Epoch[0], Val loss: 0.052365992218256\n",
      "Epoch[0], Batch[1138], Train loss: 0.06034819781780243\n",
      "Epoch[0], Val loss: 0.053600162267684937\n",
      "Epoch[0], Batch[1139], Train loss: 0.059577472507953644\n",
      "Epoch[0], Val loss: 0.051864124834537506\n",
      "Epoch[0], Batch[1140], Train loss: 0.05861353874206543\n",
      "Epoch[0], Val loss: 0.04947595298290253\n",
      "Epoch[0], Batch[1141], Train loss: 0.06621883809566498\n",
      "Epoch[0], Val loss: 0.05584431812167168\n",
      "Epoch[0], Batch[1142], Train loss: 0.056279610842466354\n",
      "Epoch[0], Val loss: 0.05450241267681122\n",
      "Epoch[0], Batch[1143], Train loss: 0.05551624298095703\n",
      "Epoch[0], Val loss: 0.05704803019762039\n",
      "Epoch[0], Batch[1144], Train loss: 0.0594865046441555\n",
      "Epoch[0], Val loss: 0.0492977499961853\n",
      "Epoch[0], Batch[1145], Train loss: 0.05951327085494995\n",
      "Epoch[0], Val loss: 0.050872236490249634\n",
      "Epoch[0], Batch[1146], Train loss: 0.05640389770269394\n",
      "Epoch[0], Val loss: 0.054746437817811966\n",
      "Epoch[0], Batch[1147], Train loss: 0.05796493589878082\n",
      "Epoch[0], Val loss: 0.053907137364149094\n",
      "Epoch[0], Batch[1148], Train loss: 0.06536785513162613\n",
      "Epoch[0], Val loss: 0.05413086712360382\n",
      "Epoch[0], Batch[1149], Train loss: 0.05959821119904518\n",
      "Epoch[0], Val loss: 0.051206689327955246\n",
      "Epoch[0], Batch[1150], Train loss: 0.05937366932630539\n",
      "Epoch[0], Val loss: 0.05333605408668518\n",
      "Epoch[0], Batch[1151], Train loss: 0.056905459612607956\n",
      "Epoch[0], Val loss: 0.05358964204788208\n",
      "Epoch[0], Batch[1152], Train loss: 0.05585859715938568\n",
      "Epoch[0], Val loss: 0.052589256316423416\n",
      "Epoch[0], Batch[1153], Train loss: 0.0631609782576561\n",
      "Epoch[0], Val loss: 0.052860062569379807\n",
      "Epoch[0], Batch[1154], Train loss: 0.0582265704870224\n",
      "Epoch[0], Val loss: 0.05399559810757637\n",
      "Epoch[0], Batch[1155], Train loss: 0.06490588188171387\n",
      "Epoch[0], Val loss: 0.051608894020318985\n",
      "Epoch[0], Batch[1156], Train loss: 0.0603509247303009\n",
      "Epoch[0], Val loss: 0.057791467756032944\n",
      "Epoch[0], Batch[1157], Train loss: 0.05866982042789459\n",
      "Epoch[0], Val loss: 0.05633125081658363\n",
      "Epoch[0], Batch[1158], Train loss: 0.06324353069067001\n",
      "Epoch[0], Val loss: 0.0541689470410347\n",
      "Epoch[0], Batch[1159], Train loss: 0.05899825692176819\n",
      "Epoch[0], Val loss: 0.051704537123441696\n",
      "Epoch[0], Batch[1160], Train loss: 0.058338623493909836\n",
      "Epoch[0], Val loss: 0.05001022294163704\n",
      "Epoch[0], Batch[1161], Train loss: 0.059636764228343964\n",
      "Epoch[0], Val loss: 0.05327114462852478\n",
      "Epoch[0], Batch[1162], Train loss: 0.06089925393462181\n",
      "Epoch[0], Val loss: 0.05367389693856239\n",
      "Epoch[0], Batch[1163], Train loss: 0.05759993940591812\n",
      "Epoch[0], Val loss: 0.052224062383174896\n",
      "Epoch[0], Batch[1164], Train loss: 0.057766832411289215\n",
      "Epoch[0], Val loss: 0.05415303632616997\n",
      "Epoch[0], Batch[1165], Train loss: 0.056097958236932755\n",
      "Epoch[0], Val loss: 0.057888664305210114\n",
      "Epoch[0], Batch[1166], Train loss: 0.060914140194654465\n",
      "Epoch[0], Val loss: 0.050314076244831085\n",
      "Epoch[0], Batch[1167], Train loss: 0.057948723435401917\n",
      "Epoch[0], Val loss: 0.05441241338849068\n",
      "Epoch[0], Batch[1168], Train loss: 0.05999128893017769\n",
      "Epoch[0], Val loss: 0.05224001407623291\n",
      "Epoch[0], Batch[1169], Train loss: 0.05906227231025696\n",
      "Epoch[0], Val loss: 0.055068764835596085\n",
      "Epoch[0], Batch[1170], Train loss: 0.05895069241523743\n",
      "Epoch[0], Val loss: 0.05529465898871422\n",
      "Epoch[0], Batch[1171], Train loss: 0.05942344292998314\n",
      "Epoch[0], Val loss: 0.054878052324056625\n",
      "Epoch[0], Batch[1172], Train loss: 0.05980456620454788\n",
      "Epoch[0], Val loss: 0.05328560993075371\n",
      "Epoch[0], Batch[1173], Train loss: 0.0567801408469677\n",
      "Epoch[0], Val loss: 0.05104868859052658\n",
      "Epoch[0], Batch[1174], Train loss: 0.057926081120967865\n",
      "Epoch[0], Val loss: 0.053391262888908386\n",
      "Epoch[0], Batch[1175], Train loss: 0.057410430163145065\n",
      "Epoch[0], Val loss: 0.05588175356388092\n",
      "Epoch[0], Batch[1176], Train loss: 0.057365939021110535\n",
      "Epoch[0], Val loss: 0.056608837097883224\n",
      "Epoch[0], Batch[1177], Train loss: 0.06003172695636749\n",
      "Epoch[0], Val loss: 0.05238410457968712\n",
      "Epoch[0], Batch[1178], Train loss: 0.05923561006784439\n",
      "Epoch[0], Val loss: 0.05186758190393448\n",
      "Epoch[0], Batch[1179], Train loss: 0.05479924753308296\n",
      "Epoch[0], Val loss: 0.054990850389003754\n",
      "Epoch[0], Batch[1180], Train loss: 0.05893564224243164\n",
      "Epoch[0], Val loss: 0.05168880894780159\n",
      "Epoch[0], Batch[1181], Train loss: 0.057428207248449326\n",
      "Epoch[0], Val loss: 0.04920303821563721\n",
      "Epoch[0], Batch[1182], Train loss: 0.05830158665776253\n",
      "Epoch[0], Val loss: 0.05245118960738182\n",
      "Epoch[0], Batch[1183], Train loss: 0.060721367597579956\n",
      "Epoch[0], Val loss: 0.05264054983854294\n",
      "Epoch[0], Batch[1184], Train loss: 0.06077215075492859\n",
      "Epoch[0], Val loss: 0.05392846837639809\n",
      "Epoch[0], Batch[1185], Train loss: 0.05830945074558258\n",
      "Epoch[0], Val loss: 0.05416882410645485\n",
      "Epoch[0], Batch[1186], Train loss: 0.05323517322540283\n",
      "Epoch[0], Val loss: 0.052247174084186554\n",
      "Epoch[0], Batch[1187], Train loss: 0.05775368586182594\n",
      "Epoch[0], Val loss: 0.0542646087706089\n",
      "Epoch[0], Batch[1188], Train loss: 0.05707336217164993\n",
      "Epoch[0], Val loss: 0.05330352112650871\n",
      "Epoch[0], Batch[1189], Train loss: 0.059568244963884354\n",
      "Epoch[0], Val loss: 0.050914742052555084\n",
      "Epoch[0], Batch[1190], Train loss: 0.05968071520328522\n",
      "Epoch[0], Val loss: 0.0533796064555645\n",
      "Epoch[0], Batch[1191], Train loss: 0.05520285293459892\n",
      "Epoch[0], Val loss: 0.05480168014764786\n",
      "Epoch[0], Batch[1192], Train loss: 0.055853221565485\n",
      "Epoch[0], Val loss: 0.05943121016025543\n",
      "Epoch[0], Batch[1193], Train loss: 0.05615749955177307\n",
      "Epoch[0], Val loss: 0.0511142797768116\n",
      "Epoch[0], Batch[1194], Train loss: 0.053067389875650406\n",
      "Epoch[0], Val loss: 0.05068570002913475\n",
      "Epoch[0], Batch[1195], Train loss: 0.05825565382838249\n",
      "Epoch[0], Val loss: 0.05482352897524834\n",
      "Epoch[0], Batch[1196], Train loss: 0.05879402160644531\n",
      "Epoch[0], Val loss: 0.05198288708925247\n",
      "Epoch[0], Batch[1197], Train loss: 0.05691114813089371\n",
      "Epoch[0], Val loss: 0.05058461055159569\n",
      "Epoch[0], Batch[1198], Train loss: 0.05452136695384979\n",
      "Epoch[0], Val loss: 0.050442829728126526\n",
      "Epoch[0], Batch[1199], Train loss: 0.06333926320075989\n",
      "Epoch[0], Val loss: 0.05112752318382263\n",
      "Epoch[0], Batch[1200], Train loss: 0.05547795072197914\n",
      "Epoch[0], Val loss: 0.05434926599264145\n",
      "Epoch[0], Batch[1201], Train loss: 0.060226473957300186\n",
      "Epoch[0], Val loss: 0.053053487092256546\n",
      "Epoch[0], Batch[1202], Train loss: 0.058813080191612244\n",
      "Epoch[0], Val loss: 0.0577671192586422\n",
      "Epoch[0], Batch[1203], Train loss: 0.06512616574764252\n",
      "Epoch[0], Val loss: 0.049599919468164444\n",
      "Epoch[0], Batch[1204], Train loss: 0.055289722979068756\n",
      "Epoch[0], Val loss: 0.057171531021595\n",
      "Epoch[0], Batch[1205], Train loss: 0.05623818561434746\n",
      "Epoch[0], Val loss: 0.05086474493145943\n",
      "Epoch[0], Batch[1206], Train loss: 0.056903135031461716\n",
      "Epoch[0], Val loss: 0.05254071578383446\n",
      "Epoch[0], Batch[1207], Train loss: 0.05899624526500702\n",
      "Epoch[0], Val loss: 0.05610968917608261\n",
      "Epoch[0], Batch[1208], Train loss: 0.061685699969530106\n",
      "Epoch[0], Val loss: 0.05315689742565155\n",
      "Epoch[0], Batch[1209], Train loss: 0.05523509159684181\n",
      "Epoch[0], Val loss: 0.051570091396570206\n",
      "Epoch[0], Batch[1210], Train loss: 0.05695607140660286\n",
      "Epoch[0], Val loss: 0.05478302761912346\n",
      "Epoch[0], Batch[1211], Train loss: 0.05852983146905899\n",
      "Epoch[0], Val loss: 0.053453534841537476\n",
      "Epoch[0], Batch[1212], Train loss: 0.05775732547044754\n",
      "Epoch[0], Val loss: 0.053777873516082764\n",
      "Epoch[0], Batch[1213], Train loss: 0.05567609891295433\n",
      "Epoch[0], Val loss: 0.051448624581098557\n",
      "Epoch[0], Batch[1214], Train loss: 0.056104108691215515\n",
      "Epoch[0], Val loss: 0.05322114750742912\n",
      "Epoch[0], Batch[1215], Train loss: 0.05958161875605583\n",
      "Epoch[0], Val loss: 0.05167339742183685\n",
      "Epoch[0], Batch[1216], Train loss: 0.05708217993378639\n",
      "Epoch[0], Val loss: 0.0550050213932991\n",
      "Epoch[0], Batch[1217], Train loss: 0.05788680166006088\n",
      "Epoch[0], Val loss: 0.053875893354415894\n",
      "Epoch[0], Batch[1218], Train loss: 0.05769963935017586\n",
      "Epoch[0], Val loss: 0.05119503289461136\n",
      "Epoch[0], Batch[1219], Train loss: 0.05928613618016243\n",
      "Epoch[0], Val loss: 0.055503636598587036\n",
      "Epoch[0], Batch[1220], Train loss: 0.05565016344189644\n",
      "Epoch[0], Val loss: 0.05499844625592232\n",
      "Epoch[0], Batch[1221], Train loss: 0.05929899960756302\n",
      "Epoch[0], Val loss: 0.05581549555063248\n",
      "Epoch[0], Batch[1222], Train loss: 0.057324446737766266\n",
      "Epoch[0], Val loss: 0.0531633123755455\n",
      "Epoch[0], Batch[1223], Train loss: 0.05613642558455467\n",
      "Epoch[0], Val loss: 0.053979892283678055\n",
      "Epoch[0], Batch[1224], Train loss: 0.054721806198358536\n",
      "Epoch[0], Val loss: 0.05301005020737648\n",
      "Epoch[0], Batch[1225], Train loss: 0.060545384883880615\n",
      "Epoch[0], Val loss: 0.051610782742500305\n",
      "Epoch[0], Batch[1226], Train loss: 0.06302112340927124\n",
      "Epoch[0], Val loss: 0.0519699826836586\n",
      "Epoch[0], Batch[1227], Train loss: 0.05859490483999252\n",
      "Epoch[0], Val loss: 0.05214226245880127\n",
      "Epoch[0], Batch[1228], Train loss: 0.05673746392130852\n",
      "Epoch[0], Val loss: 0.05135434493422508\n",
      "Epoch[0], Batch[1229], Train loss: 0.05899176746606827\n",
      "Epoch[0], Val loss: 0.05232828110456467\n",
      "Epoch[0], Batch[1230], Train loss: 0.0531521774828434\n",
      "Epoch[0], Val loss: 0.05330941081047058\n",
      "Epoch[0], Batch[1231], Train loss: 0.05622435733675957\n",
      "Epoch[0], Val loss: 0.05124305561184883\n",
      "Epoch[0], Batch[1232], Train loss: 0.05600365623831749\n",
      "Epoch[0], Val loss: 0.057006027549505234\n",
      "Epoch[0], Batch[1233], Train loss: 0.0579906664788723\n",
      "Epoch[0], Val loss: 0.05370587110519409\n",
      "Epoch[0], Batch[1234], Train loss: 0.06064566597342491\n",
      "Epoch[0], Val loss: 0.05643593892455101\n",
      "Epoch[0], Batch[1235], Train loss: 0.059165555983781815\n",
      "Epoch[0], Val loss: 0.05343318730592728\n",
      "Epoch[0], Batch[1236], Train loss: 0.054306771606206894\n",
      "Epoch[0], Val loss: 0.05100470781326294\n",
      "Epoch[0], Batch[1237], Train loss: 0.05626290291547775\n",
      "Epoch[0], Val loss: 0.05033870413899422\n",
      "Epoch[0], Batch[1238], Train loss: 0.059634141623973846\n",
      "Epoch[0], Val loss: 0.05387798324227333\n",
      "Epoch[0], Batch[1239], Train loss: 0.056783851236104965\n",
      "Epoch[0], Val loss: 0.05365993082523346\n",
      "Epoch[0], Batch[1240], Train loss: 0.056572869420051575\n",
      "Epoch[0], Val loss: 0.05415390804409981\n",
      "Epoch[0], Batch[1241], Train loss: 0.0557532012462616\n",
      "Epoch[0], Val loss: 0.052709031850099564\n",
      "Epoch[0], Batch[1242], Train loss: 0.05636870115995407\n",
      "Epoch[0], Val loss: 0.05268983170390129\n",
      "Epoch[0], Batch[1243], Train loss: 0.057725150138139725\n",
      "Epoch[0], Val loss: 0.05357806757092476\n",
      "Epoch[0], Batch[1244], Train loss: 0.06010601297020912\n",
      "Epoch[0], Val loss: 0.05266301706433296\n",
      "Epoch[0], Batch[1245], Train loss: 0.05485619604587555\n",
      "Epoch[0], Val loss: 0.05697844922542572\n",
      "Epoch[0], Batch[1246], Train loss: 0.057471420615911484\n",
      "Epoch[0], Val loss: 0.05345228314399719\n",
      "Epoch[0], Batch[1247], Train loss: 0.054427556693553925\n",
      "Epoch[0], Val loss: 0.05742596834897995\n",
      "Epoch[0], Batch[1248], Train loss: 0.0565112940967083\n",
      "Epoch[0], Val loss: 0.05165741592645645\n",
      "Epoch[0], Batch[1249], Train loss: 0.054813679307699203\n",
      "Epoch[0], Val loss: 0.04950425773859024\n",
      "Epoch[0], Batch[1250], Train loss: 0.05760511755943298\n",
      "Epoch[0], Val loss: 0.0506674125790596\n",
      "Epoch[0], Batch[1251], Train loss: 0.05565131828188896\n",
      "Epoch[0], Val loss: 0.05202338099479675\n",
      "Epoch[0], Batch[1252], Train loss: 0.053521059453487396\n",
      "Epoch[0], Val loss: 0.05692240223288536\n",
      "Epoch[0], Batch[1253], Train loss: 0.05476843938231468\n",
      "Epoch[0], Val loss: 0.052310507744550705\n",
      "Epoch[0], Batch[1254], Train loss: 0.05784367024898529\n",
      "Epoch[0], Val loss: 0.056841522455215454\n",
      "Epoch[0], Batch[1255], Train loss: 0.05646854266524315\n",
      "Epoch[0], Val loss: 0.05444132909178734\n",
      "Epoch[0], Batch[1256], Train loss: 0.05813732370734215\n",
      "Epoch[0], Val loss: 0.05553186684846878\n",
      "Epoch[0], Batch[1257], Train loss: 0.05512773245573044\n",
      "Epoch[0], Val loss: 0.04952973127365112\n",
      "Epoch[0], Batch[1258], Train loss: 0.057118333876132965\n",
      "Epoch[0], Val loss: 0.052163444459438324\n",
      "Epoch[0], Batch[1259], Train loss: 0.05764058977365494\n",
      "Epoch[0], Val loss: 0.05154884234070778\n",
      "Epoch[0], Batch[1260], Train loss: 0.05965094268321991\n",
      "Epoch[0], Val loss: 0.051295991986989975\n",
      "Epoch[0], Batch[1261], Train loss: 0.05968327447772026\n",
      "Epoch[0], Val loss: 0.05131073668599129\n",
      "Epoch[0], Batch[1262], Train loss: 0.05519916117191315\n",
      "Epoch[0], Val loss: 0.051408104598522186\n",
      "Epoch[0], Batch[1263], Train loss: 0.05779215693473816\n",
      "Epoch[0], Val loss: 0.05289672315120697\n",
      "Epoch[0], Batch[1264], Train loss: 0.0580003447830677\n",
      "Epoch[0], Val loss: 0.05116903409361839\n",
      "Epoch[0], Batch[1265], Train loss: 0.05921529605984688\n",
      "Epoch[0], Val loss: 0.05545840784907341\n",
      "Epoch[0], Batch[1266], Train loss: 0.05670671910047531\n",
      "Epoch[0], Val loss: 0.05239277333021164\n",
      "Epoch[0], Batch[1267], Train loss: 0.05649672821164131\n",
      "Epoch[0], Val loss: 0.05592891573905945\n",
      "Epoch[0], Batch[1268], Train loss: 0.05814344435930252\n",
      "Epoch[0], Val loss: 0.051441412419080734\n",
      "Epoch[0], Batch[1269], Train loss: 0.05534244701266289\n",
      "Epoch[0], Val loss: 0.05554477497935295\n",
      "Epoch[0], Batch[1270], Train loss: 0.05323389545083046\n",
      "Epoch[0], Val loss: 0.04817669466137886\n",
      "Epoch[0], Batch[1271], Train loss: 0.05358373746275902\n",
      "Epoch[0], Val loss: 0.04964478686451912\n",
      "Epoch[0], Batch[1272], Train loss: 0.05733320489525795\n",
      "Epoch[0], Val loss: 0.051725298166275024\n",
      "Epoch[0], Batch[1273], Train loss: 0.05795178934931755\n",
      "Epoch[0], Val loss: 0.04974426329135895\n",
      "Epoch[0], Batch[1274], Train loss: 0.05622872710227966\n",
      "Epoch[0], Val loss: 0.04934290796518326\n",
      "Epoch[0], Batch[1275], Train loss: 0.05468488112092018\n",
      "Epoch[0], Val loss: 0.051938604563474655\n",
      "Epoch[0], Batch[1276], Train loss: 0.05511608347296715\n",
      "Epoch[0], Val loss: 0.05322055146098137\n",
      "Epoch[0], Batch[1277], Train loss: 0.05922508239746094\n",
      "Epoch[0], Val loss: 0.05016276240348816\n",
      "Epoch[0], Batch[1278], Train loss: 0.058905720710754395\n",
      "Epoch[0], Val loss: 0.053853098303079605\n",
      "Epoch[0], Batch[1279], Train loss: 0.05614684149622917\n",
      "Epoch[0], Val loss: 0.055588651448488235\n",
      "Epoch[0], Batch[1280], Train loss: 0.05684154853224754\n",
      "Epoch[0], Val loss: 0.052478037774562836\n",
      "Epoch[0], Batch[1281], Train loss: 0.055730342864990234\n",
      "Epoch[0], Val loss: 0.05223901942372322\n",
      "Epoch[0], Batch[1282], Train loss: 0.05383313447237015\n",
      "Epoch[0], Val loss: 0.05192815512418747\n",
      "Epoch[0], Batch[1283], Train loss: 0.053737007081508636\n",
      "Epoch[0], Val loss: 0.051964838057756424\n",
      "Epoch[0], Batch[1284], Train loss: 0.054756663739681244\n",
      "Epoch[0], Val loss: 0.05215822160243988\n",
      "Epoch[0], Batch[1285], Train loss: 0.05979664996266365\n",
      "Epoch[0], Val loss: 0.05305048078298569\n",
      "Epoch[0], Batch[1286], Train loss: 0.0559539757668972\n",
      "Epoch[0], Val loss: 0.05313749983906746\n",
      "Epoch[0], Batch[1287], Train loss: 0.05441174656152725\n",
      "Epoch[0], Val loss: 0.05375059321522713\n",
      "Epoch[0], Batch[1288], Train loss: 0.05529606714844704\n",
      "Epoch[0], Val loss: 0.051689229905605316\n",
      "Epoch[0], Batch[1289], Train loss: 0.05755548179149628\n",
      "Epoch[0], Val loss: 0.05121178925037384\n",
      "Epoch[0], Batch[1290], Train loss: 0.056022364646196365\n",
      "Epoch[0], Val loss: 0.05354560911655426\n",
      "Epoch[0], Batch[1291], Train loss: 0.05469760671257973\n",
      "Epoch[0], Val loss: 0.05069001764059067\n",
      "Epoch[0], Batch[1292], Train loss: 0.055710870772600174\n",
      "Epoch[0], Val loss: 0.05566353350877762\n",
      "Epoch[0], Batch[1293], Train loss: 0.05681800842285156\n",
      "Epoch[0], Val loss: 0.05037632957100868\n",
      "Epoch[0], Batch[1294], Train loss: 0.05694388970732689\n",
      "Epoch[0], Val loss: 0.056835684925317764\n",
      "Epoch[0], Batch[1295], Train loss: 0.05512822046875954\n",
      "Epoch[0], Val loss: 0.0539737232029438\n",
      "Epoch[0], Batch[1296], Train loss: 0.05627524480223656\n",
      "Epoch[0], Val loss: 0.05378883704543114\n",
      "Epoch[0], Batch[1297], Train loss: 0.06035684049129486\n",
      "Epoch[0], Val loss: 0.05220600590109825\n",
      "Epoch[0], Batch[1298], Train loss: 0.056218866258859634\n",
      "Epoch[0], Val loss: 0.05422520264983177\n",
      "Epoch[0], Batch[1299], Train loss: 0.05891956388950348\n",
      "Epoch[0], Val loss: 0.05114372819662094\n",
      "Epoch[0], Batch[1300], Train loss: 0.054821088910102844\n",
      "Epoch[0], Val loss: 0.04945102706551552\n",
      "Epoch[0], Batch[1301], Train loss: 0.05711454898118973\n",
      "Epoch[0], Val loss: 0.05253402888774872\n",
      "Epoch[0], Batch[1302], Train loss: 0.055649977177381516\n",
      "Epoch[0], Val loss: 0.05412052199244499\n",
      "Epoch[0], Batch[1303], Train loss: 0.05333219841122627\n",
      "Epoch[0], Val loss: 0.05098056048154831\n",
      "Epoch[0], Batch[1304], Train loss: 0.05723392963409424\n",
      "Epoch[0], Val loss: 0.05284467339515686\n",
      "Epoch[0], Batch[1305], Train loss: 0.05633878335356712\n",
      "Epoch[0], Val loss: 0.051596663892269135\n",
      "Epoch[0], Batch[1306], Train loss: 0.05465913563966751\n",
      "Epoch[0], Val loss: 0.047401756048202515\n",
      "Epoch[0], Batch[1307], Train loss: 0.05333967134356499\n",
      "Epoch[0], Val loss: 0.05505514517426491\n",
      "Epoch[0], Batch[1308], Train loss: 0.05982587859034538\n",
      "Epoch[0], Val loss: 0.050784800201654434\n",
      "Epoch[0], Batch[1309], Train loss: 0.05960053205490112\n",
      "Epoch[0], Val loss: 0.05400390550494194\n",
      "Epoch[0], Batch[1310], Train loss: 0.057418808341026306\n",
      "Epoch[0], Val loss: 0.049761176109313965\n",
      "Epoch[0], Batch[1311], Train loss: 0.05702754482626915\n",
      "Epoch[0], Val loss: 0.049095991998910904\n",
      "Epoch[0], Batch[1312], Train loss: 0.05448134243488312\n",
      "Epoch[0], Val loss: 0.05183684825897217\n",
      "Epoch[0], Batch[1313], Train loss: 0.06099286302924156\n",
      "Epoch[0], Val loss: 0.052253011614084244\n",
      "Epoch[0], Batch[1314], Train loss: 0.05509336665272713\n",
      "Epoch[0], Val loss: 0.05316849797964096\n",
      "Epoch[0], Batch[1315], Train loss: 0.05562874302268028\n",
      "Epoch[0], Val loss: 0.05217870697379112\n",
      "Epoch[0], Batch[1316], Train loss: 0.05546058341860771\n",
      "Epoch[0], Val loss: 0.05327826738357544\n",
      "Epoch[0], Batch[1317], Train loss: 0.05709761381149292\n",
      "Epoch[0], Val loss: 0.0533633753657341\n",
      "Epoch[0], Batch[1318], Train loss: 0.05948537960648537\n",
      "Epoch[0], Val loss: 0.05220361053943634\n",
      "Epoch[0], Batch[1319], Train loss: 0.052544910460710526\n",
      "Epoch[0], Val loss: 0.050920240581035614\n",
      "Epoch[0], Batch[1320], Train loss: 0.05329679697751999\n",
      "Epoch[0], Val loss: 0.04976622760295868\n",
      "Epoch[0], Batch[1321], Train loss: 0.0547114759683609\n",
      "Epoch[0], Val loss: 0.050015438348054886\n",
      "Epoch[0], Batch[1322], Train loss: 0.058532312512397766\n",
      "Epoch[0], Val loss: 0.053605735301971436\n",
      "Epoch[0], Batch[1323], Train loss: 0.06058584153652191\n",
      "Epoch[0], Val loss: 0.05487985908985138\n",
      "Epoch[0], Batch[1324], Train loss: 0.05886656418442726\n",
      "Epoch[0], Val loss: 0.05233553797006607\n",
      "Epoch[0], Batch[1325], Train loss: 0.0544242337346077\n",
      "Epoch[0], Val loss: 0.05119936540722847\n",
      "Epoch[0], Batch[1326], Train loss: 0.05593344196677208\n",
      "Epoch[0], Val loss: 0.05204123258590698\n",
      "Epoch[0], Batch[1327], Train loss: 0.054576974362134933\n",
      "Epoch[0], Val loss: 0.05032249540090561\n",
      "Epoch[0], Batch[1328], Train loss: 0.056668806821107864\n",
      "Epoch[0], Val loss: 0.051744669675827026\n",
      "Epoch[0], Batch[1329], Train loss: 0.05448248237371445\n",
      "Epoch[0], Val loss: 0.05280456319451332\n",
      "Epoch[0], Batch[1330], Train loss: 0.05245569720864296\n",
      "Epoch[0], Val loss: 0.04994383826851845\n",
      "Epoch[0], Batch[1331], Train loss: 0.05510806664824486\n",
      "Epoch[0], Val loss: 0.05295734852552414\n",
      "Epoch[0], Batch[1332], Train loss: 0.05976017937064171\n",
      "Epoch[0], Val loss: 0.04902403801679611\n",
      "Epoch[0], Batch[1333], Train loss: 0.056444503366947174\n",
      "Epoch[0], Val loss: 0.05179637297987938\n",
      "Epoch[0], Batch[1334], Train loss: 0.05761285498738289\n",
      "Epoch[0], Val loss: 0.0494367890059948\n",
      "Epoch[0], Batch[1335], Train loss: 0.05470237508416176\n",
      "Epoch[0], Val loss: 0.050571259111166\n",
      "Epoch[0], Batch[1336], Train loss: 0.054685112088918686\n",
      "Epoch[0], Val loss: 0.0540141835808754\n",
      "Epoch[0], Batch[1337], Train loss: 0.05560006946325302\n",
      "Epoch[0], Val loss: 0.05230597034096718\n",
      "Epoch[0], Batch[1338], Train loss: 0.055979933589696884\n",
      "Epoch[0], Val loss: 0.053799502551555634\n",
      "Epoch[0], Batch[1339], Train loss: 0.05423013120889664\n",
      "Epoch[0], Val loss: 0.0492764450609684\n",
      "Epoch[0], Batch[1340], Train loss: 0.05457324534654617\n",
      "Epoch[0], Val loss: 0.049669232219457626\n",
      "Epoch[0], Batch[1341], Train loss: 0.05441528931260109\n",
      "Epoch[0], Val loss: 0.05385632812976837\n",
      "Epoch[0], Batch[1342], Train loss: 0.0579485148191452\n",
      "Epoch[0], Val loss: 0.049947939813137054\n",
      "Epoch[0], Batch[1343], Train loss: 0.05331899970769882\n",
      "Epoch[0], Val loss: 0.05165518447756767\n",
      "Epoch[0], Batch[1344], Train loss: 0.05292149633169174\n",
      "Epoch[0], Val loss: 0.05234631896018982\n",
      "Epoch[0], Batch[1345], Train loss: 0.0554363839328289\n",
      "Epoch[0], Val loss: 0.05041126906871796\n",
      "Epoch[0], Batch[1346], Train loss: 0.05667109414935112\n",
      "Epoch[0], Val loss: 0.053693145513534546\n",
      "Epoch[0], Batch[1347], Train loss: 0.05960996821522713\n",
      "Epoch[0], Val loss: 0.050710566341876984\n",
      "Epoch[0], Batch[1348], Train loss: 0.05584707111120224\n",
      "Epoch[0], Val loss: 0.05333304777741432\n",
      "Epoch[0], Batch[1349], Train loss: 0.054074160754680634\n",
      "Epoch[0], Val loss: 0.05529201403260231\n",
      "Epoch[0], Batch[1350], Train loss: 0.05392887443304062\n",
      "Epoch[0], Val loss: 0.050128273665905\n",
      "Epoch[0], Batch[1351], Train loss: 0.05475519597530365\n",
      "Epoch[0], Val loss: 0.0481811948120594\n",
      "Epoch[0], Batch[1352], Train loss: 0.05451297014951706\n",
      "Epoch[0], Val loss: 0.05091719701886177\n",
      "Epoch[0], Batch[1353], Train loss: 0.054317764937877655\n",
      "Epoch[0], Val loss: 0.05001519247889519\n",
      "Epoch[0], Batch[1354], Train loss: 0.05930197611451149\n",
      "Epoch[0], Val loss: 0.04964210465550423\n",
      "Epoch[0], Batch[1355], Train loss: 0.05480257049202919\n",
      "Epoch[0], Val loss: 0.050077155232429504\n",
      "Epoch[0], Batch[1356], Train loss: 0.055525172501802444\n",
      "Epoch[0], Val loss: 0.052792321890592575\n",
      "Epoch[0], Batch[1357], Train loss: 0.05599932000041008\n",
      "Epoch[0], Val loss: 0.04988902807235718\n",
      "Epoch[0], Batch[1358], Train loss: 0.05732261389493942\n",
      "Epoch[0], Val loss: 0.048472605645656586\n",
      "Epoch[0], Batch[1359], Train loss: 0.05586030334234238\n",
      "Epoch[0], Val loss: 0.05286170914769173\n",
      "Epoch[0], Batch[1360], Train loss: 0.05480481684207916\n",
      "Epoch[0], Val loss: 0.05132318288087845\n",
      "Epoch[0], Batch[1361], Train loss: 0.059552811086177826\n",
      "Epoch[0], Val loss: 0.050059255212545395\n",
      "Epoch[0], Batch[1362], Train loss: 0.054763976484537125\n",
      "Epoch[0], Val loss: 0.051822442561388016\n",
      "Epoch[0], Batch[1363], Train loss: 0.05535299330949783\n",
      "Epoch[0], Val loss: 0.051342111080884933\n",
      "Epoch[0], Batch[1364], Train loss: 0.0576750673353672\n",
      "Epoch[0], Val loss: 0.04682165011763573\n",
      "Epoch[0], Batch[1365], Train loss: 0.0546647384762764\n",
      "Epoch[0], Val loss: 0.05264689400792122\n",
      "Epoch[0], Batch[1366], Train loss: 0.05756204202771187\n",
      "Epoch[0], Val loss: 0.052490536123514175\n",
      "Epoch[0], Batch[1367], Train loss: 0.05056276544928551\n",
      "Epoch[0], Val loss: 0.04896405711770058\n",
      "Epoch[0], Batch[1368], Train loss: 0.05537448823451996\n",
      "Epoch[0], Val loss: 0.052153319120407104\n",
      "Epoch[0], Batch[1369], Train loss: 0.05495051294565201\n",
      "Epoch[0], Val loss: 0.05237862467765808\n",
      "Epoch[0], Batch[1370], Train loss: 0.05515683814883232\n",
      "Epoch[0], Val loss: 0.04910537973046303\n",
      "Epoch[0], Batch[1371], Train loss: 0.05507789924740791\n",
      "Epoch[0], Val loss: 0.05110543593764305\n",
      "Epoch[0], Batch[1372], Train loss: 0.05656241253018379\n",
      "Epoch[0], Val loss: 0.05118904262781143\n",
      "Epoch[0], Batch[1373], Train loss: 0.054464664310216904\n",
      "Epoch[0], Val loss: 0.05451735854148865\n",
      "Epoch[0], Batch[1374], Train loss: 0.05464338883757591\n",
      "Epoch[0], Val loss: 0.05085320025682449\n",
      "Epoch[0], Batch[1375], Train loss: 0.05635262653231621\n",
      "Epoch[0], Val loss: 0.05015895515680313\n",
      "Epoch[0], Batch[1376], Train loss: 0.055940911173820496\n",
      "Epoch[0], Val loss: 0.04835210368037224\n",
      "Epoch[0], Batch[1377], Train loss: 0.05638667568564415\n",
      "Epoch[0], Val loss: 0.05536266788840294\n",
      "Epoch[0], Batch[1378], Train loss: 0.0539703331887722\n",
      "Epoch[0], Val loss: 0.052252382040023804\n",
      "Epoch[0], Batch[1379], Train loss: 0.05800075829029083\n",
      "Epoch[0], Val loss: 0.053743213415145874\n",
      "Epoch[0], Batch[1380], Train loss: 0.05392534285783768\n",
      "Epoch[0], Val loss: 0.04814324155449867\n",
      "Epoch[0], Batch[1381], Train loss: 0.05389432609081268\n",
      "Epoch[0], Val loss: 0.05040118098258972\n",
      "Epoch[0], Batch[1382], Train loss: 0.05486355349421501\n",
      "Epoch[0], Val loss: 0.05112435668706894\n",
      "Epoch[0], Batch[1383], Train loss: 0.05161265283823013\n",
      "Epoch[0], Val loss: 0.05048900842666626\n",
      "Epoch[0], Batch[1384], Train loss: 0.05639854073524475\n",
      "Epoch[0], Val loss: 0.0536399781703949\n",
      "Epoch[0], Batch[1385], Train loss: 0.05492990091443062\n",
      "Epoch[0], Val loss: 0.05274203419685364\n",
      "Epoch[0], Batch[1386], Train loss: 0.055102258920669556\n",
      "Epoch[0], Val loss: 0.05211172252893448\n",
      "Epoch[0], Batch[1387], Train loss: 0.055073332041502\n",
      "Epoch[0], Val loss: 0.04853512719273567\n",
      "Epoch[0], Batch[1388], Train loss: 0.05538589134812355\n",
      "Epoch[0], Val loss: 0.04813367500901222\n",
      "Epoch[0], Batch[1389], Train loss: 0.05713324993848801\n",
      "Epoch[0], Val loss: 0.04919709637761116\n",
      "Epoch[0], Batch[1390], Train loss: 0.0567551925778389\n",
      "Epoch[0], Val loss: 0.053656741976737976\n",
      "Epoch[0], Batch[1391], Train loss: 0.05240821838378906\n",
      "Epoch[0], Val loss: 0.04876124486327171\n",
      "Epoch[0], Batch[1392], Train loss: 0.05513826012611389\n",
      "Epoch[0], Val loss: 0.0548127219080925\n",
      "Epoch[0], Batch[1393], Train loss: 0.052022967487573624\n",
      "Epoch[0], Val loss: 0.050753153860569\n",
      "Epoch[0], Batch[1394], Train loss: 0.05697737634181976\n",
      "Epoch[0], Val loss: 0.04941375553607941\n",
      "Epoch[0], Batch[1395], Train loss: 0.0525968037545681\n",
      "Epoch[0], Val loss: 0.05013047531247139\n",
      "Epoch[0], Batch[1396], Train loss: 0.05209360271692276\n",
      "Epoch[0], Val loss: 0.050764214247465134\n",
      "Epoch[0], Batch[1397], Train loss: 0.05430744215846062\n",
      "Epoch[0], Val loss: 0.055366918444633484\n",
      "Epoch[0], Batch[1398], Train loss: 0.055073875933885574\n",
      "Epoch[0], Val loss: 0.05221141129732132\n",
      "Epoch[0], Batch[1399], Train loss: 0.055936940014362335\n",
      "Epoch[0], Val loss: 0.04767932742834091\n",
      "Epoch[0], Batch[1400], Train loss: 0.05998276174068451\n",
      "Epoch[0], Val loss: 0.050253670662641525\n",
      "Epoch[0], Batch[1401], Train loss: 0.05723809823393822\n",
      "Epoch[0], Val loss: 0.05101413652300835\n",
      "Epoch[0], Batch[1402], Train loss: 0.054441120475530624\n",
      "Epoch[0], Val loss: 0.049603309482336044\n",
      "Epoch[0], Batch[1403], Train loss: 0.05188165605068207\n",
      "Epoch[0], Val loss: 0.049696505069732666\n",
      "Epoch[0], Batch[1404], Train loss: 0.05228272080421448\n",
      "Epoch[0], Val loss: 0.04781555011868477\n",
      "Epoch[0], Batch[1405], Train loss: 0.0547964945435524\n",
      "Epoch[0], Val loss: 0.052909914404153824\n",
      "Epoch[0], Batch[1406], Train loss: 0.055132899433374405\n",
      "Epoch[0], Val loss: 0.049617230892181396\n",
      "Epoch[0], Batch[1407], Train loss: 0.05101775750517845\n",
      "Epoch[0], Val loss: 0.05342615395784378\n",
      "Epoch[0], Batch[1408], Train loss: 0.053914666175842285\n",
      "Epoch[0], Val loss: 0.04937804117798805\n",
      "Epoch[0], Batch[1409], Train loss: 0.05816424638032913\n",
      "Epoch[0], Val loss: 0.05083799362182617\n",
      "Epoch[0], Batch[1410], Train loss: 0.051655363291502\n",
      "Epoch[0], Val loss: 0.04967214912176132\n",
      "Epoch[0], Batch[1411], Train loss: 0.0548320934176445\n",
      "Epoch[0], Val loss: 0.049683671444654465\n",
      "Epoch[0], Batch[1412], Train loss: 0.0553179495036602\n",
      "Epoch[0], Val loss: 0.05093539506196976\n",
      "Epoch[0], Batch[1413], Train loss: 0.05337314307689667\n",
      "Epoch[0], Val loss: 0.05207579582929611\n",
      "Epoch[0], Batch[1414], Train loss: 0.054095905274152756\n",
      "Epoch[0], Val loss: 0.049854718148708344\n",
      "Epoch[0], Batch[1415], Train loss: 0.05794411152601242\n",
      "Epoch[0], Val loss: 0.04752305895090103\n",
      "Epoch[0], Batch[1416], Train loss: 0.055377911776304245\n",
      "Epoch[0], Val loss: 0.05469300225377083\n",
      "Epoch[0], Batch[1417], Train loss: 0.05377459526062012\n",
      "Epoch[0], Val loss: 0.049085833132267\n",
      "Epoch[0], Batch[1418], Train loss: 0.051774583756923676\n",
      "Epoch[0], Val loss: 0.04562094807624817\n",
      "Epoch[0], Batch[1419], Train loss: 0.05619555711746216\n",
      "Epoch[0], Val loss: 0.0499979592859745\n",
      "Epoch[0], Batch[1420], Train loss: 0.05902663990855217\n",
      "Epoch[0], Val loss: 0.05266844481229782\n",
      "Epoch[0], Batch[1421], Train loss: 0.05447961390018463\n",
      "Epoch[0], Val loss: 0.05292711406946182\n",
      "Epoch[0], Batch[1422], Train loss: 0.055041197687387466\n",
      "Epoch[0], Val loss: 0.04847249761223793\n",
      "Epoch[0], Batch[1423], Train loss: 0.05680302530527115\n",
      "Epoch[0], Val loss: 0.051836930215358734\n",
      "Epoch[0], Batch[1424], Train loss: 0.05440092459321022\n",
      "Epoch[0], Val loss: 0.05104038491845131\n",
      "Epoch[0], Batch[1425], Train loss: 0.052132803946733475\n",
      "Epoch[0], Val loss: 0.055742137134075165\n",
      "Epoch[0], Batch[1426], Train loss: 0.05248747766017914\n",
      "Epoch[0], Val loss: 0.05037367716431618\n",
      "Epoch[0], Batch[1427], Train loss: 0.05448095500469208\n",
      "Epoch[0], Val loss: 0.05062774568796158\n",
      "Epoch[0], Batch[1428], Train loss: 0.05654080584645271\n",
      "Epoch[0], Val loss: 0.04735482484102249\n",
      "Epoch[0], Batch[1429], Train loss: 0.05492716282606125\n",
      "Epoch[0], Val loss: 0.05110049620270729\n",
      "Epoch[0], Batch[1430], Train loss: 0.05617672950029373\n",
      "Epoch[0], Val loss: 0.05114632844924927\n",
      "Epoch[0], Batch[1431], Train loss: 0.051421958953142166\n",
      "Epoch[0], Val loss: 0.05163693428039551\n",
      "Epoch[0], Batch[1432], Train loss: 0.055622272193431854\n",
      "Epoch[0], Val loss: 0.049945034086704254\n",
      "Epoch[0], Batch[1433], Train loss: 0.05466843023896217\n",
      "Epoch[0], Val loss: 0.048775237053632736\n",
      "Epoch[0], Batch[1434], Train loss: 0.053550560027360916\n",
      "Epoch[0], Val loss: 0.04930298775434494\n",
      "Epoch[0], Batch[1435], Train loss: 0.056302618235349655\n",
      "Epoch[0], Val loss: 0.050636615604162216\n",
      "Epoch[0], Batch[1436], Train loss: 0.05296529456973076\n",
      "Epoch[0], Val loss: 0.052123118191957474\n",
      "Epoch[0], Batch[1437], Train loss: 0.05338830128312111\n",
      "Epoch[0], Val loss: 0.050809141248464584\n",
      "Epoch[0], Batch[1438], Train loss: 0.05979570001363754\n",
      "Epoch[0], Val loss: 0.049982957541942596\n",
      "Epoch[0], Batch[1439], Train loss: 0.04823201522231102\n",
      "Epoch[0], Val loss: 0.05308913066983223\n",
      "Epoch[0], Batch[1440], Train loss: 0.05354223772883415\n",
      "Epoch[0], Val loss: 0.048100847750902176\n",
      "Epoch[0], Batch[1441], Train loss: 0.05684138089418411\n",
      "Epoch[0], Val loss: 0.049681954085826874\n",
      "Epoch[0], Batch[1442], Train loss: 0.05201954022049904\n",
      "Epoch[0], Val loss: 0.05169900879263878\n",
      "Epoch[0], Batch[1443], Train loss: 0.05446754768490791\n",
      "Epoch[0], Val loss: 0.05202135071158409\n",
      "Epoch[0], Batch[1444], Train loss: 0.05333366617560387\n",
      "Epoch[0], Val loss: 0.04931183159351349\n",
      "Epoch[0], Batch[1445], Train loss: 0.05353744700551033\n",
      "Epoch[0], Val loss: 0.0528361015021801\n",
      "Epoch[0], Batch[1446], Train loss: 0.05208306014537811\n",
      "Epoch[0], Val loss: 0.05108921602368355\n",
      "Epoch[0], Batch[1447], Train loss: 0.05245959386229515\n",
      "Epoch[0], Val loss: 0.04913196340203285\n",
      "Epoch[0], Batch[1448], Train loss: 0.054164692759513855\n",
      "Epoch[0], Val loss: 0.049974847584962845\n",
      "Epoch[0], Batch[1449], Train loss: 0.05308482050895691\n",
      "Epoch[0], Val loss: 0.05143404379487038\n",
      "Epoch[0], Batch[1450], Train loss: 0.05417914316058159\n",
      "Epoch[0], Val loss: 0.049028430134058\n",
      "Epoch[0], Batch[1451], Train loss: 0.053683072328567505\n",
      "Epoch[0], Val loss: 0.053566429764032364\n",
      "Epoch[0], Batch[1452], Train loss: 0.05387388914823532\n",
      "Epoch[0], Val loss: 0.05261341854929924\n",
      "Epoch[0], Batch[1453], Train loss: 0.05162736028432846\n",
      "Epoch[0], Val loss: 0.05132672190666199\n",
      "Epoch[0], Batch[1454], Train loss: 0.050099022686481476\n",
      "Epoch[0], Val loss: 0.051039036363363266\n",
      "Epoch[0], Batch[1455], Train loss: 0.05467531830072403\n",
      "Epoch[0], Val loss: 0.0500403456389904\n",
      "Epoch[0], Batch[1456], Train loss: 0.05307873710989952\n",
      "Epoch[0], Val loss: 0.052011143416166306\n",
      "Epoch[0], Batch[1457], Train loss: 0.05258815363049507\n",
      "Epoch[0], Val loss: 0.04998393729329109\n",
      "Epoch[0], Batch[1458], Train loss: 0.05198560655117035\n",
      "Epoch[0], Val loss: 0.05207652226090431\n",
      "Epoch[0], Batch[1459], Train loss: 0.05401003360748291\n",
      "Epoch[0], Val loss: 0.04738693684339523\n",
      "Epoch[0], Batch[1460], Train loss: 0.056213103234767914\n",
      "Epoch[0], Val loss: 0.04748784750699997\n",
      "Epoch[0], Batch[1461], Train loss: 0.0523589588701725\n",
      "Epoch[0], Val loss: 0.04930625855922699\n",
      "Epoch[0], Batch[1462], Train loss: 0.055145908147096634\n",
      "Epoch[0], Val loss: 0.04987914860248566\n",
      "Epoch[0], Batch[1463], Train loss: 0.053545475006103516\n",
      "Epoch[0], Val loss: 0.04856004938483238\n",
      "Epoch[0], Batch[1464], Train loss: 0.05056241899728775\n",
      "Epoch[0], Val loss: 0.04751133918762207\n",
      "Epoch[0], Batch[1465], Train loss: 0.049642689526081085\n",
      "Epoch[0], Val loss: 0.05160072073340416\n",
      "Epoch[0], Batch[1466], Train loss: 0.05307169631123543\n",
      "Epoch[0], Val loss: 0.04923737794160843\n",
      "Epoch[0], Batch[1467], Train loss: 0.04960225522518158\n",
      "Epoch[0], Val loss: 0.048892635852098465\n",
      "Epoch[0], Batch[1468], Train loss: 0.05539761483669281\n",
      "Epoch[0], Val loss: 0.048741415143013\n",
      "Epoch[0], Batch[1469], Train loss: 0.05525132268667221\n",
      "Epoch[0], Val loss: 0.05073186382651329\n",
      "Epoch[0], Batch[1470], Train loss: 0.04735402762889862\n",
      "Epoch[0], Val loss: 0.04944365844130516\n",
      "Epoch[0], Batch[1471], Train loss: 0.05536467209458351\n",
      "Epoch[0], Val loss: 0.052299488335847855\n",
      "Epoch[0], Batch[1472], Train loss: 0.051936011761426926\n",
      "Epoch[0], Val loss: 0.049256324768066406\n",
      "Epoch[0], Batch[1473], Train loss: 0.050761207938194275\n",
      "Epoch[0], Val loss: 0.05193180963397026\n",
      "Epoch[0], Batch[1474], Train loss: 0.05300573632121086\n",
      "Epoch[0], Val loss: 0.04924294725060463\n",
      "Epoch[0], Batch[1475], Train loss: 0.054239626973867416\n",
      "Epoch[0], Val loss: 0.0498502291738987\n",
      "Epoch[0], Batch[1476], Train loss: 0.05136584863066673\n",
      "Epoch[0], Val loss: 0.05056527629494667\n",
      "Epoch[0], Batch[1477], Train loss: 0.05337719991803169\n",
      "Epoch[0], Val loss: 0.04718632251024246\n",
      "Epoch[0], Batch[1478], Train loss: 0.054369714111089706\n",
      "Epoch[0], Val loss: 0.04987546056509018\n",
      "Epoch[0], Batch[1479], Train loss: 0.050014372915029526\n",
      "Epoch[0], Val loss: 0.05061040818691254\n",
      "Epoch[0], Batch[1480], Train loss: 0.0525093637406826\n",
      "Epoch[0], Val loss: 0.051447123289108276\n",
      "Epoch[0], Batch[1481], Train loss: 0.052328359335660934\n",
      "Epoch[0], Val loss: 0.05028513818979263\n",
      "Epoch[0], Batch[1482], Train loss: 0.05355024337768555\n",
      "Epoch[0], Val loss: 0.049407269805669785\n",
      "Epoch[0], Batch[1483], Train loss: 0.05135132372379303\n",
      "Epoch[0], Val loss: 0.05307658016681671\n",
      "Epoch[0], Batch[1484], Train loss: 0.05355655401945114\n",
      "Epoch[0], Val loss: 0.04974750056862831\n",
      "Epoch[0], Batch[1485], Train loss: 0.05506453663110733\n",
      "Epoch[0], Val loss: 0.05141882970929146\n",
      "Epoch[0], Batch[1486], Train loss: 0.05224670469760895\n",
      "Epoch[0], Val loss: 0.05113251134753227\n",
      "Epoch[0], Batch[1487], Train loss: 0.054433438926935196\n",
      "Epoch[0], Val loss: 0.04933437705039978\n",
      "Epoch[0], Batch[1488], Train loss: 0.054042503237724304\n",
      "Epoch[0], Val loss: 0.05354619771242142\n",
      "Epoch[0], Batch[1489], Train loss: 0.053882353007793427\n",
      "Epoch[0], Val loss: 0.04906372353434563\n",
      "Epoch[0], Batch[1490], Train loss: 0.05421172454953194\n",
      "Epoch[0], Val loss: 0.04975252225995064\n",
      "Epoch[0], Batch[1491], Train loss: 0.050884660333395004\n",
      "Epoch[0], Val loss: 0.04915958642959595\n",
      "Epoch[0], Batch[1492], Train loss: 0.0535714253783226\n",
      "Epoch[0], Val loss: 0.053465791046619415\n",
      "Epoch[0], Batch[1493], Train loss: 0.04953629523515701\n",
      "Epoch[0], Val loss: 0.0458996519446373\n",
      "Epoch[0], Batch[1494], Train loss: 0.05662362277507782\n",
      "Epoch[0], Val loss: 0.048484157770872116\n",
      "Epoch[0], Batch[1495], Train loss: 0.05514179915189743\n",
      "Epoch[0], Val loss: 0.0500711165368557\n",
      "Epoch[0], Batch[1496], Train loss: 0.05297491326928139\n",
      "Epoch[0], Val loss: 0.0508514940738678\n",
      "Epoch[0], Batch[1497], Train loss: 0.05376685410737991\n",
      "Epoch[0], Val loss: 0.05248391628265381\n",
      "Epoch[0], Batch[1498], Train loss: 0.05230793356895447\n",
      "Epoch[0], Val loss: 0.05049422010779381\n",
      "Epoch[0], Batch[1499], Train loss: 0.05473051965236664\n",
      "Epoch[0], Val loss: 0.05096562206745148\n",
      "Epoch[0], Batch[1500], Train loss: 0.053463950753211975\n",
      "Epoch[0], Val loss: 0.04825589060783386\n",
      "Epoch[0], Batch[1501], Train loss: 0.051445044577121735\n",
      "Epoch[0], Val loss: 0.04808457940816879\n",
      "Epoch[0], Batch[1502], Train loss: 0.05153314769268036\n",
      "Epoch[0], Val loss: 0.045740969479084015\n",
      "Epoch[0], Batch[1503], Train loss: 0.05424107238650322\n",
      "Epoch[0], Val loss: 0.049703121185302734\n",
      "Epoch[0], Batch[1504], Train loss: 0.05300779640674591\n",
      "Epoch[0], Val loss: 0.04645301029086113\n",
      "Epoch[0], Batch[1505], Train loss: 0.05309925973415375\n",
      "Epoch[0], Val loss: 0.05056598037481308\n",
      "Epoch[0], Batch[1506], Train loss: 0.053566109389066696\n",
      "Epoch[0], Val loss: 0.048154424875974655\n",
      "Epoch[0], Batch[1507], Train loss: 0.051165156066417694\n",
      "Epoch[0], Val loss: 0.04921264201402664\n",
      "Epoch[0], Batch[1508], Train loss: 0.04862317070364952\n",
      "Epoch[0], Val loss: 0.054085519164800644\n",
      "Epoch[0], Batch[1509], Train loss: 0.05221641808748245\n",
      "Epoch[0], Val loss: 0.04881682246923447\n",
      "Epoch[0], Batch[1510], Train loss: 0.05204376205801964\n",
      "Epoch[0], Val loss: 0.0475466325879097\n",
      "Epoch[0], Batch[1511], Train loss: 0.05233319103717804\n",
      "Epoch[0], Val loss: 0.04736900329589844\n",
      "Epoch[0], Batch[1512], Train loss: 0.05279116705060005\n",
      "Epoch[0], Val loss: 0.04822726175189018\n",
      "Epoch[0], Batch[1513], Train loss: 0.052770745009183884\n",
      "Epoch[0], Val loss: 0.045195985585451126\n",
      "Epoch[0], Batch[1514], Train loss: 0.053165070712566376\n",
      "Epoch[0], Val loss: 0.04959876462817192\n",
      "Epoch[0], Batch[1515], Train loss: 0.05490371212363243\n",
      "Epoch[0], Val loss: 0.051543232053518295\n",
      "Epoch[0], Batch[1516], Train loss: 0.04910259321331978\n",
      "Epoch[0], Val loss: 0.04982432723045349\n",
      "Epoch[0], Batch[1517], Train loss: 0.055077724158763885\n",
      "Epoch[0], Val loss: 0.04836530610918999\n",
      "Epoch[0], Batch[1518], Train loss: 0.049351006746292114\n",
      "Epoch[0], Val loss: 0.04607044905424118\n",
      "Epoch[0], Batch[1519], Train loss: 0.05264601483941078\n",
      "Epoch[0], Val loss: 0.05174948275089264\n",
      "Epoch[0], Batch[1520], Train loss: 0.05144796893000603\n",
      "Epoch[0], Val loss: 0.052384153008461\n",
      "Epoch[0], Batch[1521], Train loss: 0.05084714666008949\n",
      "Epoch[0], Val loss: 0.05487259849905968\n",
      "Epoch[0], Batch[1522], Train loss: 0.053439799696207047\n",
      "Epoch[0], Val loss: 0.047721121460199356\n",
      "Epoch[0], Batch[1523], Train loss: 0.05600675195455551\n",
      "Epoch[0], Val loss: 0.05098767206072807\n",
      "Epoch[0], Batch[1524], Train loss: 0.05402719974517822\n",
      "Epoch[0], Val loss: 0.046653252094984055\n",
      "Epoch[0], Batch[1525], Train loss: 0.05039607733488083\n",
      "Epoch[0], Val loss: 0.04764578491449356\n",
      "Epoch[0], Batch[1526], Train loss: 0.048361070454120636\n",
      "Epoch[0], Val loss: 0.04935939237475395\n",
      "Epoch[0], Batch[1527], Train loss: 0.053926706314086914\n",
      "Epoch[0], Val loss: 0.0496763251721859\n",
      "Epoch[0], Batch[1528], Train loss: 0.05104300379753113\n",
      "Epoch[0], Val loss: 0.04697105288505554\n",
      "Epoch[0], Batch[1529], Train loss: 0.05554433912038803\n",
      "Epoch[0], Val loss: 0.05316157639026642\n",
      "Epoch[0], Batch[1530], Train loss: 0.054500579833984375\n",
      "Epoch[0], Val loss: 0.0505533292889595\n",
      "Epoch[0], Batch[1531], Train loss: 0.05457941070199013\n",
      "Epoch[0], Val loss: 0.050580743700265884\n",
      "Epoch[0], Batch[1532], Train loss: 0.054199833422899246\n",
      "Epoch[0], Val loss: 0.050084833055734634\n",
      "Epoch[0], Batch[1533], Train loss: 0.055124443024396896\n",
      "Epoch[0], Val loss: 0.04668525978922844\n",
      "Epoch[0], Batch[1534], Train loss: 0.05611839145421982\n",
      "Epoch[0], Val loss: 0.04851589724421501\n",
      "Epoch[0], Batch[1535], Train loss: 0.052156612277030945\n",
      "Epoch[0], Val loss: 0.0466853603720665\n",
      "Epoch[0], Batch[1536], Train loss: 0.05487087741494179\n",
      "Epoch[0], Val loss: 0.045785192400217056\n",
      "Epoch[0], Batch[1537], Train loss: 0.05557889863848686\n",
      "Epoch[0], Val loss: 0.04907675459980965\n",
      "Epoch[0], Batch[1538], Train loss: 0.054904960095882416\n",
      "Epoch[0], Val loss: 0.0499129593372345\n",
      "Epoch[0], Batch[1539], Train loss: 0.04937507212162018\n",
      "Epoch[0], Val loss: 0.050291508436203\n",
      "Epoch[0], Batch[1540], Train loss: 0.05127022787928581\n",
      "Epoch[0], Val loss: 0.04692875221371651\n",
      "Epoch[0], Batch[1541], Train loss: 0.048993680626153946\n",
      "Epoch[0], Val loss: 0.04971279203891754\n",
      "Epoch[0], Batch[1542], Train loss: 0.05067959427833557\n",
      "Epoch[0], Val loss: 0.04938117042183876\n",
      "Epoch[0], Batch[1543], Train loss: 0.05113440006971359\n",
      "Epoch[0], Val loss: 0.04873218387365341\n",
      "Epoch[0], Batch[1544], Train loss: 0.050602931529283524\n",
      "Epoch[0], Val loss: 0.049179136753082275\n",
      "Epoch[0], Batch[1545], Train loss: 0.05239133909344673\n",
      "Epoch[0], Val loss: 0.04787933826446533\n",
      "Epoch[0], Batch[1546], Train loss: 0.053320080041885376\n",
      "Epoch[0], Val loss: 0.051590148359537125\n",
      "Epoch[0], Batch[1547], Train loss: 0.05188269913196564\n",
      "Epoch[0], Val loss: 0.04829667508602142\n",
      "Epoch[0], Batch[1548], Train loss: 0.05222414433956146\n",
      "Epoch[0], Val loss: 0.0478285513818264\n",
      "Epoch[0], Batch[1549], Train loss: 0.0493931770324707\n",
      "Epoch[0], Val loss: 0.05009358748793602\n",
      "Epoch[0], Batch[1550], Train loss: 0.0537683479487896\n",
      "Epoch[0], Val loss: 0.049921002238988876\n",
      "Epoch[0], Batch[1551], Train loss: 0.05153905227780342\n",
      "Epoch[0], Val loss: 0.05278860032558441\n",
      "Epoch[0], Batch[1552], Train loss: 0.05061637982726097\n",
      "Epoch[0], Val loss: 0.051155928522348404\n",
      "Epoch[0], Batch[1553], Train loss: 0.05152727663516998\n",
      "Epoch[0], Val loss: 0.04863150417804718\n",
      "Epoch[0], Batch[1554], Train loss: 0.054304931312799454\n",
      "Epoch[0], Val loss: 0.05168331041932106\n",
      "Epoch[0], Batch[1555], Train loss: 0.0542026162147522\n",
      "Epoch[0], Val loss: 0.04917998984456062\n",
      "Epoch[0], Batch[1556], Train loss: 0.05273400992155075\n",
      "Epoch[0], Val loss: 0.04550330713391304\n",
      "Epoch[0], Batch[1557], Train loss: 0.05207949876785278\n",
      "Epoch[0], Val loss: 0.048106931149959564\n",
      "Epoch[0], Batch[1558], Train loss: 0.051649440079927444\n",
      "Epoch[0], Val loss: 0.04956233128905296\n",
      "Epoch[0], Batch[1559], Train loss: 0.05366427078843117\n",
      "Epoch[0], Val loss: 0.051296256482601166\n",
      "Epoch[0], Batch[1560], Train loss: 0.057059384882450104\n",
      "Epoch[0], Val loss: 0.04740474000573158\n",
      "Epoch[0], Batch[1561], Train loss: 0.05188727006316185\n",
      "Epoch[0], Val loss: 0.04651327058672905\n",
      "Epoch[0], Batch[1562], Train loss: 0.055118314921855927\n",
      "Epoch[0], Val loss: 0.05093017965555191\n",
      "Epoch[0], Batch[1563], Train loss: 0.048303086310625076\n",
      "Epoch[0], Val loss: 0.05188978463411331\n",
      "Epoch[0], Batch[1564], Train loss: 0.053526949137449265\n",
      "Epoch[0], Val loss: 0.0508095808327198\n",
      "Epoch[0], Batch[1565], Train loss: 0.05570157617330551\n",
      "Epoch[0], Val loss: 0.04821392148733139\n",
      "Epoch[0], Batch[1566], Train loss: 0.05470343306660652\n",
      "Epoch[0], Val loss: 0.051087718456983566\n",
      "Epoch[0], Batch[1567], Train loss: 0.051120489835739136\n",
      "Epoch[0], Val loss: 0.04978280887007713\n",
      "Epoch[0], Batch[1568], Train loss: 0.05409429222345352\n",
      "Epoch[0], Val loss: 0.05043657496571541\n",
      "Epoch[0], Batch[1569], Train loss: 0.052915822714567184\n",
      "Epoch[0], Val loss: 0.052972979843616486\n",
      "Epoch[0], Batch[1570], Train loss: 0.05409005284309387\n",
      "Epoch[0], Val loss: 0.050510432571172714\n",
      "Epoch[0], Batch[1571], Train loss: 0.05191556736826897\n",
      "Epoch[0], Val loss: 0.04770813137292862\n",
      "Epoch[0], Batch[1572], Train loss: 0.05330965295433998\n",
      "Epoch[0], Val loss: 0.05048009008169174\n",
      "Epoch[0], Batch[1573], Train loss: 0.05112661048769951\n",
      "Epoch[0], Val loss: 0.048035215586423874\n",
      "Epoch[0], Batch[1574], Train loss: 0.055265456438064575\n",
      "Epoch[0], Val loss: 0.04776138439774513\n",
      "Epoch[0], Batch[1575], Train loss: 0.05027955770492554\n",
      "Epoch[0], Val loss: 0.04824317619204521\n",
      "Epoch[0], Batch[1576], Train loss: 0.05137574300169945\n",
      "Epoch[0], Val loss: 0.04808199405670166\n",
      "Epoch[0], Batch[1577], Train loss: 0.0526038259267807\n",
      "Epoch[0], Val loss: 0.04936317354440689\n",
      "Epoch[0], Batch[1578], Train loss: 0.05262460187077522\n",
      "Epoch[0], Val loss: 0.04904811084270477\n",
      "Epoch[0], Batch[1579], Train loss: 0.05248966068029404\n",
      "Epoch[0], Val loss: 0.04757184907793999\n",
      "Epoch[0], Batch[1580], Train loss: 0.05226285010576248\n",
      "Epoch[0], Val loss: 0.05087541043758392\n",
      "Epoch[0], Batch[1581], Train loss: 0.05169155076146126\n",
      "Epoch[0], Val loss: 0.049597665667533875\n",
      "Epoch[0], Batch[1582], Train loss: 0.05208141729235649\n",
      "Epoch[0], Val loss: 0.0478249192237854\n",
      "Epoch[0], Batch[1583], Train loss: 0.0497671402990818\n",
      "Epoch[0], Val loss: 0.04793186113238335\n",
      "Epoch[0], Batch[1584], Train loss: 0.05263511836528778\n",
      "Epoch[0], Val loss: 0.04951411113142967\n",
      "Epoch[0], Batch[1585], Train loss: 0.053254906088113785\n",
      "Epoch[0], Val loss: 0.0533161386847496\n",
      "Epoch[0], Batch[1586], Train loss: 0.049437060952186584\n",
      "Epoch[0], Val loss: 0.05016958713531494\n",
      "Epoch[0], Batch[1587], Train loss: 0.05217258632183075\n",
      "Epoch[0], Val loss: 0.053096797317266464\n",
      "Epoch[0], Batch[1588], Train loss: 0.049419332295656204\n",
      "Epoch[0], Val loss: 0.04730077460408211\n",
      "Epoch[0], Batch[1589], Train loss: 0.04933794587850571\n",
      "Epoch[0], Val loss: 0.046249762177467346\n",
      "Epoch[0], Batch[1590], Train loss: 0.05008326843380928\n",
      "Epoch[0], Val loss: 0.046742506325244904\n",
      "Epoch[0], Batch[1591], Train loss: 0.05189688131213188\n",
      "Epoch[0], Val loss: 0.04627840593457222\n",
      "Epoch[0], Batch[1592], Train loss: 0.05358004942536354\n",
      "Epoch[0], Val loss: 0.047256484627723694\n",
      "Epoch[0], Batch[1593], Train loss: 0.05107349529862404\n",
      "Epoch[0], Val loss: 0.047267548739910126\n",
      "Epoch[0], Batch[1594], Train loss: 0.053650062531232834\n",
      "Epoch[0], Val loss: 0.045447565615177155\n",
      "Epoch[0], Batch[1595], Train loss: 0.05085362493991852\n",
      "Epoch[0], Val loss: 0.0473051480948925\n",
      "Epoch[0], Batch[1596], Train loss: 0.05231766775250435\n",
      "Epoch[0], Val loss: 0.05118566006422043\n",
      "Epoch[0], Batch[1597], Train loss: 0.05506385862827301\n",
      "Epoch[0], Val loss: 0.048217132687568665\n",
      "Epoch[0], Batch[1598], Train loss: 0.05496304854750633\n",
      "Epoch[0], Val loss: 0.0481068454682827\n",
      "Epoch[0], Batch[1599], Train loss: 0.05415575951337814\n",
      "Epoch[0], Val loss: 0.052063774317502975\n",
      "Epoch[0], Batch[1600], Train loss: 0.05029887706041336\n",
      "Epoch[0], Val loss: 0.05075070261955261\n",
      "Epoch[0], Batch[1601], Train loss: 0.05259956046938896\n",
      "Epoch[0], Val loss: 0.048568494617938995\n",
      "Epoch[0], Batch[1602], Train loss: 0.04984341561794281\n",
      "Epoch[0], Val loss: 0.0472818948328495\n",
      "Epoch[0], Batch[1603], Train loss: 0.05161686986684799\n",
      "Epoch[0], Val loss: 0.047623973339796066\n",
      "Epoch[0], Batch[1604], Train loss: 0.05180632695555687\n",
      "Epoch[0], Val loss: 0.04694566875696182\n",
      "Epoch[0], Batch[1605], Train loss: 0.052604250609874725\n",
      "Epoch[0], Val loss: 0.04811866581439972\n",
      "Epoch[0], Batch[1606], Train loss: 0.05577073618769646\n",
      "Epoch[0], Val loss: 0.047501467168331146\n",
      "Epoch[0], Batch[1607], Train loss: 0.05221804603934288\n",
      "Epoch[0], Val loss: 0.04955168813467026\n",
      "Epoch[0], Batch[1608], Train loss: 0.053740281611680984\n",
      "Epoch[0], Val loss: 0.04765872657299042\n",
      "Epoch[0], Batch[1609], Train loss: 0.05016129091382027\n",
      "Epoch[0], Val loss: 0.04883985593914986\n",
      "Epoch[0], Batch[1610], Train loss: 0.05443963408470154\n",
      "Epoch[0], Val loss: 0.04752863198518753\n",
      "Epoch[0], Batch[1611], Train loss: 0.053517479449510574\n",
      "Epoch[0], Val loss: 0.048394136130809784\n",
      "Epoch[0], Batch[1612], Train loss: 0.05269540101289749\n",
      "Epoch[0], Val loss: 0.04854162782430649\n",
      "Epoch[0], Batch[1613], Train loss: 0.05484817922115326\n",
      "Epoch[0], Val loss: 0.05059579759836197\n",
      "Epoch[0], Batch[1614], Train loss: 0.05004230886697769\n",
      "Epoch[0], Val loss: 0.04863927140831947\n",
      "Epoch[0], Batch[1615], Train loss: 0.05165805295109749\n",
      "Epoch[0], Val loss: 0.04604463651776314\n",
      "Epoch[0], Batch[1616], Train loss: 0.05139916017651558\n",
      "Epoch[0], Val loss: 0.048006877303123474\n",
      "Epoch[0], Batch[1617], Train loss: 0.04930597171187401\n",
      "Epoch[0], Val loss: 0.047478556632995605\n",
      "Epoch[0], Batch[1618], Train loss: 0.05365486815571785\n",
      "Epoch[0], Val loss: 0.045171868056058884\n",
      "Epoch[0], Batch[1619], Train loss: 0.05130244791507721\n",
      "Epoch[0], Val loss: 0.05005961284041405\n",
      "Epoch[0], Batch[1620], Train loss: 0.051856789737939835\n",
      "Epoch[0], Val loss: 0.04987240955233574\n",
      "Epoch[0], Batch[1621], Train loss: 0.04921860992908478\n",
      "Epoch[0], Val loss: 0.052713099867105484\n",
      "Epoch[0], Batch[1622], Train loss: 0.052087411284446716\n",
      "Epoch[0], Val loss: 0.048798900097608566\n",
      "Epoch[0], Batch[1623], Train loss: 0.04908124357461929\n",
      "Epoch[0], Val loss: 0.04837298393249512\n",
      "Epoch[0], Batch[1624], Train loss: 0.05046500638127327\n",
      "Epoch[0], Val loss: 0.048572901636362076\n",
      "Epoch[0], Batch[1625], Train loss: 0.051206834614276886\n",
      "Epoch[0], Val loss: 0.048461154103279114\n",
      "Epoch[0], Batch[1626], Train loss: 0.05080960690975189\n",
      "Epoch[0], Val loss: 0.04742594435811043\n",
      "Epoch[0], Batch[1627], Train loss: 0.05319156125187874\n",
      "Epoch[0], Val loss: 0.05283614993095398\n",
      "Epoch[0], Batch[1628], Train loss: 0.05143805220723152\n",
      "Epoch[0], Val loss: 0.045562759041786194\n",
      "Epoch[0], Batch[1629], Train loss: 0.05201907083392143\n",
      "Epoch[0], Val loss: 0.0466017983853817\n",
      "Epoch[0], Batch[1630], Train loss: 0.05127767100930214\n",
      "Epoch[0], Val loss: 0.045713502913713455\n",
      "Epoch[0], Batch[1631], Train loss: 0.05034736171364784\n",
      "Epoch[0], Val loss: 0.050632402300834656\n",
      "Epoch[0], Batch[1632], Train loss: 0.04958587884902954\n",
      "Epoch[0], Val loss: 0.04616318643093109\n",
      "Epoch[0], Batch[1633], Train loss: 0.05280737206339836\n",
      "Epoch[0], Val loss: 0.046561840921640396\n",
      "Epoch[0], Batch[1634], Train loss: 0.049645621329545975\n",
      "Epoch[0], Val loss: 0.04784493148326874\n",
      "Epoch[0], Batch[1635], Train loss: 0.053366079926490784\n",
      "Epoch[0], Val loss: 0.04967430979013443\n",
      "Epoch[0], Batch[1636], Train loss: 0.05065548047423363\n",
      "Epoch[0], Val loss: 0.04698639363050461\n",
      "Epoch[0], Batch[1637], Train loss: 0.05060111731290817\n",
      "Epoch[0], Val loss: 0.049302589148283005\n",
      "Epoch[0], Batch[1638], Train loss: 0.05170614644885063\n",
      "Epoch[0], Val loss: 0.04784475266933441\n",
      "Epoch[0], Batch[1639], Train loss: 0.051017798483371735\n",
      "Epoch[0], Val loss: 0.050763439387083054\n",
      "Epoch[0], Batch[1640], Train loss: 0.050854023545980453\n",
      "Epoch[0], Val loss: 0.04899050295352936\n",
      "Epoch[0], Batch[1641], Train loss: 0.05127304419875145\n",
      "Epoch[0], Val loss: 0.04542376846075058\n",
      "Epoch[0], Batch[1642], Train loss: 0.04990009218454361\n",
      "Epoch[0], Val loss: 0.0484364777803421\n",
      "Epoch[0], Batch[1643], Train loss: 0.04969280958175659\n",
      "Epoch[0], Val loss: 0.048966530710458755\n",
      "Epoch[0], Batch[1644], Train loss: 0.04972813278436661\n",
      "Epoch[0], Val loss: 0.046855852007865906\n",
      "Epoch[0], Batch[1645], Train loss: 0.04991995170712471\n",
      "Epoch[0], Val loss: 0.049935244023799896\n",
      "Epoch[0], Batch[1646], Train loss: 0.05414719879627228\n",
      "Epoch[0], Val loss: 0.05039868876338005\n",
      "Epoch[0], Batch[1647], Train loss: 0.056437522172927856\n",
      "Epoch[0], Val loss: 0.04525906220078468\n",
      "Epoch[0], Batch[1648], Train loss: 0.04808688908815384\n",
      "Epoch[0], Val loss: 0.047460101544857025\n",
      "Epoch[0], Batch[1649], Train loss: 0.05271824449300766\n",
      "Epoch[0], Val loss: 0.04691516235470772\n",
      "Epoch[0], Batch[1650], Train loss: 0.049402978271245956\n",
      "Epoch[0], Val loss: 0.04809359088540077\n",
      "Epoch[0], Batch[1651], Train loss: 0.05428629741072655\n",
      "Epoch[0], Val loss: 0.04759132117033005\n",
      "Epoch[0], Batch[1652], Train loss: 0.04874217137694359\n",
      "Epoch[0], Val loss: 0.04876505583524704\n",
      "Epoch[0], Batch[1653], Train loss: 0.049702756106853485\n",
      "Epoch[0], Val loss: 0.050130121409893036\n",
      "Epoch[0], Batch[1654], Train loss: 0.05383773893117905\n",
      "Epoch[0], Val loss: 0.04965018108487129\n",
      "Epoch[0], Batch[1655], Train loss: 0.04993777722120285\n",
      "Epoch[0], Val loss: 0.04761751741170883\n",
      "Epoch[0], Batch[1656], Train loss: 0.049969568848609924\n",
      "Epoch[0], Val loss: 0.049487728625535965\n",
      "Epoch[0], Batch[1657], Train loss: 0.050677213817834854\n",
      "Epoch[0], Val loss: 0.04573686793446541\n",
      "Epoch[0], Batch[1658], Train loss: 0.04814597964286804\n",
      "Epoch[0], Val loss: 0.04903339594602585\n",
      "Epoch[0], Batch[1659], Train loss: 0.05435286834836006\n",
      "Epoch[0], Val loss: 0.04785025864839554\n",
      "Epoch[0], Batch[1660], Train loss: 0.04944229498505592\n",
      "Epoch[0], Val loss: 0.05032985284924507\n",
      "Epoch[0], Batch[1661], Train loss: 0.05229628458619118\n",
      "Epoch[0], Val loss: 0.04663032665848732\n",
      "Epoch[0], Batch[1662], Train loss: 0.05171012505888939\n",
      "Epoch[0], Val loss: 0.046950072050094604\n",
      "Epoch[0], Batch[1663], Train loss: 0.04686390236020088\n",
      "Epoch[0], Val loss: 0.048669490963220596\n",
      "Epoch[0], Batch[1664], Train loss: 0.04863005876541138\n",
      "Epoch[0], Val loss: 0.04652056097984314\n",
      "Epoch[0], Batch[1665], Train loss: 0.05110158026218414\n",
      "Epoch[0], Val loss: 0.0481179840862751\n",
      "Epoch[0], Batch[1666], Train loss: 0.047878485172986984\n",
      "Epoch[0], Val loss: 0.04897945746779442\n",
      "Epoch[0], Batch[1667], Train loss: 0.05171528086066246\n",
      "Epoch[0], Val loss: 0.05160188674926758\n",
      "Epoch[0], Batch[1668], Train loss: 0.05383070930838585\n",
      "Epoch[0], Val loss: 0.048319220542907715\n",
      "Epoch[0], Batch[1669], Train loss: 0.05198821425437927\n",
      "Epoch[0], Val loss: 0.05015595629811287\n",
      "Epoch[0], Batch[1670], Train loss: 0.04923684522509575\n",
      "Epoch[0], Val loss: 0.04810986667871475\n",
      "Epoch[0], Batch[1671], Train loss: 0.052910421043634415\n",
      "Epoch[0], Val loss: 0.048032186925411224\n",
      "Epoch[0], Batch[1672], Train loss: 0.053182631731033325\n",
      "Epoch[0], Val loss: 0.04675820469856262\n",
      "Epoch[0], Batch[1673], Train loss: 0.05297398194670677\n",
      "Epoch[0], Val loss: 0.047807756811380386\n",
      "Epoch[0], Batch[1674], Train loss: 0.05007403716444969\n",
      "Epoch[0], Val loss: 0.047025781124830246\n",
      "Epoch[0], Batch[1675], Train loss: 0.05016758292913437\n",
      "Epoch[0], Val loss: 0.0484505333006382\n",
      "Epoch[0], Batch[1676], Train loss: 0.04956952854990959\n",
      "Epoch[0], Val loss: 0.04790002480149269\n",
      "Epoch[0], Batch[1677], Train loss: 0.0501210018992424\n",
      "Epoch[0], Val loss: 0.05178050696849823\n",
      "Epoch[0], Batch[1678], Train loss: 0.05016440898180008\n",
      "Epoch[0], Val loss: 0.045452602207660675\n",
      "Epoch[0], Batch[1679], Train loss: 0.05021175742149353\n",
      "Epoch[0], Val loss: 0.051380712538957596\n",
      "Epoch[0], Batch[1680], Train loss: 0.05056565999984741\n",
      "Epoch[0], Val loss: 0.04639210179448128\n",
      "Epoch[0], Batch[1681], Train loss: 0.05022187530994415\n",
      "Epoch[0], Val loss: 0.0460536926984787\n",
      "Epoch[0], Batch[1682], Train loss: 0.048304617404937744\n",
      "Epoch[0], Val loss: 0.05082122981548309\n",
      "Epoch[0], Batch[1683], Train loss: 0.04951495677232742\n",
      "Epoch[0], Val loss: 0.04502986744046211\n",
      "Epoch[0], Batch[1684], Train loss: 0.04972591996192932\n",
      "Epoch[0], Val loss: 0.046883393079042435\n",
      "Epoch[0], Batch[1685], Train loss: 0.05186501890420914\n",
      "Epoch[0], Val loss: 0.046923521906137466\n",
      "Epoch[0], Batch[1686], Train loss: 0.04780694097280502\n",
      "Epoch[0], Val loss: 0.0474700964987278\n",
      "Epoch[0], Batch[1687], Train loss: 0.051797281950712204\n",
      "Epoch[0], Val loss: 0.05011449381709099\n",
      "Epoch[0], Batch[1688], Train loss: 0.050533026456832886\n",
      "Epoch[0], Val loss: 0.048282403498888016\n",
      "Epoch[0], Batch[1689], Train loss: 0.05023762583732605\n",
      "Epoch[0], Val loss: 0.04663395136594772\n",
      "Epoch[0], Batch[1690], Train loss: 0.05041155591607094\n",
      "Epoch[0], Val loss: 0.04974516108632088\n",
      "Epoch[0], Batch[1691], Train loss: 0.049622196704149246\n",
      "Epoch[0], Val loss: 0.04499184712767601\n",
      "Epoch[0], Batch[1692], Train loss: 0.05061982572078705\n",
      "Epoch[0], Val loss: 0.048077140003442764\n",
      "Epoch[0], Batch[1693], Train loss: 0.05467623844742775\n",
      "Epoch[0], Val loss: 0.0495324581861496\n",
      "Epoch[0], Batch[1694], Train loss: 0.05131850764155388\n",
      "Epoch[0], Val loss: 0.047430116683244705\n",
      "Epoch[0], Batch[1695], Train loss: 0.052375443279743195\n",
      "Epoch[0], Val loss: 0.04498923197388649\n",
      "Epoch[0], Batch[1696], Train loss: 0.05312969535589218\n",
      "Epoch[0], Val loss: 0.0467185378074646\n",
      "Epoch[0], Batch[1697], Train loss: 0.05066520348191261\n",
      "Epoch[0], Val loss: 0.04638415575027466\n",
      "Epoch[0], Batch[1698], Train loss: 0.05208839103579521\n",
      "Epoch[0], Val loss: 0.04717471823096275\n",
      "Epoch[0], Batch[1699], Train loss: 0.052428681403398514\n",
      "Epoch[0], Val loss: 0.048214785754680634\n",
      "Epoch[0], Batch[1700], Train loss: 0.04958336427807808\n",
      "Epoch[0], Val loss: 0.048051729798316956\n",
      "Epoch[0], Batch[1701], Train loss: 0.05277348682284355\n",
      "Epoch[0], Val loss: 0.04660892114043236\n",
      "Epoch[0], Batch[1702], Train loss: 0.05315385386347771\n",
      "Epoch[0], Val loss: 0.05081668496131897\n",
      "Epoch[0], Batch[1703], Train loss: 0.05266876891255379\n",
      "Epoch[0], Val loss: 0.04849207028746605\n",
      "Epoch[0], Batch[1704], Train loss: 0.05120403692126274\n",
      "Epoch[0], Val loss: 0.04848744347691536\n",
      "Epoch[0], Batch[1705], Train loss: 0.051464688032865524\n",
      "Epoch[0], Val loss: 0.049678217619657516\n",
      "Epoch[0], Batch[1706], Train loss: 0.05058754235506058\n",
      "Epoch[0], Val loss: 0.048105355352163315\n",
      "Epoch[0], Batch[1707], Train loss: 0.048659391701221466\n",
      "Epoch[0], Val loss: 0.047246649861335754\n",
      "Epoch[0], Batch[1708], Train loss: 0.0518174022436142\n",
      "Epoch[0], Val loss: 0.0461091622710228\n",
      "Epoch[0], Batch[1709], Train loss: 0.051325730979442596\n",
      "Epoch[0], Val loss: 0.04980048909783363\n",
      "Epoch[0], Batch[1710], Train loss: 0.049831002950668335\n",
      "Epoch[0], Val loss: 0.04691775515675545\n",
      "Epoch[0], Batch[1711], Train loss: 0.04965425282716751\n",
      "Epoch[0], Val loss: 0.045547306537628174\n",
      "Epoch[0], Batch[1712], Train loss: 0.052456893026828766\n",
      "Epoch[0], Val loss: 0.04716317728161812\n",
      "Epoch[0], Batch[1713], Train loss: 0.050764620304107666\n",
      "Epoch[0], Val loss: 0.044699057936668396\n",
      "Epoch[0], Batch[1714], Train loss: 0.0504172258079052\n",
      "Epoch[0], Val loss: 0.046354252845048904\n",
      "Epoch[0], Batch[1715], Train loss: 0.04953160509467125\n",
      "Epoch[0], Val loss: 0.049023933708667755\n",
      "Epoch[0], Batch[1716], Train loss: 0.05120488256216049\n",
      "Epoch[0], Val loss: 0.047028474509716034\n",
      "Epoch[0], Batch[1717], Train loss: 0.05314129590988159\n",
      "Epoch[0], Val loss: 0.04820786416530609\n",
      "Epoch[0], Batch[1718], Train loss: 0.050227876752614975\n",
      "Epoch[0], Val loss: 0.04783684387803078\n",
      "Epoch[0], Batch[1719], Train loss: 0.04967891424894333\n",
      "Epoch[0], Val loss: 0.05030830204486847\n",
      "Epoch[0], Batch[1720], Train loss: 0.04942268133163452\n",
      "Epoch[0], Val loss: 0.04454199969768524\n",
      "Epoch[0], Batch[1721], Train loss: 0.05047735944390297\n",
      "Epoch[0], Val loss: 0.04631848260760307\n",
      "Epoch[0], Batch[1722], Train loss: 0.050505854189395905\n",
      "Epoch[0], Val loss: 0.04551763832569122\n",
      "Epoch[0], Batch[1723], Train loss: 0.05324220657348633\n",
      "Epoch[0], Val loss: 0.04916926100850105\n",
      "Epoch[0], Batch[1724], Train loss: 0.05083288997411728\n",
      "Epoch[0], Val loss: 0.048159025609493256\n",
      "Epoch[0], Batch[1725], Train loss: 0.04956740885972977\n",
      "Epoch[0], Val loss: 0.047601524740457535\n",
      "Epoch[0], Batch[1726], Train loss: 0.051818057894706726\n",
      "Epoch[0], Val loss: 0.04603146016597748\n",
      "Epoch[0], Batch[1727], Train loss: 0.04984291270375252\n",
      "Epoch[0], Val loss: 0.05170001834630966\n",
      "Epoch[0], Batch[1728], Train loss: 0.05096223205327988\n",
      "Epoch[0], Val loss: 0.049761202186346054\n",
      "Epoch[0], Batch[1729], Train loss: 0.048778507858514786\n",
      "Epoch[0], Val loss: 0.04761572927236557\n",
      "Epoch[0], Batch[1730], Train loss: 0.05119564011693001\n",
      "Epoch[0], Val loss: 0.04791419953107834\n",
      "Epoch[0], Batch[1731], Train loss: 0.04938478395342827\n",
      "Epoch[0], Val loss: 0.046146754175424576\n",
      "Epoch[0], Batch[1732], Train loss: 0.04778522625565529\n",
      "Epoch[0], Val loss: 0.04468730837106705\n",
      "Epoch[0], Batch[1733], Train loss: 0.05235758796334267\n",
      "Epoch[0], Val loss: 0.04782072454690933\n",
      "Epoch[0], Batch[1734], Train loss: 0.055797919631004333\n",
      "Epoch[0], Val loss: 0.050097111612558365\n",
      "Epoch[0], Batch[1735], Train loss: 0.05144216865301132\n",
      "Epoch[0], Val loss: 0.04943707585334778\n",
      "Epoch[0], Batch[1736], Train loss: 0.04683296009898186\n",
      "Epoch[0], Val loss: 0.04711572453379631\n",
      "Epoch[0], Batch[1737], Train loss: 0.048100896179676056\n",
      "Epoch[0], Val loss: 0.04609458148479462\n",
      "Epoch[0], Batch[1738], Train loss: 0.05017653480172157\n",
      "Epoch[0], Val loss: 0.04839487746357918\n",
      "Epoch[0], Batch[1739], Train loss: 0.05028294399380684\n",
      "Epoch[0], Val loss: 0.042587388306856155\n",
      "Epoch[0], Batch[1740], Train loss: 0.048171307891607285\n",
      "Epoch[0], Val loss: 0.051580313593149185\n",
      "Epoch[0], Batch[1741], Train loss: 0.05151379480957985\n",
      "Epoch[0], Val loss: 0.04531940445303917\n",
      "Epoch[0], Batch[1742], Train loss: 0.04960568621754646\n",
      "Epoch[0], Val loss: 0.0449763759970665\n",
      "Epoch[0], Batch[1743], Train loss: 0.0487518273293972\n",
      "Epoch[0], Val loss: 0.04888186231255531\n",
      "Epoch[0], Batch[1744], Train loss: 0.04855376482009888\n",
      "Epoch[0], Val loss: 0.04733878746628761\n",
      "Epoch[0], Batch[1745], Train loss: 0.052725378423929214\n",
      "Epoch[0], Val loss: 0.04432124271988869\n",
      "Epoch[0], Batch[1746], Train loss: 0.04986660182476044\n",
      "Epoch[0], Val loss: 0.046313971281051636\n",
      "Epoch[0], Batch[1747], Train loss: 0.049163784831762314\n",
      "Epoch[0], Val loss: 0.044940341264009476\n",
      "Epoch[0], Batch[1748], Train loss: 0.05218621715903282\n",
      "Epoch[0], Val loss: 0.04909146577119827\n",
      "Epoch[0], Batch[1749], Train loss: 0.04888635873794556\n",
      "Epoch[0], Val loss: 0.04862263426184654\n",
      "Epoch[0], Batch[1750], Train loss: 0.05199820175766945\n",
      "Epoch[0], Val loss: 0.04844061657786369\n",
      "Epoch[0], Batch[1751], Train loss: 0.049748554825782776\n",
      "Epoch[0], Val loss: 0.049280840903520584\n",
      "Epoch[0], Batch[1752], Train loss: 0.0497477762401104\n",
      "Epoch[0], Val loss: 0.04618591442704201\n",
      "Epoch[0], Batch[1753], Train loss: 0.04987028241157532\n",
      "Epoch[0], Val loss: 0.04598868265748024\n",
      "Epoch[0], Batch[1754], Train loss: 0.05246959254145622\n",
      "Epoch[0], Val loss: 0.044601861387491226\n",
      "Epoch[0], Batch[1755], Train loss: 0.050054702907800674\n",
      "Epoch[0], Val loss: 0.047984153032302856\n",
      "Epoch[0], Batch[1756], Train loss: 0.052258748561143875\n",
      "Epoch[0], Val loss: 0.04697302728891373\n",
      "Epoch[0], Batch[1757], Train loss: 0.04864591732621193\n",
      "Epoch[0], Val loss: 0.04795290529727936\n",
      "Epoch[0], Batch[1758], Train loss: 0.0501776821911335\n",
      "Epoch[0], Val loss: 0.04698112979531288\n",
      "Epoch[0], Batch[1759], Train loss: 0.04646719619631767\n",
      "Epoch[0], Val loss: 0.04980389401316643\n",
      "Epoch[0], Batch[1760], Train loss: 0.05016801878809929\n",
      "Epoch[0], Val loss: 0.043026864528656006\n",
      "Epoch[0], Batch[1761], Train loss: 0.048975568264722824\n",
      "Epoch[0], Val loss: 0.0471549928188324\n",
      "Epoch[0], Batch[1762], Train loss: 0.04946376010775566\n",
      "Epoch[0], Val loss: 0.04821865260601044\n",
      "Epoch[0], Batch[1763], Train loss: 0.0475178025662899\n",
      "Epoch[0], Val loss: 0.048721134662628174\n",
      "Epoch[0], Batch[1764], Train loss: 0.047339677810668945\n",
      "Epoch[0], Val loss: 0.04812677949666977\n",
      "Epoch[0], Batch[1765], Train loss: 0.050512224435806274\n",
      "Epoch[0], Val loss: 0.0478072352707386\n",
      "Epoch[0], Batch[1766], Train loss: 0.045972201973199844\n",
      "Epoch[0], Val loss: 0.0516490712761879\n",
      "Epoch[0], Batch[1767], Train loss: 0.04933440312743187\n",
      "Epoch[0], Val loss: 0.04762785881757736\n",
      "Epoch[0], Batch[1768], Train loss: 0.0504639707505703\n",
      "Epoch[0], Val loss: 0.0444033145904541\n",
      "Epoch[0], Batch[1769], Train loss: 0.04989545792341232\n",
      "Epoch[0], Val loss: 0.045410096645355225\n",
      "Epoch[0], Batch[1770], Train loss: 0.050558436661958694\n",
      "Epoch[0], Val loss: 0.04608440399169922\n",
      "Epoch[0], Batch[1771], Train loss: 0.04968447610735893\n",
      "Epoch[0], Val loss: 0.046549856662750244\n",
      "Epoch[0], Batch[1772], Train loss: 0.051164161413908005\n",
      "Epoch[0], Val loss: 0.047843657433986664\n",
      "Epoch[0], Batch[1773], Train loss: 0.05098453164100647\n",
      "Epoch[0], Val loss: 0.047870513051748276\n",
      "Epoch[0], Batch[1774], Train loss: 0.05142123997211456\n",
      "Epoch[0], Val loss: 0.05045710876584053\n",
      "Epoch[0], Batch[1775], Train loss: 0.05081074312329292\n",
      "Epoch[0], Val loss: 0.045207880437374115\n",
      "Epoch[0], Batch[1776], Train loss: 0.051632050424814224\n",
      "Epoch[0], Val loss: 0.04362260922789574\n",
      "Epoch[0], Batch[1777], Train loss: 0.04958577826619148\n",
      "Epoch[0], Val loss: 0.04865255951881409\n",
      "Epoch[0], Batch[1778], Train loss: 0.04978591203689575\n",
      "Epoch[0], Val loss: 0.048602744936943054\n",
      "Epoch[0], Batch[1779], Train loss: 0.04580705612897873\n",
      "Epoch[0], Val loss: 0.04732351750135422\n",
      "Epoch[0], Batch[1780], Train loss: 0.04834393039345741\n",
      "Epoch[0], Val loss: 0.048231951892375946\n",
      "Epoch[0], Batch[1781], Train loss: 0.049735527485609055\n",
      "Epoch[0], Val loss: 0.048263758420944214\n",
      "Epoch[0], Batch[1782], Train loss: 0.053148314356803894\n",
      "Epoch[0], Val loss: 0.0478312186896801\n",
      "Epoch[0], Batch[1783], Train loss: 0.05357208102941513\n",
      "Epoch[0], Val loss: 0.04558597877621651\n",
      "Epoch[0], Batch[1784], Train loss: 0.04758477583527565\n",
      "Epoch[0], Val loss: 0.046762507408857346\n",
      "Epoch[0], Batch[1785], Train loss: 0.05005466938018799\n",
      "Epoch[0], Val loss: 0.04939942806959152\n",
      "Epoch[0], Batch[1786], Train loss: 0.047530632466077805\n",
      "Epoch[0], Val loss: 0.048139430582523346\n",
      "Epoch[0], Batch[1787], Train loss: 0.0489119291305542\n",
      "Epoch[0], Val loss: 0.05023771896958351\n",
      "Epoch[0], Batch[1788], Train loss: 0.05017121881246567\n",
      "Epoch[0], Val loss: 0.046501804143190384\n",
      "Epoch[0], Batch[1789], Train loss: 0.04804360494017601\n",
      "Epoch[0], Val loss: 0.05227476358413696\n",
      "Epoch[0], Batch[1790], Train loss: 0.05217110365629196\n",
      "Epoch[0], Val loss: 0.04550494998693466\n",
      "Epoch[0], Batch[1791], Train loss: 0.04860188439488411\n",
      "Epoch[0], Val loss: 0.04536847025156021\n",
      "Epoch[0], Batch[1792], Train loss: 0.05001015588641167\n",
      "Epoch[0], Val loss: 0.044269856065511703\n",
      "Epoch[0], Batch[1793], Train loss: 0.05248555541038513\n",
      "Epoch[0], Val loss: 0.05302022024989128\n",
      "Epoch[0], Batch[1794], Train loss: 0.04872589185833931\n",
      "Epoch[0], Val loss: 0.04546855390071869\n",
      "Epoch[0], Batch[1795], Train loss: 0.04811130836606026\n",
      "Epoch[0], Val loss: 0.04978037625551224\n",
      "Epoch[0], Batch[1796], Train loss: 0.04844091087579727\n",
      "Epoch[0], Val loss: 0.045340005308389664\n",
      "Epoch[0], Batch[1797], Train loss: 0.04932798445224762\n",
      "Epoch[0], Val loss: 0.04450559988617897\n",
      "Epoch[0], Batch[1798], Train loss: 0.05388442426919937\n",
      "Epoch[0], Val loss: 0.04534836485981941\n",
      "Epoch[0], Batch[1799], Train loss: 0.05197279900312424\n",
      "Epoch[0], Val loss: 0.048323847353458405\n",
      "Epoch[0], Batch[1800], Train loss: 0.05083948001265526\n",
      "Epoch[0], Val loss: 0.04596545174717903\n",
      "Epoch[0], Batch[1801], Train loss: 0.04836190864443779\n",
      "Epoch[0], Val loss: 0.04765460640192032\n",
      "Epoch[0], Batch[1802], Train loss: 0.049700282514095306\n",
      "Epoch[0], Val loss: 0.046209488064050674\n",
      "Epoch[0], Batch[1803], Train loss: 0.0505581833422184\n",
      "Epoch[0], Val loss: 0.047446753829717636\n",
      "Epoch[0], Batch[1804], Train loss: 0.05319725722074509\n",
      "Epoch[0], Val loss: 0.045789483934640884\n",
      "Epoch[0], Batch[1805], Train loss: 0.05052320286631584\n",
      "Epoch[0], Val loss: 0.047494109719991684\n",
      "Epoch[0], Batch[1806], Train loss: 0.052797336131334305\n",
      "Epoch[0], Val loss: 0.0460546649992466\n",
      "Epoch[0], Batch[1807], Train loss: 0.048439495265483856\n",
      "Epoch[0], Val loss: 0.04608865827322006\n",
      "Epoch[0], Batch[1808], Train loss: 0.047899942845106125\n",
      "Epoch[0], Val loss: 0.047172293066978455\n",
      "Epoch[0], Batch[1809], Train loss: 0.05111918970942497\n",
      "Epoch[0], Val loss: 0.048402994871139526\n",
      "Epoch[0], Batch[1810], Train loss: 0.0467485673725605\n",
      "Epoch[0], Val loss: 0.048358507454395294\n",
      "Epoch[0], Batch[1811], Train loss: 0.05171254649758339\n",
      "Epoch[0], Val loss: 0.04724985361099243\n",
      "Epoch[0], Batch[1812], Train loss: 0.05073018744587898\n",
      "Epoch[0], Val loss: 0.04636524245142937\n",
      "Epoch[0], Batch[1813], Train loss: 0.04974589869379997\n",
      "Epoch[0], Val loss: 0.04507672041654587\n",
      "Epoch[0], Batch[1814], Train loss: 0.052185285836458206\n",
      "Epoch[0], Val loss: 0.04775359109044075\n",
      "Epoch[0], Batch[1815], Train loss: 0.04818399250507355\n",
      "Epoch[0], Val loss: 0.04779382422566414\n",
      "Epoch[0], Batch[1816], Train loss: 0.047767993062734604\n",
      "Epoch[0], Val loss: 0.047883469611406326\n",
      "Epoch[0], Batch[1817], Train loss: 0.04885187745094299\n",
      "Epoch[0], Val loss: 0.04605870321393013\n",
      "Epoch[0], Batch[1818], Train loss: 0.0492178238928318\n",
      "Epoch[0], Val loss: 0.048110898584127426\n",
      "Epoch[0], Batch[1819], Train loss: 0.049690503627061844\n",
      "Epoch[0], Val loss: 0.04708478972315788\n",
      "Epoch[0], Batch[1820], Train loss: 0.049932707101106644\n",
      "Epoch[0], Val loss: 0.04455668479204178\n",
      "Epoch[0], Batch[1821], Train loss: 0.049809928983449936\n",
      "Epoch[0], Val loss: 0.04850352555513382\n",
      "Epoch[0], Batch[1822], Train loss: 0.049117837101221085\n",
      "Epoch[0], Val loss: 0.04714134335517883\n",
      "Epoch[0], Batch[1823], Train loss: 0.04832630231976509\n",
      "Epoch[0], Val loss: 0.04804017394781113\n",
      "Epoch[0], Batch[1824], Train loss: 0.051547419279813766\n",
      "Epoch[0], Val loss: 0.045621875673532486\n",
      "Epoch[0], Batch[1825], Train loss: 0.05140572041273117\n",
      "Epoch[0], Val loss: 0.0460331030189991\n",
      "Epoch[0], Batch[1826], Train loss: 0.05297777056694031\n",
      "Epoch[0], Val loss: 0.047485072165727615\n",
      "Epoch[0], Batch[1827], Train loss: 0.04843449592590332\n",
      "Epoch[0], Val loss: 0.0453672856092453\n",
      "Epoch[0], Batch[1828], Train loss: 0.04994341731071472\n",
      "Epoch[0], Val loss: 0.04905226454138756\n",
      "Epoch[0], Batch[1829], Train loss: 0.05045172944664955\n",
      "Epoch[0], Val loss: 0.04709494113922119\n",
      "Epoch[0], Batch[1830], Train loss: 0.04830782860517502\n",
      "Epoch[0], Val loss: 0.04963807016611099\n",
      "Epoch[0], Batch[1831], Train loss: 0.05070190131664276\n",
      "Epoch[0], Val loss: 0.049559250473976135\n",
      "Epoch[0], Batch[1832], Train loss: 0.04921383783221245\n",
      "Epoch[0], Val loss: 0.04685624688863754\n",
      "Epoch[0], Batch[1833], Train loss: 0.050279151648283005\n",
      "Epoch[0], Val loss: 0.04759923741221428\n",
      "Epoch[0], Batch[1834], Train loss: 0.047206953167915344\n",
      "Epoch[0], Val loss: 0.04788843169808388\n",
      "Epoch[0], Batch[1835], Train loss: 0.05124007165431976\n",
      "Epoch[0], Val loss: 0.04571933299303055\n",
      "Epoch[0], Batch[1836], Train loss: 0.04969535395503044\n",
      "Epoch[0], Val loss: 0.04437718540430069\n",
      "Epoch[0], Batch[1837], Train loss: 0.05026132985949516\n",
      "Epoch[0], Val loss: 0.04585985094308853\n",
      "Epoch[0], Batch[1838], Train loss: 0.052332062274217606\n",
      "Epoch[0], Val loss: 0.0479285791516304\n",
      "Epoch[0], Batch[1839], Train loss: 0.04798635467886925\n",
      "Epoch[0], Val loss: 0.04761356860399246\n",
      "Epoch[0], Batch[1840], Train loss: 0.04990270733833313\n",
      "Epoch[0], Val loss: 0.044110216200351715\n",
      "Epoch[0], Batch[1841], Train loss: 0.04895062372088432\n",
      "Epoch[0], Val loss: 0.04702970013022423\n",
      "Epoch[0], Batch[1842], Train loss: 0.05003199353814125\n",
      "Epoch[0], Val loss: 0.047749944031238556\n",
      "Epoch[0], Batch[1843], Train loss: 0.055049896240234375\n",
      "Epoch[0], Val loss: 0.046268850564956665\n",
      "Epoch[0], Batch[1844], Train loss: 0.05102655664086342\n",
      "Epoch[0], Val loss: 0.044596027582883835\n",
      "Epoch[0], Batch[1845], Train loss: 0.051013194024562836\n",
      "Epoch[0], Val loss: 0.04788823798298836\n",
      "Epoch[0], Batch[1846], Train loss: 0.04789731651544571\n",
      "Epoch[0], Val loss: 0.04753384366631508\n",
      "Epoch[0], Batch[1847], Train loss: 0.04811558872461319\n",
      "Epoch[0], Val loss: 0.047753650695085526\n",
      "Epoch[0], Batch[1848], Train loss: 0.05058226361870766\n",
      "Epoch[0], Val loss: 0.04534979537129402\n",
      "Epoch[0], Batch[1849], Train loss: 0.04823952168226242\n",
      "Epoch[0], Val loss: 0.04499168321490288\n",
      "Epoch[0], Batch[1850], Train loss: 0.0496024563908577\n",
      "Epoch[0], Val loss: 0.046711333096027374\n",
      "Epoch[0], Batch[1851], Train loss: 0.05178084969520569\n",
      "Epoch[0], Val loss: 0.04440901800990105\n",
      "Epoch[0], Batch[1852], Train loss: 0.0478639118373394\n",
      "Epoch[0], Val loss: 0.047703973948955536\n",
      "Epoch[0], Batch[1853], Train loss: 0.04822469875216484\n",
      "Epoch[0], Val loss: 0.04639146476984024\n",
      "Epoch[0], Batch[1854], Train loss: 0.048633795231580734\n",
      "Epoch[0], Val loss: 0.04782221466302872\n",
      "Epoch[0], Batch[1855], Train loss: 0.049682941287755966\n",
      "Epoch[0], Val loss: 0.050202786922454834\n",
      "Epoch[0], Batch[1856], Train loss: 0.05014438182115555\n",
      "Epoch[0], Val loss: 0.04702296853065491\n",
      "Epoch[0], Batch[1857], Train loss: 0.04667799174785614\n",
      "Epoch[0], Val loss: 0.04694301262497902\n",
      "Epoch[0], Batch[1858], Train loss: 0.04975321516394615\n",
      "Epoch[0], Val loss: 0.04408792033791542\n",
      "Epoch[0], Batch[1859], Train loss: 0.05061628669500351\n",
      "Epoch[0], Val loss: 0.04484105855226517\n",
      "Epoch[0], Batch[1860], Train loss: 0.04775308445096016\n",
      "Epoch[0], Val loss: 0.04374990612268448\n",
      "Epoch[0], Batch[1861], Train loss: 0.0479687936604023\n",
      "Epoch[0], Val loss: 0.04468487948179245\n",
      "Epoch[0], Batch[1862], Train loss: 0.0500076487660408\n",
      "Epoch[0], Val loss: 0.04608980566263199\n",
      "Epoch[0], Batch[1863], Train loss: 0.047857895493507385\n",
      "Epoch[0], Val loss: 0.042801134288311005\n",
      "Epoch[0], Batch[1864], Train loss: 0.04974975064396858\n",
      "Epoch[0], Val loss: 0.0475967600941658\n",
      "Epoch[0], Batch[1865], Train loss: 0.05336032435297966\n",
      "Epoch[0], Val loss: 0.044584695249795914\n",
      "Epoch[0], Batch[1866], Train loss: 0.048715073615312576\n",
      "Epoch[0], Val loss: 0.04574993625283241\n",
      "Epoch[0], Batch[1867], Train loss: 0.04864747077226639\n",
      "Epoch[0], Val loss: 0.04552984982728958\n",
      "Epoch[0], Batch[1868], Train loss: 0.04771297797560692\n",
      "Epoch[0], Val loss: 0.04855595901608467\n",
      "Epoch[0], Batch[1869], Train loss: 0.05105139687657356\n",
      "Epoch[0], Val loss: 0.04808557778596878\n",
      "Epoch[0], Batch[1870], Train loss: 0.0487961508333683\n",
      "Epoch[0], Val loss: 0.048421621322631836\n",
      "Epoch[0], Batch[1871], Train loss: 0.049300141632556915\n",
      "Epoch[0], Val loss: 0.04501550272107124\n",
      "Epoch[0], Batch[1872], Train loss: 0.04718725010752678\n",
      "Epoch[0], Val loss: 0.04833348095417023\n",
      "Epoch[0], Batch[1873], Train loss: 0.048615239560604095\n",
      "Epoch[0], Val loss: 0.04896388575434685\n",
      "Epoch[0], Batch[1874], Train loss: 0.04895946756005287\n",
      "Epoch[0], Val loss: 0.047967467457056046\n",
      "Epoch[0], Batch[1875], Train loss: 0.04802829399704933\n",
      "Epoch[0], Val loss: 0.04617176949977875\n",
      "Epoch[0], Batch[1876], Train loss: 0.04774430766701698\n",
      "Epoch[0], Val loss: 0.04962095618247986\n",
      "Epoch[0], Batch[1877], Train loss: 0.04651260003447533\n",
      "Epoch[0], Val loss: 0.04568687826395035\n",
      "Epoch[0], Batch[1878], Train loss: 0.04867313429713249\n",
      "Epoch[0], Val loss: 0.0473296195268631\n",
      "Epoch[0], Batch[1879], Train loss: 0.04594074562191963\n",
      "Epoch[0], Val loss: 0.04258193075656891\n",
      "Epoch[0], Batch[1880], Train loss: 0.046451449394226074\n",
      "Epoch[0], Val loss: 0.04563058912754059\n",
      "Epoch[0], Batch[1881], Train loss: 0.050079990178346634\n",
      "Epoch[0], Val loss: 0.044790808111429214\n",
      "Epoch[0], Batch[1882], Train loss: 0.046990521252155304\n",
      "Epoch[0], Val loss: 0.04748931899666786\n",
      "Epoch[0], Batch[1883], Train loss: 0.052574481815099716\n",
      "Epoch[0], Val loss: 0.04485701397061348\n",
      "Epoch[0], Batch[1884], Train loss: 0.044835157692432404\n",
      "Epoch[0], Val loss: 0.04447424039244652\n",
      "Epoch[0], Batch[1885], Train loss: 0.051868729293346405\n",
      "Epoch[0], Val loss: 0.04852600395679474\n",
      "Epoch[0], Batch[1886], Train loss: 0.04827956110239029\n",
      "Epoch[0], Val loss: 0.04286932200193405\n",
      "Epoch[0], Batch[1887], Train loss: 0.05080784112215042\n",
      "Epoch[0], Val loss: 0.04547790065407753\n",
      "Epoch[0], Batch[1888], Train loss: 0.047246869653463364\n",
      "Epoch[0], Val loss: 0.04813127964735031\n",
      "Epoch[0], Batch[1889], Train loss: 0.046071022748947144\n",
      "Epoch[0], Val loss: 0.04832015186548233\n",
      "Epoch[0], Batch[1890], Train loss: 0.04835871234536171\n",
      "Epoch[0], Val loss: 0.047815218567848206\n",
      "Epoch[0], Batch[1891], Train loss: 0.050476133823394775\n",
      "Epoch[0], Val loss: 0.049435801804065704\n",
      "Epoch[0], Batch[1892], Train loss: 0.04627107083797455\n",
      "Epoch[0], Val loss: 0.048236407339572906\n",
      "Epoch[0], Batch[1893], Train loss: 0.0449194498360157\n",
      "Epoch[0], Val loss: 0.048492252826690674\n",
      "Epoch[0], Batch[1894], Train loss: 0.0494905561208725\n",
      "Epoch[0], Val loss: 0.048540931195020676\n",
      "Epoch[0], Batch[1895], Train loss: 0.04858876392245293\n",
      "Epoch[0], Val loss: 0.044966358691453934\n",
      "Epoch[0], Batch[1896], Train loss: 0.05003637820482254\n",
      "Epoch[0], Val loss: 0.04258998855948448\n",
      "Epoch[0], Batch[1897], Train loss: 0.04775243252515793\n",
      "Epoch[0], Val loss: 0.04402465000748634\n",
      "Epoch[0], Batch[1898], Train loss: 0.050529543310403824\n",
      "Epoch[0], Val loss: 0.045885417610406876\n",
      "Epoch[0], Batch[1899], Train loss: 0.0473799929022789\n",
      "Epoch[0], Val loss: 0.04806861653923988\n",
      "Epoch[0], Batch[1900], Train loss: 0.04875825718045235\n",
      "Epoch[0], Val loss: 0.04582683742046356\n",
      "Epoch[0], Batch[1901], Train loss: 0.04991783946752548\n",
      "Epoch[0], Val loss: 0.04535955190658569\n",
      "Epoch[0], Batch[1902], Train loss: 0.04897994548082352\n",
      "Epoch[0], Val loss: 0.048595067113637924\n",
      "Epoch[0], Batch[1903], Train loss: 0.04864194616675377\n",
      "Epoch[0], Val loss: 0.047309983521699905\n",
      "Epoch[0], Batch[1904], Train loss: 0.05254444107413292\n",
      "Epoch[0], Val loss: 0.045804113149642944\n",
      "Epoch[0], Batch[1905], Train loss: 0.05027644336223602\n",
      "Epoch[0], Val loss: 0.04845857247710228\n",
      "Epoch[0], Batch[1906], Train loss: 0.050115346908569336\n",
      "Epoch[0], Val loss: 0.045483943074941635\n",
      "Epoch[0], Batch[1907], Train loss: 0.04904710128903389\n",
      "Epoch[0], Val loss: 0.04627944156527519\n",
      "Epoch[0], Batch[1908], Train loss: 0.047587230801582336\n",
      "Epoch[0], Val loss: 0.04686542600393295\n",
      "Epoch[0], Batch[1909], Train loss: 0.049198176711797714\n",
      "Epoch[0], Val loss: 0.04618808627128601\n",
      "Epoch[0], Batch[1910], Train loss: 0.046801913529634476\n",
      "Epoch[0], Val loss: 0.052862849086523056\n",
      "Epoch[0], Batch[1911], Train loss: 0.049533046782016754\n",
      "Epoch[0], Val loss: 0.04562859982252121\n",
      "Epoch[0], Batch[1912], Train loss: 0.04733942821621895\n",
      "Epoch[0], Val loss: 0.04590612277388573\n",
      "Epoch[0], Batch[1913], Train loss: 0.0480048693716526\n",
      "Epoch[0], Val loss: 0.046874359250068665\n",
      "Epoch[0], Batch[1914], Train loss: 0.048459745943546295\n",
      "Epoch[0], Val loss: 0.04318247362971306\n",
      "Epoch[0], Batch[1915], Train loss: 0.050304852426052094\n",
      "Epoch[0], Val loss: 0.04348074644804001\n",
      "Epoch[0], Batch[1916], Train loss: 0.047930143773555756\n",
      "Epoch[0], Val loss: 0.046820349991321564\n",
      "Epoch[0], Batch[1917], Train loss: 0.048580314964056015\n",
      "Epoch[0], Val loss: 0.04703987017273903\n",
      "Epoch[0], Batch[1918], Train loss: 0.04847889393568039\n",
      "Epoch[0], Val loss: 0.048478029668331146\n",
      "Epoch[0], Batch[1919], Train loss: 0.04750135540962219\n",
      "Epoch[0], Val loss: 0.0456644669175148\n",
      "Epoch[0], Batch[1920], Train loss: 0.05014185234904289\n",
      "Epoch[0], Val loss: 0.04793594032526016\n",
      "Epoch[0], Batch[1921], Train loss: 0.04630858823657036\n",
      "Epoch[0], Val loss: 0.043215908110141754\n",
      "Epoch[0], Batch[1922], Train loss: 0.048952434211969376\n",
      "Epoch[0], Val loss: 0.04734455794095993\n",
      "Epoch[0], Batch[1923], Train loss: 0.04699835926294327\n",
      "Epoch[0], Val loss: 0.04563022032380104\n",
      "Epoch[0], Batch[1924], Train loss: 0.04649558290839195\n",
      "Epoch[0], Val loss: 0.04593298211693764\n",
      "Epoch[0], Batch[1925], Train loss: 0.04957085847854614\n",
      "Epoch[0], Val loss: 0.04617239907383919\n",
      "Epoch[0], Batch[1926], Train loss: 0.04704379290342331\n",
      "Epoch[0], Val loss: 0.04401366785168648\n",
      "Epoch[0], Batch[1927], Train loss: 0.04601103067398071\n",
      "Epoch[0], Val loss: 0.045641399919986725\n",
      "Epoch[0], Batch[1928], Train loss: 0.045750126242637634\n",
      "Epoch[0], Val loss: 0.046458616852760315\n",
      "Epoch[0], Batch[1929], Train loss: 0.049036599695682526\n",
      "Epoch[0], Val loss: 0.04570532962679863\n",
      "Epoch[0], Batch[1930], Train loss: 0.0523553341627121\n",
      "Epoch[0], Val loss: 0.045114051550626755\n",
      "Epoch[0], Batch[1931], Train loss: 0.04420261085033417\n",
      "Epoch[0], Val loss: 0.04482423886656761\n",
      "Epoch[0], Batch[1932], Train loss: 0.04803258180618286\n",
      "Epoch[0], Val loss: 0.04542187228798866\n",
      "Epoch[0], Batch[1933], Train loss: 0.047911472618579865\n",
      "Epoch[0], Val loss: 0.04772064462304115\n",
      "Epoch[0], Batch[1934], Train loss: 0.048810459673404694\n",
      "Epoch[0], Val loss: 0.04311344400048256\n",
      "Epoch[0], Batch[1935], Train loss: 0.04976680502295494\n",
      "Epoch[0], Val loss: 0.04495270922780037\n",
      "Epoch[0], Batch[1936], Train loss: 0.046863362193107605\n",
      "Epoch[0], Val loss: 0.049162086099386215\n",
      "Epoch[0], Batch[1937], Train loss: 0.04888446256518364\n",
      "Epoch[0], Val loss: 0.04750542715191841\n",
      "Epoch[0], Batch[1938], Train loss: 0.05019117891788483\n",
      "Epoch[0], Val loss: 0.04766474664211273\n",
      "Epoch[0], Batch[1939], Train loss: 0.04599319025874138\n",
      "Epoch[0], Val loss: 0.04628558084368706\n",
      "Epoch[0], Batch[1940], Train loss: 0.04908675700426102\n",
      "Epoch[0], Val loss: 0.046903345733881\n",
      "Epoch[0], Batch[1941], Train loss: 0.04779442399740219\n",
      "Epoch[0], Val loss: 0.04777045547962189\n",
      "Epoch[0], Batch[1942], Train loss: 0.04631830379366875\n",
      "Epoch[0], Val loss: 0.04580503702163696\n",
      "Epoch[0], Batch[1943], Train loss: 0.047730445861816406\n",
      "Epoch[0], Val loss: 0.0445789098739624\n",
      "Epoch[0], Batch[1944], Train loss: 0.04808254912495613\n",
      "Epoch[0], Val loss: 0.04340934380888939\n",
      "Epoch[0], Batch[1945], Train loss: 0.04862705245614052\n",
      "Epoch[0], Val loss: 0.043402232229709625\n",
      "Epoch[0], Batch[1946], Train loss: 0.04804235324263573\n",
      "Epoch[0], Val loss: 0.046978071331977844\n",
      "Epoch[0], Batch[1947], Train loss: 0.04962107539176941\n",
      "Epoch[0], Val loss: 0.04694961756467819\n",
      "Epoch[0], Batch[1948], Train loss: 0.050843384116888046\n",
      "Epoch[0], Val loss: 0.04630344733595848\n",
      "Epoch[0], Batch[1949], Train loss: 0.04948100075125694\n",
      "Epoch[0], Val loss: 0.04666967689990997\n",
      "Epoch[0], Batch[1950], Train loss: 0.04923304170370102\n",
      "Epoch[0], Val loss: 0.044726721942424774\n",
      "Epoch[0], Batch[1951], Train loss: 0.05142877995967865\n",
      "Epoch[0], Val loss: 0.045203521847724915\n",
      "Epoch[0], Batch[1952], Train loss: 0.04681982472538948\n",
      "Epoch[0], Val loss: 0.04708263278007507\n",
      "Epoch[0], Batch[1953], Train loss: 0.0466543585062027\n",
      "Epoch[0], Val loss: 0.04762085899710655\n",
      "Epoch[0], Batch[1954], Train loss: 0.049627792090177536\n",
      "Epoch[0], Val loss: 0.04213099926710129\n",
      "Epoch[0], Batch[1955], Train loss: 0.04951981082558632\n",
      "Epoch[0], Val loss: 0.04433352127671242\n",
      "Epoch[0], Batch[1956], Train loss: 0.04972505196928978\n",
      "Epoch[0], Val loss: 0.04253664240241051\n",
      "Epoch[0], Batch[1957], Train loss: 0.04944879561662674\n",
      "Epoch[0], Val loss: 0.04477551579475403\n",
      "Epoch[0], Batch[1958], Train loss: 0.04858472943305969\n",
      "Epoch[0], Val loss: 0.0474061593413353\n",
      "Epoch[0], Batch[1959], Train loss: 0.04954561963677406\n",
      "Epoch[0], Val loss: 0.04372461140155792\n",
      "Epoch[0], Batch[1960], Train loss: 0.04475850984454155\n",
      "Epoch[0], Val loss: 0.04576796665787697\n",
      "Epoch[0], Batch[1961], Train loss: 0.04692421853542328\n",
      "Epoch[0], Val loss: 0.044383931905031204\n",
      "Epoch[0], Batch[1962], Train loss: 0.046527378261089325\n",
      "Epoch[0], Val loss: 0.04465361684560776\n",
      "Epoch[0], Batch[1963], Train loss: 0.04746581241488457\n",
      "Epoch[0], Val loss: 0.04497430473566055\n",
      "Epoch[0], Batch[1964], Train loss: 0.049675703048706055\n",
      "Epoch[0], Val loss: 0.044653166085481644\n",
      "Epoch[0], Batch[1965], Train loss: 0.047950416803359985\n",
      "Epoch[0], Val loss: 0.04755006730556488\n",
      "Epoch[0], Batch[1966], Train loss: 0.04907108098268509\n",
      "Epoch[0], Val loss: 0.04336768388748169\n",
      "Epoch[0], Batch[1967], Train loss: 0.04976974427700043\n",
      "Epoch[0], Val loss: 0.0463225394487381\n",
      "Epoch[0], Batch[1968], Train loss: 0.047985728830099106\n",
      "Epoch[0], Val loss: 0.0456237718462944\n",
      "Epoch[0], Batch[1969], Train loss: 0.04684259369969368\n",
      "Epoch[0], Val loss: 0.044321171939373016\n",
      "Epoch[0], Batch[1970], Train loss: 0.04754406213760376\n",
      "Epoch[0], Val loss: 0.04677674546837807\n",
      "Epoch[0], Batch[1971], Train loss: 0.0469834990799427\n",
      "Epoch[0], Val loss: 0.046081043779850006\n",
      "Epoch[0], Batch[1972], Train loss: 0.04908888041973114\n",
      "Epoch[0], Val loss: 0.04311126843094826\n",
      "Epoch[0], Batch[1973], Train loss: 0.04707842692732811\n",
      "Epoch[0], Val loss: 0.04532849043607712\n",
      "Epoch[0], Batch[1974], Train loss: 0.04829946905374527\n",
      "Epoch[0], Val loss: 0.04922157898545265\n",
      "Epoch[0], Batch[1975], Train loss: 0.04821765050292015\n",
      "Epoch[0], Val loss: 0.04602129012346268\n",
      "Epoch[0], Batch[1976], Train loss: 0.04426191747188568\n",
      "Epoch[0], Val loss: 0.046867985278367996\n",
      "Epoch[0], Batch[1977], Train loss: 0.048097796738147736\n",
      "Epoch[0], Val loss: 0.04758501797914505\n",
      "Epoch[0], Batch[1978], Train loss: 0.04662204161286354\n",
      "Epoch[0], Val loss: 0.04842882230877876\n",
      "Epoch[0], Batch[1979], Train loss: 0.05063720792531967\n",
      "Epoch[0], Val loss: 0.047207191586494446\n",
      "Epoch[0], Batch[1980], Train loss: 0.04722045361995697\n",
      "Epoch[0], Val loss: 0.045023418962955475\n",
      "Epoch[0], Batch[1981], Train loss: 0.04552467539906502\n",
      "Epoch[0], Val loss: 0.04846401512622833\n",
      "Epoch[0], Batch[1982], Train loss: 0.04615619778633118\n",
      "Epoch[0], Val loss: 0.0455925278365612\n",
      "Epoch[0], Batch[1983], Train loss: 0.04679573327302933\n",
      "Epoch[0], Val loss: 0.04484155774116516\n",
      "Epoch[0], Batch[1984], Train loss: 0.046837519854307175\n",
      "Epoch[0], Val loss: 0.041533999145030975\n",
      "Epoch[0], Batch[1985], Train loss: 0.050410594791173935\n",
      "Epoch[0], Val loss: 0.044548433274030685\n",
      "Epoch[0], Batch[1986], Train loss: 0.04927504435181618\n",
      "Epoch[0], Val loss: 0.04659698158502579\n",
      "Epoch[0], Batch[1987], Train loss: 0.048510827124118805\n",
      "Epoch[0], Val loss: 0.044441431760787964\n",
      "Epoch[0], Batch[1988], Train loss: 0.04709579050540924\n",
      "Epoch[0], Val loss: 0.04653267189860344\n",
      "Epoch[0], Batch[1989], Train loss: 0.04664548486471176\n",
      "Epoch[0], Val loss: 0.047756779938936234\n",
      "Epoch[0], Batch[1990], Train loss: 0.04817373305559158\n",
      "Epoch[0], Val loss: 0.04897028207778931\n",
      "Epoch[0], Batch[1991], Train loss: 0.05024892836809158\n",
      "Epoch[0], Val loss: 0.046903081238269806\n",
      "Epoch[0], Batch[1992], Train loss: 0.04623306915163994\n",
      "Epoch[0], Val loss: 0.04449640214443207\n",
      "Epoch[0], Batch[1993], Train loss: 0.045060064643621445\n",
      "Epoch[0], Val loss: 0.045536164194345474\n",
      "Epoch[0], Batch[1994], Train loss: 0.04779605194926262\n",
      "Epoch[0], Val loss: 0.044789206236600876\n",
      "Epoch[0], Batch[1995], Train loss: 0.046905651688575745\n",
      "Epoch[0], Val loss: 0.045807670801877975\n",
      "Epoch[0], Batch[1996], Train loss: 0.048734042793512344\n",
      "Epoch[0], Val loss: 0.04486437514424324\n",
      "Epoch[0], Batch[1997], Train loss: 0.049294840544462204\n",
      "Epoch[0], Val loss: 0.04448487237095833\n",
      "Epoch[0], Batch[1998], Train loss: 0.04746577516198158\n",
      "Epoch[0], Val loss: 0.04286881536245346\n",
      "Epoch[0], Batch[1999], Train loss: 0.0476655475795269\n",
      "Epoch[0], Val loss: 0.04436738044023514\n",
      "Epoch[0], Batch[2000], Train loss: 0.04816456139087677\n",
      "Epoch[0], Val loss: 0.04495406150817871\n",
      "Epoch[0], Batch[2001], Train loss: 0.0473802275955677\n",
      "Epoch[0], Val loss: 0.047941748052835464\n",
      "Epoch[0], Batch[2002], Train loss: 0.04725891724228859\n",
      "Epoch[0], Val loss: 0.04416566342115402\n",
      "Epoch[0], Batch[2003], Train loss: 0.04719530791044235\n",
      "Epoch[0], Val loss: 0.04694553464651108\n",
      "Epoch[0], Batch[2004], Train loss: 0.04878619313240051\n",
      "Epoch[0], Val loss: 0.04332541301846504\n",
      "Epoch[0], Batch[2005], Train loss: 0.05018801987171173\n",
      "Epoch[0], Val loss: 0.04435957968235016\n",
      "Epoch[0], Batch[2006], Train loss: 0.047607846558094025\n",
      "Epoch[0], Val loss: 0.04363163560628891\n",
      "Epoch[0], Batch[2007], Train loss: 0.04874496906995773\n",
      "Epoch[0], Val loss: 0.04596887528896332\n",
      "Epoch[0], Batch[2008], Train loss: 0.04617290571331978\n",
      "Epoch[0], Val loss: 0.04437430575489998\n",
      "Epoch[0], Batch[2009], Train loss: 0.045465730130672455\n",
      "Epoch[0], Val loss: 0.044494614005088806\n",
      "Epoch[0], Batch[2010], Train loss: 0.0474102757871151\n",
      "Epoch[0], Val loss: 0.043229714035987854\n",
      "Epoch[0], Batch[2011], Train loss: 0.05055256932973862\n",
      "Epoch[0], Val loss: 0.04578405246138573\n",
      "Epoch[0], Batch[2012], Train loss: 0.04953356832265854\n",
      "Epoch[0], Val loss: 0.04558967426419258\n",
      "Epoch[0], Batch[2013], Train loss: 0.048489585518836975\n",
      "Epoch[0], Val loss: 0.04422464966773987\n",
      "Epoch[0], Batch[2014], Train loss: 0.04510810226202011\n",
      "Epoch[0], Val loss: 0.04542849212884903\n",
      "Epoch[0], Batch[2015], Train loss: 0.04390957951545715\n",
      "Epoch[0], Val loss: 0.043499380350112915\n",
      "Epoch[0], Batch[2016], Train loss: 0.04801121726632118\n",
      "Epoch[0], Val loss: 0.042914532124996185\n",
      "Epoch[0], Batch[2017], Train loss: 0.0457601323723793\n",
      "Epoch[0], Val loss: 0.04625207558274269\n",
      "Epoch[0], Batch[2018], Train loss: 0.047265857458114624\n",
      "Epoch[0], Val loss: 0.04240907356142998\n",
      "Epoch[0], Batch[2019], Train loss: 0.04472746327519417\n",
      "Epoch[0], Val loss: 0.04469573125243187\n",
      "Epoch[0], Batch[2020], Train loss: 0.04784490913152695\n",
      "Epoch[0], Val loss: 0.04369710758328438\n",
      "Epoch[0], Batch[2021], Train loss: 0.04697190970182419\n",
      "Epoch[0], Val loss: 0.046919599175453186\n",
      "Epoch[0], Batch[2022], Train loss: 0.04630891978740692\n",
      "Epoch[0], Val loss: 0.04568268358707428\n",
      "Epoch[0], Batch[2023], Train loss: 0.046597424894571304\n",
      "Epoch[0], Val loss: 0.04473526403307915\n",
      "Epoch[0], Batch[2024], Train loss: 0.04825689271092415\n",
      "Epoch[0], Val loss: 0.042009200900793076\n",
      "Epoch[0], Batch[2025], Train loss: 0.047034647315740585\n",
      "Epoch[0], Val loss: 0.04354218766093254\n",
      "Epoch[0], Batch[2026], Train loss: 0.04413725435733795\n",
      "Epoch[0], Val loss: 0.045666392892599106\n",
      "Epoch[0], Batch[2027], Train loss: 0.04867232218384743\n",
      "Epoch[0], Val loss: 0.04509759694337845\n",
      "Epoch[0], Batch[2028], Train loss: 0.04579443857073784\n",
      "Epoch[0], Val loss: 0.04264068603515625\n",
      "Epoch[0], Batch[2029], Train loss: 0.04759828373789787\n",
      "Epoch[0], Val loss: 0.0484248548746109\n",
      "Epoch[0], Batch[2030], Train loss: 0.0510648749768734\n",
      "Epoch[0], Val loss: 0.04481340944766998\n",
      "Epoch[0], Batch[2031], Train loss: 0.048353176563978195\n",
      "Epoch[0], Val loss: 0.04515892639756203\n",
      "Epoch[0], Batch[2032], Train loss: 0.04459599778056145\n",
      "Epoch[0], Val loss: 0.0439932607114315\n",
      "Epoch[0], Batch[2033], Train loss: 0.04827462509274483\n",
      "Epoch[0], Val loss: 0.04300608113408089\n",
      "Epoch[0], Batch[2034], Train loss: 0.04543735831975937\n",
      "Epoch[0], Val loss: 0.0444621704518795\n",
      "Epoch[0], Batch[2035], Train loss: 0.05031299963593483\n",
      "Epoch[0], Val loss: 0.0443427748978138\n",
      "Epoch[0], Batch[2036], Train loss: 0.048310086131095886\n",
      "Epoch[0], Val loss: 0.04413769766688347\n",
      "Epoch[0], Batch[2037], Train loss: 0.04548686742782593\n",
      "Epoch[0], Val loss: 0.04650893434882164\n",
      "Epoch[0], Batch[2038], Train loss: 0.046975426375865936\n",
      "Epoch[0], Val loss: 0.04402460157871246\n",
      "Epoch[0], Batch[2039], Train loss: 0.04671752452850342\n",
      "Epoch[0], Val loss: 0.044599633663892746\n",
      "Epoch[0], Batch[2040], Train loss: 0.04956407845020294\n",
      "Epoch[0], Val loss: 0.044346634298563004\n",
      "Epoch[0], Batch[2041], Train loss: 0.04689992219209671\n",
      "Epoch[0], Val loss: 0.04385995492339134\n",
      "Epoch[0], Batch[2042], Train loss: 0.04536282643675804\n",
      "Epoch[0], Val loss: 0.0448915958404541\n",
      "Epoch[0], Batch[2043], Train loss: 0.04911072924733162\n",
      "Epoch[0], Val loss: 0.04595734179019928\n",
      "Epoch[0], Batch[2044], Train loss: 0.047383230179548264\n",
      "Epoch[0], Val loss: 0.04508553072810173\n",
      "Epoch[0], Batch[2045], Train loss: 0.04575454443693161\n",
      "Epoch[0], Val loss: 0.044940222054719925\n",
      "Epoch[0], Batch[2046], Train loss: 0.04783215373754501\n",
      "Epoch[0], Val loss: 0.04346785694360733\n",
      "Epoch[0], Batch[2047], Train loss: 0.04784110188484192\n",
      "Epoch[0], Val loss: 0.043077871203422546\n",
      "Epoch[0], Batch[2048], Train loss: 0.0466252826154232\n",
      "Epoch[0], Val loss: 0.04412286728620529\n",
      "Epoch[0], Batch[2049], Train loss: 0.0437614880502224\n",
      "Epoch[0], Val loss: 0.04193462058901787\n",
      "Epoch[0], Batch[2050], Train loss: 0.04709523916244507\n",
      "Epoch[0], Val loss: 0.04698555916547775\n",
      "Epoch[0], Batch[2051], Train loss: 0.043989427387714386\n",
      "Epoch[0], Val loss: 0.04403876140713692\n",
      "Epoch[0], Batch[2052], Train loss: 0.04438444972038269\n",
      "Epoch[0], Val loss: 0.04534740000963211\n",
      "Epoch[0], Batch[2053], Train loss: 0.05044260621070862\n",
      "Epoch[0], Val loss: 0.0445035845041275\n",
      "Epoch[0], Batch[2054], Train loss: 0.05008706450462341\n",
      "Epoch[0], Val loss: 0.041654136031866074\n",
      "Epoch[0], Batch[2055], Train loss: 0.0472089946269989\n",
      "Epoch[0], Val loss: 0.0425540953874588\n",
      "Epoch[0], Batch[2056], Train loss: 0.04656780883669853\n",
      "Epoch[0], Val loss: 0.04515930637717247\n",
      "Epoch[0], Batch[2057], Train loss: 0.04652734100818634\n",
      "Epoch[0], Val loss: 0.043856699019670486\n",
      "Epoch[0], Batch[2058], Train loss: 0.04258642718195915\n",
      "Epoch[0], Val loss: 0.04461384192109108\n",
      "Epoch[0], Batch[2059], Train loss: 0.0457095131278038\n",
      "Epoch[0], Val loss: 0.04526704177260399\n",
      "Epoch[0], Batch[2060], Train loss: 0.049850400537252426\n",
      "Epoch[0], Val loss: 0.0439748577773571\n",
      "Epoch[0], Batch[2061], Train loss: 0.046403829008340836\n",
      "Epoch[0], Val loss: 0.04536271095275879\n",
      "Epoch[0], Batch[2062], Train loss: 0.050807371735572815\n",
      "Epoch[0], Val loss: 0.04684830456972122\n",
      "Epoch[0], Batch[2063], Train loss: 0.05240451544523239\n",
      "Epoch[0], Val loss: 0.0439586378633976\n",
      "Epoch[0], Batch[2064], Train loss: 0.045430462807416916\n",
      "Epoch[0], Val loss: 0.043766088783741\n",
      "Epoch[0], Batch[2065], Train loss: 0.04512270167469978\n",
      "Epoch[0], Val loss: 0.04295091703534126\n",
      "Epoch[0], Batch[2066], Train loss: 0.05034225806593895\n",
      "Epoch[0], Val loss: 0.042753543704748154\n",
      "Epoch[0], Batch[2067], Train loss: 0.04597753658890724\n",
      "Epoch[0], Val loss: 0.043133508414030075\n",
      "Epoch[0], Batch[2068], Train loss: 0.04895416647195816\n",
      "Epoch[0], Val loss: 0.042340874671936035\n",
      "Epoch[0], Batch[2069], Train loss: 0.0483403243124485\n",
      "Epoch[0], Val loss: 0.04400129243731499\n",
      "Epoch[0], Batch[2070], Train loss: 0.04751987382769585\n",
      "Epoch[0], Val loss: 0.04408475384116173\n",
      "Epoch[0], Batch[2071], Train loss: 0.04652620106935501\n",
      "Epoch[0], Val loss: 0.04398166015744209\n",
      "Epoch[0], Batch[2072], Train loss: 0.043347395956516266\n",
      "Epoch[0], Val loss: 0.04328545928001404\n",
      "Epoch[0], Batch[2073], Train loss: 0.04458441212773323\n",
      "Epoch[0], Val loss: 0.045160893350839615\n",
      "Epoch[0], Batch[2074], Train loss: 0.04626719653606415\n",
      "Epoch[0], Val loss: 0.04288606345653534\n",
      "Epoch[0], Batch[2075], Train loss: 0.04555660858750343\n",
      "Epoch[0], Val loss: 0.04353569820523262\n",
      "Epoch[0], Batch[2076], Train loss: 0.045315586030483246\n",
      "Epoch[0], Val loss: 0.044003888964653015\n",
      "Epoch[0], Batch[2077], Train loss: 0.04552554339170456\n",
      "Epoch[0], Val loss: 0.04775078594684601\n",
      "Epoch[0], Batch[2078], Train loss: 0.04451236128807068\n",
      "Epoch[0], Val loss: 0.04530806094408035\n",
      "Epoch[0], Batch[2079], Train loss: 0.04377598688006401\n",
      "Epoch[0], Val loss: 0.04211938753724098\n",
      "Epoch[0], Batch[2080], Train loss: 0.04826956242322922\n",
      "Epoch[0], Val loss: 0.04775267839431763\n",
      "Epoch[0], Batch[2081], Train loss: 0.04515177384018898\n",
      "Epoch[0], Val loss: 0.04553072527050972\n",
      "Epoch[0], Batch[2082], Train loss: 0.048465169966220856\n",
      "Epoch[0], Val loss: 0.043763045221567154\n",
      "Epoch[0], Batch[2083], Train loss: 0.047981902956962585\n",
      "Epoch[0], Val loss: 0.04367024451494217\n",
      "Epoch[0], Batch[2084], Train loss: 0.04482248052954674\n",
      "Epoch[0], Val loss: 0.04872206225991249\n",
      "Epoch[0], Batch[2085], Train loss: 0.04628301411867142\n",
      "Epoch[0], Val loss: 0.04362267255783081\n",
      "Epoch[0], Batch[2086], Train loss: 0.04879050701856613\n",
      "Epoch[0], Val loss: 0.044817935675382614\n",
      "Epoch[0], Batch[2087], Train loss: 0.04615087807178497\n",
      "Epoch[0], Val loss: 0.04407140985131264\n",
      "Epoch[0], Batch[2088], Train loss: 0.049210987985134125\n",
      "Epoch[0], Val loss: 0.04532371088862419\n",
      "Epoch[0], Batch[2089], Train loss: 0.04783625900745392\n",
      "Epoch[0], Val loss: 0.04265563562512398\n",
      "Epoch[0], Batch[2090], Train loss: 0.047096773982048035\n",
      "Epoch[0], Val loss: 0.0436384491622448\n",
      "Epoch[0], Batch[2091], Train loss: 0.04560454934835434\n",
      "Epoch[0], Val loss: 0.04376696050167084\n",
      "Epoch[0], Batch[2092], Train loss: 0.04907494783401489\n",
      "Epoch[0], Val loss: 0.045400138944387436\n",
      "Epoch[0], Batch[2093], Train loss: 0.046366602182388306\n",
      "Epoch[0], Val loss: 0.042994752526283264\n",
      "Epoch[0], Batch[2094], Train loss: 0.050378523766994476\n",
      "Epoch[0], Val loss: 0.042498596012592316\n",
      "Epoch[0], Batch[2095], Train loss: 0.044479914009571075\n",
      "Epoch[0], Val loss: 0.04694170504808426\n",
      "Epoch[0], Batch[2096], Train loss: 0.047945499420166016\n",
      "Epoch[0], Val loss: 0.04402051493525505\n",
      "Epoch[0], Batch[2097], Train loss: 0.04795444384217262\n",
      "Epoch[0], Val loss: 0.04338182508945465\n",
      "Epoch[0], Batch[2098], Train loss: 0.048692598938941956\n",
      "Epoch[0], Val loss: 0.04529144987463951\n",
      "Epoch[0], Batch[2099], Train loss: 0.048647161573171616\n",
      "Epoch[0], Val loss: 0.04557323083281517\n",
      "Epoch[0], Batch[2100], Train loss: 0.046432916074991226\n",
      "Epoch[0], Val loss: 0.04390701279044151\n",
      "Epoch[0], Batch[2101], Train loss: 0.04401639476418495\n",
      "Epoch[0], Val loss: 0.04317531734704971\n",
      "Epoch[0], Batch[2102], Train loss: 0.04775002971291542\n",
      "Epoch[0], Val loss: 0.045919254422187805\n",
      "Epoch[0], Batch[2103], Train loss: 0.04445932060480118\n",
      "Epoch[0], Val loss: 0.04465777426958084\n",
      "Epoch[0], Batch[2104], Train loss: 0.04427589476108551\n",
      "Epoch[0], Val loss: 0.044892869889736176\n",
      "Epoch[0], Batch[2105], Train loss: 0.04591663181781769\n",
      "Epoch[0], Val loss: 0.04245961084961891\n",
      "Epoch[0], Batch[2106], Train loss: 0.04851849004626274\n",
      "Epoch[0], Val loss: 0.04359433799982071\n",
      "Epoch[0], Batch[2107], Train loss: 0.04513808712363243\n",
      "Epoch[0], Val loss: 0.045655783265829086\n",
      "Epoch[0], Batch[2108], Train loss: 0.0457690954208374\n",
      "Epoch[0], Val loss: 0.04019802808761597\n",
      "Epoch[0], Batch[2109], Train loss: 0.045746784657239914\n",
      "Epoch[0], Val loss: 0.043744366616010666\n",
      "Epoch[0], Batch[2110], Train loss: 0.0441058948636055\n",
      "Epoch[0], Val loss: 0.04679299145936966\n",
      "Epoch[0], Batch[2111], Train loss: 0.048291414976119995\n",
      "Epoch[0], Val loss: 0.04357578977942467\n",
      "Epoch[0], Batch[2112], Train loss: 0.04950493201613426\n",
      "Epoch[0], Val loss: 0.04338601604104042\n",
      "Epoch[0], Batch[2113], Train loss: 0.045715272426605225\n",
      "Epoch[0], Val loss: 0.04707909747958183\n",
      "Epoch[0], Batch[2114], Train loss: 0.04310799762606621\n",
      "Epoch[0], Val loss: 0.04474184662103653\n",
      "Epoch[0], Batch[2115], Train loss: 0.04723113402724266\n",
      "Epoch[0], Val loss: 0.043435096740722656\n",
      "Epoch[0], Batch[2116], Train loss: 0.04792816564440727\n",
      "Epoch[0], Val loss: 0.042517635971307755\n",
      "Epoch[0], Batch[2117], Train loss: 0.04619569331407547\n",
      "Epoch[0], Val loss: 0.045781202614307404\n",
      "Epoch[0], Batch[2118], Train loss: 0.04483822360634804\n",
      "Epoch[0], Val loss: 0.0439518541097641\n",
      "Epoch[0], Batch[2119], Train loss: 0.04557473957538605\n",
      "Epoch[0], Val loss: 0.04682253301143646\n",
      "Epoch[0], Batch[2120], Train loss: 0.0454147569835186\n",
      "Epoch[0], Val loss: 0.046247243881225586\n",
      "Epoch[0], Batch[2121], Train loss: 0.04674685746431351\n",
      "Epoch[0], Val loss: 0.044470880180597305\n",
      "Epoch[0], Batch[2122], Train loss: 0.04438409581780434\n",
      "Epoch[0], Val loss: 0.042353712022304535\n",
      "Epoch[0], Batch[2123], Train loss: 0.046434689313173294\n",
      "Epoch[0], Val loss: 0.041450727730989456\n",
      "Epoch[0], Batch[2124], Train loss: 0.04528670385479927\n",
      "Epoch[0], Val loss: 0.04547342285513878\n",
      "Epoch[0], Batch[2125], Train loss: 0.042898304760456085\n",
      "Epoch[0], Val loss: 0.045202720910310745\n",
      "Epoch[0], Batch[2126], Train loss: 0.04549719765782356\n",
      "Epoch[0], Val loss: 0.04371356591582298\n",
      "Epoch[0], Batch[2127], Train loss: 0.043831728398799896\n",
      "Epoch[0], Val loss: 0.045260727405548096\n",
      "Epoch[0], Batch[2128], Train loss: 0.04852062091231346\n",
      "Epoch[0], Val loss: 0.04207409545779228\n",
      "Epoch[0], Batch[2129], Train loss: 0.04833165183663368\n",
      "Epoch[0], Val loss: 0.04422764852643013\n",
      "Epoch[0], Batch[2130], Train loss: 0.04836837202310562\n",
      "Epoch[0], Val loss: 0.045463256537914276\n",
      "Epoch[0], Batch[2131], Train loss: 0.0443149134516716\n",
      "Epoch[0], Val loss: 0.045739490538835526\n",
      "Epoch[0], Batch[2132], Train loss: 0.04868914559483528\n",
      "Epoch[0], Val loss: 0.045354798436164856\n",
      "Epoch[0], Batch[2133], Train loss: 0.04559551551938057\n",
      "Epoch[0], Val loss: 0.043164920061826706\n",
      "Epoch[0], Batch[2134], Train loss: 0.04626482352614403\n",
      "Epoch[0], Val loss: 0.04418272525072098\n",
      "Epoch[0], Batch[2135], Train loss: 0.04738706350326538\n",
      "Epoch[0], Val loss: 0.04485291987657547\n",
      "Epoch[0], Batch[2136], Train loss: 0.047826044261455536\n",
      "Epoch[0], Val loss: 0.04239301756024361\n",
      "Epoch[0], Batch[2137], Train loss: 0.04750268906354904\n",
      "Epoch[0], Val loss: 0.04137410223484039\n",
      "Epoch[0], Batch[2138], Train loss: 0.047057557851076126\n",
      "Epoch[0], Val loss: 0.04303565248847008\n",
      "Epoch[0], Batch[2139], Train loss: 0.044828709214925766\n",
      "Epoch[0], Val loss: 0.042461834847927094\n",
      "Epoch[0], Batch[2140], Train loss: 0.047220610082149506\n",
      "Epoch[0], Val loss: 0.044329527765512466\n",
      "Epoch[0], Batch[2141], Train loss: 0.04602491855621338\n",
      "Epoch[0], Val loss: 0.03978019207715988\n",
      "Epoch[0], Batch[2142], Train loss: 0.0503045991063118\n",
      "Epoch[0], Val loss: 0.04351413995027542\n",
      "Epoch[0], Batch[2143], Train loss: 0.04436010494828224\n",
      "Epoch[0], Val loss: 0.044168125838041306\n",
      "Epoch[0], Batch[2144], Train loss: 0.042699214071035385\n",
      "Epoch[0], Val loss: 0.04303615540266037\n",
      "Epoch[0], Batch[2145], Train loss: 0.04408028721809387\n",
      "Epoch[0], Val loss: 0.04222864285111427\n",
      "Epoch[0], Batch[2146], Train loss: 0.04686468839645386\n",
      "Epoch[0], Val loss: 0.04411991685628891\n",
      "Epoch[0], Batch[2147], Train loss: 0.045938678085803986\n",
      "Epoch[0], Val loss: 0.04403562843799591\n",
      "Epoch[0], Batch[2148], Train loss: 0.04534595459699631\n",
      "Epoch[0], Val loss: 0.04158252477645874\n",
      "Epoch[0], Batch[2149], Train loss: 0.046041179448366165\n",
      "Epoch[0], Val loss: 0.042294178158044815\n",
      "Epoch[0], Batch[2150], Train loss: 0.04681197553873062\n",
      "Epoch[0], Val loss: 0.04251502826809883\n",
      "Epoch[0], Batch[2151], Train loss: 0.046380359679460526\n",
      "Epoch[0], Val loss: 0.04764656350016594\n",
      "Epoch[0], Batch[2152], Train loss: 0.04642157629132271\n",
      "Epoch[0], Val loss: 0.04657723754644394\n",
      "Epoch[0], Batch[2153], Train loss: 0.04496179521083832\n",
      "Epoch[0], Val loss: 0.044215425848960876\n",
      "Epoch[0], Batch[2154], Train loss: 0.0454043447971344\n",
      "Epoch[0], Val loss: 0.0426814891397953\n",
      "Epoch[0], Batch[2155], Train loss: 0.04435401409864426\n",
      "Epoch[0], Val loss: 0.040941931307315826\n",
      "Epoch[0], Batch[2156], Train loss: 0.04494868218898773\n",
      "Epoch[0], Val loss: 0.04156223312020302\n",
      "Epoch[0], Batch[2157], Train loss: 0.04641397297382355\n",
      "Epoch[0], Val loss: 0.04213866963982582\n",
      "Epoch[0], Batch[2158], Train loss: 0.0464787520468235\n",
      "Epoch[0], Val loss: 0.04028552398085594\n",
      "Epoch[0], Batch[2159], Train loss: 0.04340679198503494\n",
      "Epoch[0], Val loss: 0.043586939573287964\n",
      "Epoch[0], Batch[2160], Train loss: 0.044397156685590744\n",
      "Epoch[0], Val loss: 0.04421720281243324\n",
      "Epoch[0], Batch[2161], Train loss: 0.04670710861682892\n",
      "Epoch[0], Val loss: 0.04212018847465515\n",
      "Epoch[0], Batch[2162], Train loss: 0.04441281780600548\n",
      "Epoch[0], Val loss: 0.04453704133629799\n",
      "Epoch[0], Batch[2163], Train loss: 0.04670896753668785\n",
      "Epoch[0], Val loss: 0.04231812804937363\n",
      "Epoch[0], Batch[2164], Train loss: 0.04876407980918884\n",
      "Epoch[0], Val loss: 0.043440427631139755\n",
      "Epoch[0], Batch[2165], Train loss: 0.04470588639378548\n",
      "Epoch[0], Val loss: 0.043382685631513596\n",
      "Epoch[0], Batch[2166], Train loss: 0.04466988518834114\n",
      "Epoch[0], Val loss: 0.04213842749595642\n",
      "Epoch[0], Batch[2167], Train loss: 0.04397333040833473\n",
      "Epoch[0], Val loss: 0.04509782791137695\n",
      "Epoch[0], Batch[2168], Train loss: 0.044764239341020584\n",
      "Epoch[0], Val loss: 0.04189296439290047\n",
      "Epoch[0], Batch[2169], Train loss: 0.04649391770362854\n",
      "Epoch[0], Val loss: 0.04529573768377304\n",
      "Epoch[0], Batch[2170], Train loss: 0.045526910573244095\n",
      "Epoch[0], Val loss: 0.0456668920814991\n",
      "Epoch[0], Batch[2171], Train loss: 0.047211531549692154\n",
      "Epoch[0], Val loss: 0.04356354847550392\n",
      "Epoch[0], Batch[2172], Train loss: 0.045036956667900085\n",
      "Epoch[0], Val loss: 0.042409107089042664\n",
      "Epoch[0], Batch[2173], Train loss: 0.04876638948917389\n",
      "Epoch[0], Val loss: 0.04473036900162697\n",
      "Epoch[0], Batch[2174], Train loss: 0.04586761072278023\n",
      "Epoch[0], Val loss: 0.04416191205382347\n",
      "Epoch[0], Batch[2175], Train loss: 0.045100271701812744\n",
      "Epoch[0], Val loss: 0.04492461308836937\n",
      "Epoch[0], Batch[2176], Train loss: 0.04506811127066612\n",
      "Epoch[0], Val loss: 0.04213513061404228\n",
      "Epoch[0], Batch[2177], Train loss: 0.048961423337459564\n",
      "Epoch[0], Val loss: 0.04433286935091019\n",
      "Epoch[0], Batch[2178], Train loss: 0.0474245548248291\n",
      "Epoch[0], Val loss: 0.046135906130075455\n",
      "Epoch[0], Batch[2179], Train loss: 0.04482325538992882\n",
      "Epoch[0], Val loss: 0.04279148951172829\n",
      "Epoch[0], Batch[2180], Train loss: 0.04815877228975296\n",
      "Epoch[0], Val loss: 0.041121166199445724\n",
      "Epoch[0], Batch[2181], Train loss: 0.04762459173798561\n",
      "Epoch[0], Val loss: 0.044704265892505646\n",
      "Epoch[0], Batch[2182], Train loss: 0.043597038835287094\n",
      "Epoch[0], Val loss: 0.04462910071015358\n",
      "Epoch[0], Batch[2183], Train loss: 0.042495038360357285\n",
      "Epoch[0], Val loss: 0.04663670063018799\n",
      "Epoch[0], Batch[2184], Train loss: 0.04564039036631584\n",
      "Epoch[0], Val loss: 0.044583722949028015\n",
      "Epoch[0], Batch[2185], Train loss: 0.04743851348757744\n",
      "Epoch[0], Val loss: 0.04515591263771057\n",
      "Epoch[0], Batch[2186], Train loss: 0.045212358236312866\n",
      "Epoch[0], Val loss: 0.041883260011672974\n",
      "Epoch[0], Batch[2187], Train loss: 0.04712479189038277\n",
      "Epoch[0], Val loss: 0.044004254043102264\n",
      "Epoch[0], Batch[2188], Train loss: 0.04432610422372818\n",
      "Epoch[0], Val loss: 0.041309669613838196\n",
      "Epoch[0], Batch[2189], Train loss: 0.04567180201411247\n",
      "Epoch[0], Val loss: 0.041925109922885895\n",
      "Epoch[0], Batch[2190], Train loss: 0.04661143571138382\n",
      "Epoch[0], Val loss: 0.046458568423986435\n",
      "Epoch[0], Batch[2191], Train loss: 0.045256614685058594\n",
      "Epoch[0], Val loss: 0.040088653564453125\n",
      "Epoch[0], Batch[2192], Train loss: 0.04580383375287056\n",
      "Epoch[0], Val loss: 0.043017711490392685\n",
      "Epoch[0], Batch[2193], Train loss: 0.04361309856176376\n",
      "Epoch[0], Val loss: 0.04145226255059242\n",
      "Epoch[0], Batch[2194], Train loss: 0.04713089391589165\n",
      "Epoch[0], Val loss: 0.043581292033195496\n",
      "Epoch[0], Batch[2195], Train loss: 0.04330551251769066\n",
      "Epoch[0], Val loss: 0.04257332161068916\n",
      "Epoch[0], Batch[2196], Train loss: 0.041725918650627136\n",
      "Epoch[0], Val loss: 0.045902855694293976\n",
      "Epoch[0], Batch[2197], Train loss: 0.045486584305763245\n",
      "Epoch[0], Val loss: 0.043732400983572006\n",
      "Epoch[0], Batch[2198], Train loss: 0.04740263149142265\n",
      "Epoch[0], Val loss: 0.04353347420692444\n",
      "Epoch[0], Batch[2199], Train loss: 0.04537540674209595\n",
      "Epoch[0], Val loss: 0.04300897940993309\n",
      "Epoch[0], Batch[2200], Train loss: 0.045906804502010345\n",
      "Epoch[0], Val loss: 0.04309378191828728\n",
      "Epoch[0], Batch[2201], Train loss: 0.046686116605997086\n",
      "Epoch[0], Val loss: 0.04630142077803612\n",
      "Epoch[0], Batch[2202], Train loss: 0.0442856028676033\n",
      "Epoch[0], Val loss: 0.04423912242054939\n",
      "Epoch[0], Batch[2203], Train loss: 0.04394272714853287\n",
      "Epoch[0], Val loss: 0.04671970754861832\n",
      "Epoch[0], Batch[2204], Train loss: 0.04664452001452446\n",
      "Epoch[0], Val loss: 0.04410508647561073\n",
      "Epoch[0], Batch[2205], Train loss: 0.0472642257809639\n",
      "Epoch[0], Val loss: 0.043827448040246964\n",
      "Epoch[0], Batch[2206], Train loss: 0.049717050045728683\n",
      "Epoch[0], Val loss: 0.04221256822347641\n",
      "Epoch[0], Batch[2207], Train loss: 0.04496767744421959\n",
      "Epoch[0], Val loss: 0.04195576161146164\n",
      "Epoch[0], Batch[2208], Train loss: 0.04566316306591034\n",
      "Epoch[0], Val loss: 0.04054095596075058\n",
      "Epoch[0], Batch[2209], Train loss: 0.04755927622318268\n",
      "Epoch[0], Val loss: 0.04404512792825699\n",
      "Epoch[0], Batch[2210], Train loss: 0.04361635819077492\n",
      "Epoch[0], Val loss: 0.04500580206513405\n",
      "Epoch[0], Batch[2211], Train loss: 0.04585140198469162\n",
      "Epoch[0], Val loss: 0.04320654273033142\n",
      "Epoch[0], Batch[2212], Train loss: 0.044758014380931854\n",
      "Epoch[0], Val loss: 0.042556751519441605\n",
      "Epoch[0], Batch[2213], Train loss: 0.04741214960813522\n",
      "Epoch[0], Val loss: 0.0451434887945652\n",
      "Epoch[0], Batch[2214], Train loss: 0.042861439287662506\n",
      "Epoch[0], Val loss: 0.0424073152244091\n",
      "Epoch[0], Batch[2215], Train loss: 0.04500548914074898\n",
      "Epoch[0], Val loss: 0.04185061156749725\n",
      "Epoch[0], Batch[2216], Train loss: 0.04383162781596184\n",
      "Epoch[0], Val loss: 0.04086675867438316\n",
      "Epoch[0], Batch[2217], Train loss: 0.043808113783597946\n",
      "Epoch[0], Val loss: 0.04082128033041954\n",
      "Epoch[0], Batch[2218], Train loss: 0.046279165893793106\n",
      "Epoch[0], Val loss: 0.04074489325284958\n",
      "Epoch[0], Batch[2219], Train loss: 0.044582098722457886\n",
      "Epoch[0], Val loss: 0.04336150363087654\n",
      "Epoch[0], Batch[2220], Train loss: 0.05012154579162598\n",
      "Epoch[0], Val loss: 0.0410289540886879\n",
      "Epoch[0], Batch[2221], Train loss: 0.044118065387010574\n",
      "Epoch[0], Val loss: 0.043646618723869324\n",
      "Epoch[0], Batch[2222], Train loss: 0.0464443676173687\n",
      "Epoch[0], Val loss: 0.044206731021404266\n",
      "Epoch[0], Batch[2223], Train loss: 0.04527280479669571\n",
      "Epoch[0], Val loss: 0.041952572762966156\n",
      "Epoch[0], Batch[2224], Train loss: 0.04460177943110466\n",
      "Epoch[0], Val loss: 0.041932035237550735\n",
      "Epoch[0], Batch[2225], Train loss: 0.04433475807309151\n",
      "Epoch[0], Val loss: 0.04407181963324547\n",
      "Epoch[0], Batch[2226], Train loss: 0.043628666549921036\n",
      "Epoch[0], Val loss: 0.04194004833698273\n",
      "Epoch[0], Batch[2227], Train loss: 0.04254918172955513\n",
      "Epoch[0], Val loss: 0.04350033029913902\n",
      "Epoch[0], Batch[2228], Train loss: 0.04425911232829094\n",
      "Epoch[0], Val loss: 0.04549604281783104\n",
      "Epoch[0], Batch[2229], Train loss: 0.045207679271698\n",
      "Epoch[0], Val loss: 0.04434219002723694\n",
      "Epoch[0], Batch[2230], Train loss: 0.045090265572071075\n",
      "Epoch[0], Val loss: 0.04383595660328865\n",
      "Epoch[0], Batch[2231], Train loss: 0.043716661632061005\n",
      "Epoch[0], Val loss: 0.04504737630486488\n",
      "Epoch[0], Batch[2232], Train loss: 0.046774476766586304\n",
      "Epoch[0], Val loss: 0.03983268886804581\n",
      "Epoch[0], Batch[2233], Train loss: 0.04359782114624977\n",
      "Epoch[0], Val loss: 0.04351908713579178\n",
      "Epoch[0], Batch[2234], Train loss: 0.04712129011750221\n",
      "Epoch[0], Val loss: 0.04142531752586365\n",
      "Epoch[0], Batch[2235], Train loss: 0.044091902673244476\n",
      "Epoch[0], Val loss: 0.043632689863443375\n",
      "Epoch[0], Batch[2236], Train loss: 0.0440983884036541\n",
      "Epoch[0], Val loss: 0.04571972414851189\n",
      "Epoch[0], Batch[2237], Train loss: 0.04207753390073776\n",
      "Epoch[0], Val loss: 0.044304508715867996\n",
      "Epoch[0], Batch[2238], Train loss: 0.04274757578969002\n",
      "Epoch[0], Val loss: 0.04360448569059372\n",
      "Epoch[0], Batch[2239], Train loss: 0.04711253568530083\n",
      "Epoch[0], Val loss: 0.0423312671482563\n",
      "Epoch[0], Batch[2240], Train loss: 0.04758509248495102\n",
      "Epoch[0], Val loss: 0.045307230204343796\n",
      "Epoch[0], Batch[2241], Train loss: 0.045900944620370865\n",
      "Epoch[0], Val loss: 0.04242447763681412\n",
      "Epoch[0], Batch[2242], Train loss: 0.04527440294623375\n",
      "Epoch[0], Val loss: 0.0417795404791832\n",
      "Epoch[0], Batch[2243], Train loss: 0.04309427738189697\n",
      "Epoch[0], Val loss: 0.04485688358545303\n",
      "Epoch[0], Batch[2244], Train loss: 0.0484655499458313\n",
      "Epoch[0], Val loss: 0.041664741933345795\n",
      "Epoch[0], Batch[2245], Train loss: 0.04359959810972214\n",
      "Epoch[0], Val loss: 0.044352397322654724\n",
      "Epoch[0], Batch[2246], Train loss: 0.04361369460821152\n",
      "Epoch[0], Val loss: 0.04390987753868103\n",
      "Epoch[0], Batch[2247], Train loss: 0.043365608900785446\n",
      "Epoch[0], Val loss: 0.04219793900847435\n",
      "Epoch[0], Batch[2248], Train loss: 0.043677251785993576\n",
      "Epoch[0], Val loss: 0.043543197214603424\n",
      "Epoch[0], Batch[2249], Train loss: 0.046764079481363297\n",
      "Epoch[0], Val loss: 0.04450831562280655\n",
      "Epoch[0], Batch[2250], Train loss: 0.04573957994580269\n",
      "Epoch[0], Val loss: 0.04612773656845093\n",
      "Epoch[0], Batch[2251], Train loss: 0.048121120780706406\n",
      "Epoch[0], Val loss: 0.04464137181639671\n",
      "Epoch[0], Batch[2252], Train loss: 0.046207673847675323\n",
      "Epoch[0], Val loss: 0.041287291795015335\n",
      "Epoch[0], Batch[2253], Train loss: 0.04526973515748978\n",
      "Epoch[0], Val loss: 0.042854830622673035\n",
      "Epoch[0], Batch[2254], Train loss: 0.045782219618558884\n",
      "Epoch[0], Val loss: 0.04207077994942665\n",
      "Epoch[0], Batch[2255], Train loss: 0.044824834913015366\n",
      "Epoch[0], Val loss: 0.04422902315855026\n",
      "Epoch[0], Batch[2256], Train loss: 0.04469114914536476\n",
      "Epoch[0], Val loss: 0.04131342098116875\n",
      "Epoch[0], Batch[2257], Train loss: 0.04523110389709473\n",
      "Epoch[0], Val loss: 0.042086802423000336\n",
      "Epoch[0], Batch[2258], Train loss: 0.04275782033801079\n",
      "Epoch[0], Val loss: 0.041653111577034\n",
      "Epoch[0], Batch[2259], Train loss: 0.049680691212415695\n",
      "Epoch[0], Val loss: 0.042590100318193436\n",
      "Epoch[0], Batch[2260], Train loss: 0.04502196982502937\n",
      "Epoch[0], Val loss: 0.04402553662657738\n",
      "Epoch[0], Batch[2261], Train loss: 0.04331142455339432\n",
      "Epoch[0], Val loss: 0.04598449170589447\n",
      "Epoch[0], Batch[2262], Train loss: 0.043183982372283936\n",
      "Epoch[0], Val loss: 0.04206172749400139\n",
      "Epoch[0], Batch[2263], Train loss: 0.04425681009888649\n",
      "Epoch[0], Val loss: 0.04337000846862793\n",
      "Epoch[0], Batch[2264], Train loss: 0.043200988322496414\n",
      "Epoch[0], Val loss: 0.0414913073182106\n",
      "Epoch[0], Batch[2265], Train loss: 0.044179823249578476\n",
      "Epoch[0], Val loss: 0.04273711144924164\n",
      "Epoch[0], Batch[2266], Train loss: 0.04635496810078621\n",
      "Epoch[0], Val loss: 0.04233728349208832\n",
      "Epoch[0], Batch[2267], Train loss: 0.0494154617190361\n",
      "Epoch[0], Val loss: 0.046137481927871704\n",
      "Epoch[0], Batch[2268], Train loss: 0.04429452866315842\n",
      "Epoch[0], Val loss: 0.04412679746747017\n",
      "Epoch[0], Batch[2269], Train loss: 0.0475904606282711\n",
      "Epoch[0], Val loss: 0.04489210993051529\n",
      "Epoch[0], Batch[2270], Train loss: 0.04602952301502228\n",
      "Epoch[0], Val loss: 0.04168650880455971\n",
      "Epoch[0], Batch[2271], Train loss: 0.04487244412302971\n",
      "Epoch[0], Val loss: 0.042278874665498734\n",
      "Epoch[0], Batch[2272], Train loss: 0.04692704975605011\n",
      "Epoch[0], Val loss: 0.04333839938044548\n",
      "Epoch[0], Batch[2273], Train loss: 0.04539370909333229\n",
      "Epoch[0], Val loss: 0.04206020012497902\n",
      "Epoch[0], Batch[2274], Train loss: 0.04283910244703293\n",
      "Epoch[0], Val loss: 0.042131755501031876\n",
      "Epoch[0], Batch[2275], Train loss: 0.045296911150217056\n",
      "Epoch[0], Val loss: 0.04363919049501419\n",
      "Epoch[0], Batch[2276], Train loss: 0.04219822585582733\n",
      "Epoch[0], Val loss: 0.044963449239730835\n",
      "Epoch[0], Batch[2277], Train loss: 0.04345598444342613\n",
      "Epoch[0], Val loss: 0.04226836934685707\n",
      "Epoch[0], Batch[2278], Train loss: 0.04378712177276611\n",
      "Epoch[0], Val loss: 0.04212573915719986\n",
      "Epoch[0], Batch[2279], Train loss: 0.045392464846372604\n",
      "Epoch[0], Val loss: 0.042016323655843735\n",
      "Epoch[0], Batch[2280], Train loss: 0.04518130421638489\n",
      "Epoch[0], Val loss: 0.04647324979305267\n",
      "Epoch[0], Batch[2281], Train loss: 0.04551638662815094\n",
      "Epoch[0], Val loss: 0.04149440675973892\n",
      "Epoch[0], Batch[2282], Train loss: 0.044944338500499725\n",
      "Epoch[0], Val loss: 0.044409364461898804\n",
      "Epoch[0], Batch[2283], Train loss: 0.04310803487896919\n",
      "Epoch[0], Val loss: 0.04226692393422127\n",
      "Epoch[0], Batch[2284], Train loss: 0.04515600949525833\n",
      "Epoch[0], Val loss: 0.041695281863212585\n",
      "Epoch[0], Batch[2285], Train loss: 0.046408187597990036\n",
      "Epoch[0], Val loss: 0.0397174172103405\n",
      "Epoch[0], Batch[2286], Train loss: 0.045478783547878265\n",
      "Epoch[0], Val loss: 0.04219953715801239\n",
      "Epoch[0], Batch[2287], Train loss: 0.04567083716392517\n",
      "Epoch[0], Val loss: 0.04382331296801567\n",
      "Epoch[0], Batch[2288], Train loss: 0.042617928236722946\n",
      "Epoch[0], Val loss: 0.040846869349479675\n",
      "Epoch[0], Batch[2289], Train loss: 0.04649385064840317\n",
      "Epoch[0], Val loss: 0.04347318783402443\n",
      "Epoch[0], Batch[2290], Train loss: 0.04429156333208084\n",
      "Epoch[0], Val loss: 0.04655686393380165\n",
      "Epoch[0], Batch[2291], Train loss: 0.047327592968940735\n",
      "Epoch[0], Val loss: 0.0412551611661911\n",
      "Epoch[0], Batch[2292], Train loss: 0.039964426308870316\n",
      "Epoch[0], Val loss: 0.04162554070353508\n",
      "Epoch[0], Batch[2293], Train loss: 0.044814933091402054\n",
      "Epoch[0], Val loss: 0.03978171572089195\n",
      "Epoch[0], Batch[2294], Train loss: 0.04346904903650284\n",
      "Epoch[0], Val loss: 0.043369848281145096\n",
      "Epoch[0], Batch[2295], Train loss: 0.046099428087472916\n",
      "Epoch[0], Val loss: 0.04382145777344704\n",
      "Epoch[0], Batch[2296], Train loss: 0.04394558072090149\n",
      "Epoch[0], Val loss: 0.04182909429073334\n",
      "Epoch[0], Batch[2297], Train loss: 0.04215867444872856\n",
      "Epoch[0], Val loss: 0.04076613113284111\n",
      "Epoch[0], Batch[2298], Train loss: 0.04687129333615303\n",
      "Epoch[0], Val loss: 0.04164787754416466\n",
      "Epoch[0], Batch[2299], Train loss: 0.044578179717063904\n",
      "Epoch[0], Val loss: 0.040093421936035156\n",
      "Epoch[0], Batch[2300], Train loss: 0.04791673272848129\n",
      "Epoch[0], Val loss: 0.044804636389017105\n",
      "Epoch[0], Batch[2301], Train loss: 0.04871164262294769\n",
      "Epoch[0], Val loss: 0.04483255371451378\n",
      "Epoch[0], Batch[2302], Train loss: 0.04620935395359993\n",
      "Epoch[0], Val loss: 0.04663562402129173\n",
      "Epoch[0], Batch[2303], Train loss: 0.0437244214117527\n",
      "Epoch[0], Val loss: 0.04319307208061218\n",
      "Epoch[0], Batch[2304], Train loss: 0.047615401446819305\n",
      "Epoch[0], Val loss: 0.04803157225251198\n",
      "Epoch[0], Batch[2305], Train loss: 0.047621194273233414\n",
      "Epoch[0], Val loss: 0.04506297782063484\n",
      "Epoch[0], Batch[2306], Train loss: 0.04442697763442993\n",
      "Epoch[0], Val loss: 0.04325230419635773\n",
      "Epoch[0], Batch[2307], Train loss: 0.04266238957643509\n",
      "Epoch[0], Val loss: 0.04510481655597687\n",
      "Epoch[0], Batch[2308], Train loss: 0.04500573128461838\n",
      "Epoch[0], Val loss: 0.04029605910181999\n",
      "Epoch[0], Batch[2309], Train loss: 0.04264532029628754\n",
      "Epoch[0], Val loss: 0.04156002774834633\n",
      "Epoch[0], Batch[2310], Train loss: 0.04684830456972122\n",
      "Epoch[0], Val loss: 0.04267662391066551\n",
      "Epoch[0], Batch[2311], Train loss: 0.047207657247781754\n",
      "Epoch[0], Val loss: 0.041756726801395416\n",
      "Epoch[0], Batch[2312], Train loss: 0.04459962248802185\n",
      "Epoch[0], Val loss: 0.04336578771471977\n",
      "Epoch[0], Batch[2313], Train loss: 0.04618969187140465\n",
      "Epoch[0], Val loss: 0.04367509111762047\n",
      "Epoch[0], Batch[2314], Train loss: 0.04559871181845665\n",
      "Epoch[0], Val loss: 0.04175573214888573\n",
      "Epoch[0], Batch[2315], Train loss: 0.045447222888469696\n",
      "Epoch[0], Val loss: 0.04190019145607948\n",
      "Epoch[0], Batch[2316], Train loss: 0.04439115896821022\n",
      "Epoch[0], Val loss: 0.041603006422519684\n",
      "Epoch[0], Batch[2317], Train loss: 0.04563404619693756\n",
      "Epoch[0], Val loss: 0.04453542083501816\n",
      "Epoch[0], Batch[2318], Train loss: 0.04405630752444267\n",
      "Epoch[0], Val loss: 0.043893687427043915\n",
      "Epoch[0], Batch[2319], Train loss: 0.04471549391746521\n",
      "Epoch[0], Val loss: 0.044171690940856934\n",
      "Epoch[0], Batch[2320], Train loss: 0.04404580220580101\n",
      "Epoch[0], Val loss: 0.041416317224502563\n",
      "Epoch[0], Batch[2321], Train loss: 0.04472566023468971\n",
      "Epoch[0], Val loss: 0.04456734657287598\n",
      "Epoch[0], Batch[2322], Train loss: 0.04577149823307991\n",
      "Epoch[0], Val loss: 0.04041694849729538\n",
      "Epoch[0], Batch[2323], Train loss: 0.04411196708679199\n",
      "Epoch[0], Val loss: 0.04170863702893257\n",
      "Epoch[0], Batch[2324], Train loss: 0.043466754257678986\n",
      "Epoch[0], Val loss: 0.04528200626373291\n",
      "Epoch[0], Batch[2325], Train loss: 0.04340646043419838\n",
      "Epoch[0], Val loss: 0.04188460484147072\n",
      "Epoch[0], Batch[2326], Train loss: 0.04281207546591759\n",
      "Epoch[0], Val loss: 0.043259646743535995\n",
      "Epoch[0], Batch[2327], Train loss: 0.04338768869638443\n",
      "Epoch[0], Val loss: 0.04105186089873314\n",
      "Epoch[0], Batch[2328], Train loss: 0.043522220104932785\n",
      "Epoch[0], Val loss: 0.043301764875650406\n",
      "Epoch[0], Batch[2329], Train loss: 0.04412485659122467\n",
      "Epoch[0], Val loss: 0.042813293635845184\n",
      "Epoch[0], Batch[2330], Train loss: 0.04477386176586151\n",
      "Epoch[0], Val loss: 0.042881641536951065\n",
      "Epoch[0], Batch[2331], Train loss: 0.04270648956298828\n",
      "Epoch[0], Val loss: 0.043837182223796844\n",
      "Epoch[0], Batch[2332], Train loss: 0.044295262545347214\n",
      "Epoch[0], Val loss: 0.043203745037317276\n",
      "Epoch[0], Batch[2333], Train loss: 0.04709477722644806\n",
      "Epoch[0], Val loss: 0.04196393862366676\n",
      "Epoch[0], Batch[2334], Train loss: 0.045141756534576416\n",
      "Epoch[0], Val loss: 0.044060878455638885\n",
      "Epoch[0], Batch[2335], Train loss: 0.04405700042843819\n",
      "Epoch[0], Val loss: 0.042288441210985184\n",
      "Epoch[0], Batch[2336], Train loss: 0.04359202831983566\n",
      "Epoch[0], Val loss: 0.04328547418117523\n",
      "Epoch[0], Batch[2337], Train loss: 0.04436874762177467\n",
      "Epoch[0], Val loss: 0.04259207099676132\n",
      "Epoch[0], Batch[2338], Train loss: 0.044446852058172226\n",
      "Epoch[0], Val loss: 0.042498741298913956\n",
      "Epoch[0], Batch[2339], Train loss: 0.04539719223976135\n",
      "Epoch[0], Val loss: 0.04242318868637085\n",
      "Epoch[0], Batch[2340], Train loss: 0.046485986560583115\n",
      "Epoch[0], Val loss: 0.04313713312149048\n",
      "Epoch[0], Batch[2341], Train loss: 0.043396372348070145\n",
      "Epoch[0], Val loss: 0.04242708906531334\n",
      "Epoch[0], Batch[2342], Train loss: 0.04235035181045532\n",
      "Epoch[0], Val loss: 0.04366525262594223\n",
      "Epoch[0], Batch[2343], Train loss: 0.04217761754989624\n",
      "Epoch[0], Val loss: 0.04297072812914848\n",
      "Epoch[0], Batch[2344], Train loss: 0.04676329344511032\n",
      "Epoch[0], Val loss: 0.04282175377011299\n",
      "Epoch[0], Batch[2345], Train loss: 0.04639466479420662\n",
      "Epoch[0], Val loss: 0.04221522435545921\n",
      "Epoch[0], Batch[2346], Train loss: 0.04475594311952591\n",
      "Epoch[0], Val loss: 0.04415129870176315\n",
      "Epoch[0], Batch[2347], Train loss: 0.04441192373633385\n",
      "Epoch[0], Val loss: 0.04260663688182831\n",
      "Epoch[0], Batch[2348], Train loss: 0.04479027912020683\n",
      "Epoch[0], Val loss: 0.04219529777765274\n",
      "Epoch[0], Batch[2349], Train loss: 0.04679219052195549\n",
      "Epoch[0], Val loss: 0.041821178048849106\n",
      "Epoch[0], Batch[2350], Train loss: 0.048069946467876434\n",
      "Epoch[0], Val loss: 0.04251465946435928\n",
      "Epoch[0], Batch[2351], Train loss: 0.045372724533081055\n",
      "Epoch[0], Val loss: 0.04197029769420624\n",
      "Epoch[0], Batch[2352], Train loss: 0.04502788186073303\n",
      "Epoch[0], Val loss: 0.04166874289512634\n",
      "Epoch[0], Batch[2353], Train loss: 0.04215342923998833\n",
      "Epoch[0], Val loss: 0.04238148778676987\n",
      "Epoch[0], Batch[2354], Train loss: 0.04508677124977112\n",
      "Epoch[0], Val loss: 0.045062750577926636\n",
      "Epoch[0], Batch[2355], Train loss: 0.04392990842461586\n",
      "Epoch[0], Val loss: 0.04223013296723366\n",
      "Epoch[0], Batch[2356], Train loss: 0.047570351511240005\n",
      "Epoch[0], Val loss: 0.04169783741235733\n",
      "Epoch[0], Batch[2357], Train loss: 0.04739220067858696\n",
      "Epoch[0], Val loss: 0.04144206643104553\n",
      "Epoch[0], Batch[2358], Train loss: 0.042969960719347\n",
      "Epoch[0], Val loss: 0.04302069917321205\n",
      "Epoch[0], Batch[2359], Train loss: 0.0462975800037384\n",
      "Epoch[0], Val loss: 0.04403593763709068\n",
      "Epoch[0], Batch[2360], Train loss: 0.04715190827846527\n",
      "Epoch[0], Val loss: 0.04312834516167641\n",
      "Epoch[0], Batch[2361], Train loss: 0.04498046264052391\n",
      "Epoch[0], Val loss: 0.04010182246565819\n",
      "Epoch[0], Batch[2362], Train loss: 0.044559791684150696\n",
      "Epoch[0], Val loss: 0.039948102086782455\n",
      "Epoch[0], Batch[2363], Train loss: 0.04490469768643379\n",
      "Epoch[0], Val loss: 0.042303264141082764\n",
      "Epoch[0], Batch[2364], Train loss: 0.045271310955286026\n",
      "Epoch[0], Val loss: 0.03889991715550423\n",
      "Epoch[0], Batch[2365], Train loss: 0.045275233685970306\n",
      "Epoch[0], Val loss: 0.044077131897211075\n",
      "Epoch[0], Batch[2366], Train loss: 0.044749949127435684\n",
      "Epoch[0], Val loss: 0.04347699508070946\n",
      "Epoch[0], Batch[2367], Train loss: 0.04374177008867264\n",
      "Epoch[0], Val loss: 0.04197252541780472\n",
      "Epoch[0], Batch[2368], Train loss: 0.04487701505422592\n",
      "Epoch[0], Val loss: 0.04382675141096115\n",
      "Epoch[0], Batch[2369], Train loss: 0.045226022601127625\n",
      "Epoch[0], Val loss: 0.04222237318754196\n",
      "Epoch[0], Batch[2370], Train loss: 0.04303636774420738\n",
      "Epoch[0], Val loss: 0.04190885275602341\n",
      "Epoch[0], Batch[2371], Train loss: 0.04312968999147415\n",
      "Epoch[0], Val loss: 0.04426752030849457\n",
      "Epoch[0], Batch[2372], Train loss: 0.04399051144719124\n",
      "Epoch[0], Val loss: 0.04177989810705185\n",
      "Epoch[0], Batch[2373], Train loss: 0.04349014163017273\n",
      "Epoch[0], Val loss: 0.04153205454349518\n",
      "Epoch[0], Batch[2374], Train loss: 0.043645791709423065\n",
      "Epoch[0], Val loss: 0.041570670902729034\n",
      "Epoch[0], Batch[2375], Train loss: 0.04412129893898964\n",
      "Epoch[0], Val loss: 0.04158169403672218\n",
      "Epoch[0], Batch[2376], Train loss: 0.0458865761756897\n",
      "Epoch[0], Val loss: 0.043942779302597046\n",
      "Epoch[0], Batch[2377], Train loss: 0.046448320150375366\n",
      "Epoch[0], Val loss: 0.04206984117627144\n",
      "Epoch[0], Batch[2378], Train loss: 0.047556858509778976\n",
      "Epoch[0], Val loss: 0.04091675579547882\n",
      "Epoch[0], Batch[2379], Train loss: 0.04363016039133072\n",
      "Epoch[0], Val loss: 0.042336881160736084\n",
      "Epoch[0], Batch[2380], Train loss: 0.0446908064186573\n",
      "Epoch[0], Val loss: 0.03974594548344612\n",
      "Epoch[0], Batch[2381], Train loss: 0.04492431879043579\n",
      "Epoch[0], Val loss: 0.0422162264585495\n",
      "Epoch[0], Batch[2382], Train loss: 0.04606347903609276\n",
      "Epoch[0], Val loss: 0.04214932769536972\n",
      "Epoch[0], Batch[2383], Train loss: 0.04885278269648552\n",
      "Epoch[0], Val loss: 0.04557151347398758\n",
      "Epoch[0], Batch[2384], Train loss: 0.042809367179870605\n",
      "Epoch[0], Val loss: 0.04176187142729759\n",
      "Epoch[0], Batch[2385], Train loss: 0.04499367997050285\n",
      "Epoch[0], Val loss: 0.04369749501347542\n",
      "Epoch[0], Batch[2386], Train loss: 0.04489854350686073\n",
      "Epoch[0], Val loss: 0.04095277562737465\n",
      "Epoch[0], Batch[2387], Train loss: 0.04331367090344429\n",
      "Epoch[0], Val loss: 0.039680469781160355\n",
      "Epoch[0], Batch[2388], Train loss: 0.04287115857005119\n",
      "Epoch[0], Val loss: 0.04503635689616203\n",
      "Epoch[0], Batch[2389], Train loss: 0.04344061017036438\n",
      "Epoch[0], Val loss: 0.04048995301127434\n",
      "Epoch[0], Batch[2390], Train loss: 0.04207869991660118\n",
      "Epoch[0], Val loss: 0.04441522806882858\n",
      "Epoch[0], Batch[2391], Train loss: 0.04623100906610489\n",
      "Epoch[0], Val loss: 0.0399971529841423\n",
      "Epoch[0], Batch[2392], Train loss: 0.044368572533130646\n",
      "Epoch[0], Val loss: 0.04346122965216637\n",
      "Epoch[0], Batch[2393], Train loss: 0.04552318528294563\n",
      "Epoch[0], Val loss: 0.044288091361522675\n",
      "Epoch[0], Batch[2394], Train loss: 0.04345397651195526\n",
      "Epoch[0], Val loss: 0.04225683584809303\n",
      "Epoch[0], Batch[2395], Train loss: 0.04490986466407776\n",
      "Epoch[0], Val loss: 0.042619556188583374\n",
      "Epoch[0], Batch[2396], Train loss: 0.04340691864490509\n",
      "Epoch[0], Val loss: 0.040526654571294785\n",
      "Epoch[0], Batch[2397], Train loss: 0.044518835842609406\n",
      "Epoch[0], Val loss: 0.0409560464322567\n",
      "Epoch[0], Batch[2398], Train loss: 0.04330655559897423\n",
      "Epoch[0], Val loss: 0.04411623254418373\n",
      "Epoch[0], Batch[2399], Train loss: 0.044558148831129074\n",
      "Epoch[0], Val loss: 0.042714089155197144\n",
      "Epoch[0], Batch[2400], Train loss: 0.04342923313379288\n",
      "Epoch[0], Val loss: 0.04245595261454582\n",
      "Epoch[0], Batch[2401], Train loss: 0.04556742310523987\n",
      "Epoch[0], Val loss: 0.04235117882490158\n",
      "Epoch[0], Batch[2402], Train loss: 0.04422938451170921\n",
      "Epoch[0], Val loss: 0.040758486837148666\n",
      "Epoch[0], Batch[2403], Train loss: 0.045067351311445236\n",
      "Epoch[0], Val loss: 0.042273253202438354\n",
      "Epoch[0], Batch[2404], Train loss: 0.044297292828559875\n",
      "Epoch[0], Val loss: 0.041965167969465256\n",
      "Epoch[0], Batch[2405], Train loss: 0.043223485350608826\n",
      "Epoch[0], Val loss: 0.04159146174788475\n",
      "Epoch[0], Batch[2406], Train loss: 0.04469684138894081\n",
      "Epoch[0], Val loss: 0.04225713014602661\n",
      "Epoch[0], Batch[2407], Train loss: 0.04969877004623413\n",
      "Epoch[0], Val loss: 0.04155156761407852\n",
      "Epoch[0], Batch[2408], Train loss: 0.04059695824980736\n",
      "Epoch[0], Val loss: 0.04284119978547096\n",
      "Epoch[0], Batch[2409], Train loss: 0.0452561117708683\n",
      "Epoch[0], Val loss: 0.04080506041646004\n",
      "Epoch[0], Batch[2410], Train loss: 0.045759156346321106\n",
      "Epoch[0], Val loss: 0.04131117835640907\n",
      "Epoch[0], Batch[2411], Train loss: 0.042582035064697266\n",
      "Epoch[0], Val loss: 0.04219149798154831\n",
      "Epoch[0], Batch[2412], Train loss: 0.04546592757105827\n",
      "Epoch[0], Val loss: 0.044606201350688934\n",
      "Epoch[0], Batch[2413], Train loss: 0.043171755969524384\n",
      "Epoch[0], Val loss: 0.043039243668317795\n",
      "Epoch[0], Batch[2414], Train loss: 0.04189393296837807\n",
      "Epoch[0], Val loss: 0.04271003603935242\n",
      "Epoch[0], Batch[2415], Train loss: 0.04559410363435745\n",
      "Epoch[0], Val loss: 0.04372313991189003\n",
      "Epoch[0], Batch[2416], Train loss: 0.044252000749111176\n",
      "Epoch[0], Val loss: 0.043003443628549576\n",
      "Epoch[0], Batch[2417], Train loss: 0.04389609396457672\n",
      "Epoch[0], Val loss: 0.04245766997337341\n",
      "Epoch[0], Batch[2418], Train loss: 0.04238608852028847\n",
      "Epoch[0], Val loss: 0.043979451060295105\n",
      "Epoch[0], Batch[2419], Train loss: 0.045951150357723236\n",
      "Epoch[0], Val loss: 0.04345971345901489\n",
      "Epoch[0], Batch[2420], Train loss: 0.04583590850234032\n",
      "Epoch[0], Val loss: 0.043852172791957855\n",
      "Epoch[0], Batch[2421], Train loss: 0.045440759509801865\n",
      "Epoch[0], Val loss: 0.04188286513090134\n",
      "Epoch[0], Batch[2422], Train loss: 0.04322883486747742\n",
      "Epoch[0], Val loss: 0.041216157376766205\n",
      "Epoch[0], Batch[2423], Train loss: 0.04303169623017311\n",
      "Epoch[0], Val loss: 0.04341977462172508\n",
      "Epoch[0], Batch[2424], Train loss: 0.04628235474228859\n",
      "Epoch[0], Val loss: 0.04489286243915558\n",
      "Epoch[0], Batch[2425], Train loss: 0.0431596077978611\n",
      "Epoch[0], Val loss: 0.04166201502084732\n",
      "Epoch[0], Batch[2426], Train loss: 0.04563923180103302\n",
      "Epoch[0], Val loss: 0.04222508892416954\n",
      "Epoch[0], Batch[2427], Train loss: 0.04435295611619949\n",
      "Epoch[0], Val loss: 0.042632028460502625\n",
      "Epoch[0], Batch[2428], Train loss: 0.04462020471692085\n",
      "Epoch[0], Val loss: 0.04119043052196503\n",
      "Epoch[0], Batch[2429], Train loss: 0.045943256467580795\n",
      "Epoch[0], Val loss: 0.042721670120954514\n",
      "Epoch[0], Batch[2430], Train loss: 0.04580775275826454\n",
      "Epoch[0], Val loss: 0.04071831330657005\n",
      "Epoch[0], Batch[2431], Train loss: 0.0480075366795063\n",
      "Epoch[0], Val loss: 0.03936624154448509\n",
      "Epoch[0], Batch[2432], Train loss: 0.045363083481788635\n",
      "Epoch[0], Val loss: 0.0397365503013134\n",
      "Epoch[0], Batch[2433], Train loss: 0.04576321691274643\n",
      "Epoch[0], Val loss: 0.04403030127286911\n",
      "Epoch[0], Batch[2434], Train loss: 0.045565832406282425\n",
      "Epoch[0], Val loss: 0.04168103635311127\n",
      "Epoch[0], Batch[2435], Train loss: 0.04365318641066551\n",
      "Epoch[0], Val loss: 0.04418816789984703\n",
      "Epoch[0], Batch[2436], Train loss: 0.04320025444030762\n",
      "Epoch[0], Val loss: 0.04258427023887634\n",
      "Epoch[0], Batch[2437], Train loss: 0.045278556644916534\n",
      "Epoch[0], Val loss: 0.04098736122250557\n",
      "Epoch[0], Batch[2438], Train loss: 0.04382885992527008\n",
      "Epoch[0], Val loss: 0.041633490473032\n",
      "Epoch[0], Batch[2439], Train loss: 0.04328327253460884\n",
      "Epoch[0], Val loss: 0.041016001254320145\n",
      "Epoch[0], Batch[2440], Train loss: 0.0443800687789917\n",
      "Epoch[0], Val loss: 0.04536890983581543\n",
      "Epoch[0], Batch[2441], Train loss: 0.04330340027809143\n",
      "Epoch[0], Val loss: 0.04197120666503906\n",
      "Epoch[0], Batch[2442], Train loss: 0.042892783880233765\n",
      "Epoch[0], Val loss: 0.041215188801288605\n",
      "Epoch[0], Batch[2443], Train loss: 0.04458443820476532\n",
      "Epoch[0], Val loss: 0.041331905871629715\n",
      "Epoch[0], Batch[2444], Train loss: 0.042259007692337036\n",
      "Epoch[0], Val loss: 0.0449201762676239\n",
      "Epoch[0], Batch[2445], Train loss: 0.042278602719306946\n",
      "Epoch[0], Val loss: 0.04350503534078598\n",
      "Epoch[0], Batch[2446], Train loss: 0.04260033741593361\n",
      "Epoch[0], Val loss: 0.04309012368321419\n",
      "Epoch[0], Batch[2447], Train loss: 0.04384978488087654\n",
      "Epoch[0], Val loss: 0.040943704545497894\n",
      "Epoch[0], Batch[2448], Train loss: 0.044749483466148376\n",
      "Epoch[0], Val loss: 0.040950413793325424\n",
      "Epoch[0], Batch[2449], Train loss: 0.04548182711005211\n",
      "Epoch[0], Val loss: 0.039994727820158005\n",
      "Epoch[0], Batch[2450], Train loss: 0.04429314285516739\n",
      "Epoch[0], Val loss: 0.0422474667429924\n",
      "Epoch[0], Batch[2451], Train loss: 0.04380064457654953\n",
      "Epoch[0], Val loss: 0.04163309559226036\n",
      "Epoch[0], Batch[2452], Train loss: 0.04568846523761749\n",
      "Epoch[0], Val loss: 0.04296155646443367\n",
      "Epoch[0], Batch[2453], Train loss: 0.04261472076177597\n",
      "Epoch[0], Val loss: 0.04232439771294594\n",
      "Epoch[0], Batch[2454], Train loss: 0.04445916786789894\n",
      "Epoch[0], Val loss: 0.040474455803632736\n",
      "Epoch[0], Batch[2455], Train loss: 0.04357864335179329\n",
      "Epoch[0], Val loss: 0.040624067187309265\n",
      "Epoch[0], Batch[2456], Train loss: 0.043721918016672134\n",
      "Epoch[0], Val loss: 0.04210434481501579\n",
      "Epoch[0], Batch[2457], Train loss: 0.04414234682917595\n",
      "Epoch[0], Val loss: 0.04394085332751274\n",
      "Epoch[0], Batch[2458], Train loss: 0.045327141880989075\n",
      "Epoch[0], Val loss: 0.041855406016111374\n",
      "Epoch[0], Batch[2459], Train loss: 0.046266406774520874\n",
      "Epoch[0], Val loss: 0.041143786162137985\n",
      "Epoch[0], Batch[2460], Train loss: 0.04719120264053345\n",
      "Epoch[0], Val loss: 0.04343615099787712\n",
      "Epoch[0], Batch[2461], Train loss: 0.04114152491092682\n",
      "Epoch[0], Val loss: 0.04548526927828789\n",
      "Epoch[0], Batch[2462], Train loss: 0.042942583560943604\n",
      "Epoch[0], Val loss: 0.043207742273807526\n",
      "Epoch[0], Batch[2463], Train loss: 0.044457003474235535\n",
      "Epoch[0], Val loss: 0.041421905159950256\n",
      "Epoch[0], Batch[2464], Train loss: 0.04304485395550728\n",
      "Epoch[0], Val loss: 0.03995932638645172\n",
      "Epoch[0], Batch[2465], Train loss: 0.04442640766501427\n",
      "Epoch[0], Val loss: 0.04561251029372215\n",
      "Epoch[0], Batch[2466], Train loss: 0.04422410950064659\n",
      "Epoch[0], Val loss: 0.04296709597110748\n",
      "Epoch[0], Batch[2467], Train loss: 0.04386512562632561\n",
      "Epoch[0], Val loss: 0.040219806134700775\n",
      "Epoch[0], Batch[2468], Train loss: 0.04518209770321846\n",
      "Epoch[0], Val loss: 0.04145794361829758\n",
      "Epoch[0], Batch[2469], Train loss: 0.044858112931251526\n",
      "Epoch[0], Val loss: 0.04304051771759987\n",
      "Epoch[0], Batch[2470], Train loss: 0.045960091054439545\n",
      "Epoch[0], Val loss: 0.04294690862298012\n",
      "Epoch[0], Batch[2471], Train loss: 0.04420043155550957\n",
      "Epoch[0], Val loss: 0.04312368109822273\n",
      "Epoch[0], Batch[2472], Train loss: 0.041499607264995575\n",
      "Epoch[0], Val loss: 0.0435992032289505\n",
      "Epoch[0], Batch[2473], Train loss: 0.04370315372943878\n",
      "Epoch[0], Val loss: 0.0409928523004055\n",
      "Epoch[0], Batch[2474], Train loss: 0.04472631588578224\n",
      "Epoch[0], Val loss: 0.04090719297528267\n",
      "Epoch[0], Batch[2475], Train loss: 0.04432428255677223\n",
      "Epoch[0], Val loss: 0.042017702013254166\n",
      "Epoch[0], Batch[2476], Train loss: 0.048218317329883575\n",
      "Epoch[0], Val loss: 0.04312271997332573\n",
      "Epoch[0], Batch[2477], Train loss: 0.04732097312808037\n",
      "Epoch[0], Val loss: 0.04402032494544983\n",
      "Epoch[0], Batch[2478], Train loss: 0.04091406613588333\n",
      "Epoch[0], Val loss: 0.04240584000945091\n",
      "Epoch[0], Batch[2479], Train loss: 0.04402456805109978\n",
      "Epoch[0], Val loss: 0.04060602933168411\n",
      "Epoch[0], Batch[2480], Train loss: 0.04429418593645096\n",
      "Epoch[0], Val loss: 0.04233534634113312\n",
      "Epoch[0], Batch[2481], Train loss: 0.045820437371730804\n",
      "Epoch[0], Val loss: 0.04440511390566826\n",
      "Epoch[0], Batch[2482], Train loss: 0.04576067626476288\n",
      "Epoch[0], Val loss: 0.04077903926372528\n",
      "Epoch[0], Batch[2483], Train loss: 0.0450628362596035\n",
      "Epoch[0], Val loss: 0.04029083251953125\n",
      "Epoch[0], Batch[2484], Train loss: 0.044487930834293365\n",
      "Epoch[0], Val loss: 0.04284895956516266\n",
      "Epoch[0], Batch[2485], Train loss: 0.041599445044994354\n",
      "Epoch[0], Val loss: 0.04199506714940071\n",
      "Epoch[0], Batch[2486], Train loss: 0.04390745609998703\n",
      "Epoch[0], Val loss: 0.03970353677868843\n",
      "Epoch[0], Batch[2487], Train loss: 0.045454222708940506\n",
      "Epoch[0], Val loss: 0.041050877422094345\n",
      "Epoch[0], Batch[2488], Train loss: 0.04343482851982117\n",
      "Epoch[0], Val loss: 0.04327058792114258\n",
      "Epoch[0], Batch[2489], Train loss: 0.045130398124456406\n",
      "Epoch[0], Val loss: 0.04438713192939758\n",
      "Epoch[0], Batch[2490], Train loss: 0.044678449630737305\n",
      "Epoch[0], Val loss: 0.043352171778678894\n",
      "Epoch[0], Batch[2491], Train loss: 0.04475891962647438\n",
      "Epoch[0], Val loss: 0.041776858270168304\n",
      "Epoch[0], Batch[2492], Train loss: 0.04717928171157837\n",
      "Epoch[0], Val loss: 0.04287341982126236\n",
      "Epoch[0], Batch[2493], Train loss: 0.044763050973415375\n",
      "Epoch[0], Val loss: 0.04178551584482193\n",
      "Epoch[0], Batch[2494], Train loss: 0.043589115142822266\n",
      "Epoch[0], Val loss: 0.041411492973566055\n",
      "Epoch[0], Batch[2495], Train loss: 0.04268825799226761\n",
      "Epoch[0], Val loss: 0.041314367204904556\n",
      "Epoch[0], Batch[2496], Train loss: 0.04443806782364845\n",
      "Epoch[0], Val loss: 0.040479980409145355\n",
      "Epoch[0], Batch[2497], Train loss: 0.04379864037036896\n",
      "Epoch[0], Val loss: 0.03974840044975281\n",
      "Epoch[0], Batch[2498], Train loss: 0.04631844535470009\n",
      "Epoch[0], Val loss: 0.042975787073373795\n",
      "Epoch[0], Batch[2499], Train loss: 0.044112805277109146\n",
      "Epoch[0], Val loss: 0.04172245040535927\n",
      "Epoch[0], Batch[2500], Train loss: 0.04637419059872627\n",
      "Epoch[0], Val loss: 0.04263222962617874\n",
      "Epoch[0], Batch[2501], Train loss: 0.04650517553091049\n",
      "Epoch[0], Val loss: 0.040966570377349854\n",
      "Epoch[0], Batch[2502], Train loss: 0.044614922255277634\n",
      "Epoch[0], Val loss: 0.040751587599515915\n",
      "Epoch[0], Batch[2503], Train loss: 0.04472554475069046\n",
      "Epoch[0], Val loss: 0.04124985635280609\n",
      "Epoch[0], Batch[2504], Train loss: 0.04352060705423355\n",
      "Epoch[0], Val loss: 0.04100681468844414\n",
      "Epoch[0], Batch[2505], Train loss: 0.041484709829092026\n",
      "Epoch[0], Val loss: 0.040552154183387756\n",
      "Epoch[0], Batch[2506], Train loss: 0.044653382152318954\n",
      "Epoch[0], Val loss: 0.041901953518390656\n",
      "Epoch[0], Batch[2507], Train loss: 0.04348406940698624\n",
      "Epoch[0], Val loss: 0.04046432301402092\n",
      "Epoch[0], Batch[2508], Train loss: 0.044457245618104935\n",
      "Epoch[0], Val loss: 0.039622481912374496\n",
      "Epoch[0], Batch[2509], Train loss: 0.04332027956843376\n",
      "Epoch[0], Val loss: 0.04306327551603317\n",
      "Epoch[0], Batch[2510], Train loss: 0.04383782669901848\n",
      "Epoch[0], Val loss: 0.038227859884500504\n",
      "Epoch[0], Batch[2511], Train loss: 0.04596049711108208\n",
      "Epoch[0], Val loss: 0.042027320712804794\n",
      "Epoch[0], Batch[2512], Train loss: 0.042290911078453064\n",
      "Epoch[0], Val loss: 0.04156672582030296\n",
      "Epoch[0], Batch[2513], Train loss: 0.04150494188070297\n",
      "Epoch[0], Val loss: 0.04127756506204605\n",
      "Epoch[0], Batch[2514], Train loss: 0.04340014234185219\n",
      "Epoch[0], Val loss: 0.04120999574661255\n",
      "Epoch[0], Batch[2515], Train loss: 0.04630694538354874\n",
      "Epoch[0], Val loss: 0.04244977980852127\n",
      "Epoch[0], Batch[2516], Train loss: 0.04326650872826576\n",
      "Epoch[0], Val loss: 0.04074135795235634\n",
      "Epoch[0], Batch[2517], Train loss: 0.04372774437069893\n",
      "Epoch[0], Val loss: 0.041053805500268936\n",
      "Epoch[0], Batch[2518], Train loss: 0.044522665441036224\n",
      "Epoch[0], Val loss: 0.039303168654441833\n",
      "Epoch[0], Batch[2519], Train loss: 0.04243076220154762\n",
      "Epoch[0], Val loss: 0.04142560437321663\n",
      "Epoch[0], Batch[2520], Train loss: 0.04243869334459305\n",
      "Epoch[0], Val loss: 0.0419582799077034\n",
      "Epoch[0], Batch[2521], Train loss: 0.043301988393068314\n",
      "Epoch[0], Val loss: 0.04118476063013077\n",
      "Epoch[0], Batch[2522], Train loss: 0.04280145466327667\n",
      "Epoch[0], Val loss: 0.0431673638522625\n",
      "Epoch[0], Batch[2523], Train loss: 0.04660910740494728\n",
      "Epoch[0], Val loss: 0.03987986594438553\n",
      "Epoch[0], Batch[2524], Train loss: 0.04210232198238373\n",
      "Epoch[0], Val loss: 0.03993085026741028\n",
      "Epoch[0], Batch[2525], Train loss: 0.04240955412387848\n",
      "Epoch[0], Val loss: 0.04012675583362579\n",
      "Epoch[0], Batch[2526], Train loss: 0.04407553747296333\n",
      "Epoch[0], Val loss: 0.042342424392700195\n",
      "Epoch[0], Batch[2527], Train loss: 0.04086117446422577\n",
      "Epoch[0], Val loss: 0.042153164744377136\n",
      "Epoch[0], Batch[2528], Train loss: 0.04181491956114769\n",
      "Epoch[0], Val loss: 0.04128364846110344\n",
      "Epoch[0], Batch[2529], Train loss: 0.04279840365052223\n",
      "Epoch[0], Val loss: 0.03812016546726227\n",
      "Epoch[0], Batch[2530], Train loss: 0.044847238808870316\n",
      "Epoch[0], Val loss: 0.042677704244852066\n",
      "Epoch[0], Batch[2531], Train loss: 0.04198943078517914\n",
      "Epoch[0], Val loss: 0.04218485206365585\n",
      "Epoch[0], Batch[2532], Train loss: 0.04129309952259064\n",
      "Epoch[0], Val loss: 0.042215846478939056\n",
      "Epoch[0], Batch[2533], Train loss: 0.041117846965789795\n",
      "Epoch[0], Val loss: 0.04134434834122658\n",
      "Epoch[0], Batch[2534], Train loss: 0.045262303203344345\n",
      "Epoch[0], Val loss: 0.04175478219985962\n",
      "Epoch[0], Batch[2535], Train loss: 0.041449446231126785\n",
      "Epoch[0], Val loss: 0.042120933532714844\n",
      "Epoch[0], Batch[2536], Train loss: 0.042942289263010025\n",
      "Epoch[0], Val loss: 0.04047585651278496\n",
      "Epoch[0], Batch[2537], Train loss: 0.04179007560014725\n",
      "Epoch[0], Val loss: 0.041583139449357986\n",
      "Epoch[0], Batch[2538], Train loss: 0.043163031339645386\n",
      "Epoch[0], Val loss: 0.04503905773162842\n",
      "Epoch[0], Batch[2539], Train loss: 0.04669330269098282\n",
      "Epoch[0], Val loss: 0.04300319775938988\n",
      "Epoch[0], Batch[2540], Train loss: 0.0444963239133358\n",
      "Epoch[0], Val loss: 0.04499664157629013\n",
      "Epoch[0], Batch[2541], Train loss: 0.04447585344314575\n",
      "Epoch[0], Val loss: 0.0428311862051487\n",
      "Epoch[0], Batch[2542], Train loss: 0.045735761523246765\n",
      "Epoch[0], Val loss: 0.04361341521143913\n",
      "Epoch[0], Batch[2543], Train loss: 0.04175732284784317\n",
      "Epoch[0], Val loss: 0.04234951362013817\n",
      "Epoch[0], Batch[2544], Train loss: 0.04346511512994766\n",
      "Epoch[0], Val loss: 0.044377025216817856\n",
      "Epoch[0], Batch[2545], Train loss: 0.04228062555193901\n",
      "Epoch[0], Val loss: 0.04049243777990341\n",
      "Epoch[0], Batch[2546], Train loss: 0.04749629646539688\n",
      "Epoch[0], Val loss: 0.042112477123737335\n",
      "Epoch[0], Batch[2547], Train loss: 0.04387303814291954\n",
      "Epoch[0], Val loss: 0.04262339696288109\n",
      "Epoch[0], Batch[2548], Train loss: 0.043707218021154404\n",
      "Epoch[0], Val loss: 0.04226725175976753\n",
      "Epoch[0], Batch[2549], Train loss: 0.04314040020108223\n",
      "Epoch[0], Val loss: 0.04090999439358711\n",
      "Epoch[0], Batch[2550], Train loss: 0.04365473613142967\n",
      "Epoch[0], Val loss: 0.04202895984053612\n",
      "Epoch[0], Batch[2551], Train loss: 0.04569962993264198\n",
      "Epoch[0], Val loss: 0.041513338685035706\n",
      "Epoch[0], Batch[2552], Train loss: 0.046044353395700455\n",
      "Epoch[0], Val loss: 0.04287412390112877\n",
      "Epoch[0], Batch[2553], Train loss: 0.04185401275753975\n",
      "Epoch[0], Val loss: 0.042508479207754135\n",
      "Epoch[0], Batch[2554], Train loss: 0.04436922445893288\n",
      "Epoch[0], Val loss: 0.03922631964087486\n",
      "Epoch[0], Batch[2555], Train loss: 0.041559088975191116\n",
      "Epoch[0], Val loss: 0.04020991548895836\n",
      "Epoch[0], Batch[2556], Train loss: 0.042209986597299576\n",
      "Epoch[0], Val loss: 0.0420747846364975\n",
      "Epoch[0], Batch[2557], Train loss: 0.04174692928791046\n",
      "Epoch[0], Val loss: 0.042355265468358994\n",
      "Epoch[0], Batch[2558], Train loss: 0.04174124076962471\n",
      "Epoch[0], Val loss: 0.04484742134809494\n",
      "Epoch[0], Batch[2559], Train loss: 0.04593585804104805\n",
      "Epoch[0], Val loss: 0.04212630167603493\n",
      "Epoch[0], Batch[2560], Train loss: 0.042007435113191605\n",
      "Epoch[0], Val loss: 0.04026799276471138\n",
      "Epoch[0], Batch[2561], Train loss: 0.04208008572459221\n",
      "Epoch[0], Val loss: 0.04135923832654953\n",
      "Epoch[0], Batch[2562], Train loss: 0.042150091379880905\n",
      "Epoch[0], Val loss: 0.041918154805898666\n",
      "Epoch[0], Batch[2563], Train loss: 0.041738029569387436\n",
      "Epoch[0], Val loss: 0.042743977159261703\n",
      "Epoch[0], Batch[2564], Train loss: 0.045067254453897476\n",
      "Epoch[0], Val loss: 0.044638168066740036\n",
      "Epoch[0], Batch[2565], Train loss: 0.04616124927997589\n",
      "Epoch[0], Val loss: 0.040707387030124664\n",
      "Epoch[0], Batch[2566], Train loss: 0.04535619169473648\n",
      "Epoch[0], Val loss: 0.04250354319810867\n",
      "Epoch[0], Batch[2567], Train loss: 0.04605972766876221\n",
      "Epoch[0], Val loss: 0.0417596809566021\n",
      "Epoch[0], Batch[2568], Train loss: 0.041634511202573776\n",
      "Epoch[0], Val loss: 0.040339287370443344\n",
      "Epoch[0], Batch[2569], Train loss: 0.0428573340177536\n",
      "Epoch[0], Val loss: 0.040310125797986984\n",
      "Epoch[0], Batch[2570], Train loss: 0.04296673461794853\n",
      "Epoch[0], Val loss: 0.04126495495438576\n",
      "Epoch[0], Batch[2571], Train loss: 0.04349213093519211\n",
      "Epoch[0], Val loss: 0.042605116963386536\n",
      "Epoch[0], Batch[2572], Train loss: 0.04153776913881302\n",
      "Epoch[0], Val loss: 0.03991333767771721\n",
      "Epoch[0], Batch[2573], Train loss: 0.040766555815935135\n",
      "Epoch[0], Val loss: 0.04426261782646179\n",
      "Epoch[0], Batch[2574], Train loss: 0.04185505211353302\n",
      "Epoch[0], Val loss: 0.03921885788440704\n",
      "Epoch[0], Batch[2575], Train loss: 0.04023787006735802\n",
      "Epoch[0], Val loss: 0.040486931800842285\n",
      "Epoch[0], Batch[2576], Train loss: 0.04498669505119324\n",
      "Epoch[0], Val loss: 0.03727881610393524\n",
      "Epoch[0], Batch[2577], Train loss: 0.043337300419807434\n",
      "Epoch[0], Val loss: 0.04178348556160927\n",
      "Epoch[0], Batch[2578], Train loss: 0.04189681634306908\n",
      "Epoch[0], Val loss: 0.04200036823749542\n",
      "Epoch[0], Batch[2579], Train loss: 0.04504481703042984\n",
      "Epoch[0], Val loss: 0.04027611389756203\n",
      "Epoch[0], Batch[2580], Train loss: 0.042539048939943314\n",
      "Epoch[0], Val loss: 0.04332680627703667\n",
      "Epoch[0], Batch[2581], Train loss: 0.045824166387319565\n",
      "Epoch[0], Val loss: 0.04414769634604454\n",
      "Epoch[0], Batch[2582], Train loss: 0.04270901903510094\n",
      "Epoch[0], Val loss: 0.042670685797929764\n",
      "Epoch[0], Batch[2583], Train loss: 0.04187088832259178\n",
      "Epoch[0], Val loss: 0.04154185205698013\n",
      "Epoch[0], Batch[2584], Train loss: 0.043008867651224136\n",
      "Epoch[0], Val loss: 0.03940830007195473\n",
      "Epoch[0], Batch[2585], Train loss: 0.041034769266843796\n",
      "Epoch[0], Val loss: 0.04040435701608658\n",
      "Epoch[0], Batch[2586], Train loss: 0.04611273482441902\n",
      "Epoch[0], Val loss: 0.040247753262519836\n",
      "Epoch[0], Batch[2587], Train loss: 0.042641740292310715\n",
      "Epoch[0], Val loss: 0.03948331996798515\n",
      "Epoch[0], Batch[2588], Train loss: 0.04248519614338875\n",
      "Epoch[0], Val loss: 0.039978157728910446\n",
      "Epoch[0], Batch[2589], Train loss: 0.04444703832268715\n",
      "Epoch[0], Val loss: 0.03911294788122177\n",
      "Epoch[0], Batch[2590], Train loss: 0.042843323200941086\n",
      "Epoch[0], Val loss: 0.042144469916820526\n",
      "Epoch[0], Batch[2591], Train loss: 0.04251658916473389\n",
      "Epoch[0], Val loss: 0.03907856345176697\n",
      "Epoch[0], Batch[2592], Train loss: 0.043873973190784454\n",
      "Epoch[0], Val loss: 0.04354378581047058\n",
      "Epoch[0], Batch[2593], Train loss: 0.039706744253635406\n",
      "Epoch[0], Val loss: 0.041988447308540344\n",
      "Epoch[0], Batch[2594], Train loss: 0.0424298495054245\n",
      "Epoch[0], Val loss: 0.03882818669080734\n",
      "Epoch[0], Batch[2595], Train loss: 0.04402420297265053\n",
      "Epoch[0], Val loss: 0.040170736610889435\n",
      "Epoch[0], Batch[2596], Train loss: 0.04234028235077858\n",
      "Epoch[0], Val loss: 0.04297401383519173\n",
      "Epoch[0], Batch[2597], Train loss: 0.0435023233294487\n",
      "Epoch[0], Val loss: 0.04282201826572418\n",
      "Epoch[0], Batch[2598], Train loss: 0.0442715659737587\n",
      "Epoch[0], Val loss: 0.03999878466129303\n",
      "Epoch[0], Batch[2599], Train loss: 0.04161093384027481\n",
      "Epoch[0], Val loss: 0.04310554265975952\n",
      "Epoch[0], Batch[2600], Train loss: 0.04354926943778992\n",
      "Epoch[0], Val loss: 0.042158786207437515\n",
      "Epoch[0], Batch[2601], Train loss: 0.042502373456954956\n",
      "Epoch[0], Val loss: 0.04052668809890747\n",
      "Epoch[0], Batch[2602], Train loss: 0.04203294590115547\n",
      "Epoch[0], Val loss: 0.04274694249033928\n",
      "Epoch[0], Batch[2603], Train loss: 0.0420919805765152\n",
      "Epoch[0], Val loss: 0.04277733340859413\n",
      "Epoch[0], Batch[2604], Train loss: 0.04372403025627136\n",
      "Epoch[0], Val loss: 0.041702188551425934\n",
      "Epoch[0], Batch[2605], Train loss: 0.043969664722681046\n",
      "Epoch[0], Val loss: 0.04160543531179428\n",
      "Epoch[0], Batch[2606], Train loss: 0.04368636757135391\n",
      "Epoch[0], Val loss: 0.04340352118015289\n",
      "Epoch[0], Batch[2607], Train loss: 0.04320858046412468\n",
      "Epoch[0], Val loss: 0.04115309938788414\n",
      "Epoch[0], Batch[2608], Train loss: 0.041183557361364365\n",
      "Epoch[0], Val loss: 0.042545247822999954\n",
      "Epoch[0], Batch[2609], Train loss: 0.04424351081252098\n",
      "Epoch[0], Val loss: 0.04102896898984909\n",
      "Epoch[0], Batch[2610], Train loss: 0.04391628876328468\n",
      "Epoch[0], Val loss: 0.04370369762182236\n",
      "Epoch[0], Batch[2611], Train loss: 0.045507196336984634\n",
      "Epoch[0], Val loss: 0.04203827306628227\n",
      "Epoch[0], Batch[2612], Train loss: 0.04123919829726219\n",
      "Epoch[0], Val loss: 0.03944913670420647\n",
      "Epoch[0], Batch[2613], Train loss: 0.04291696101427078\n",
      "Epoch[0], Val loss: 0.04130089282989502\n",
      "Epoch[0], Batch[2614], Train loss: 0.041941430419683456\n",
      "Epoch[0], Val loss: 0.04149936884641647\n",
      "Epoch[0], Batch[2615], Train loss: 0.043137338012456894\n",
      "Epoch[0], Val loss: 0.04124337434768677\n",
      "Epoch[0], Batch[2616], Train loss: 0.04620911180973053\n",
      "Epoch[0], Val loss: 0.041442226618528366\n",
      "Epoch[0], Batch[2617], Train loss: 0.03844026103615761\n",
      "Epoch[0], Val loss: 0.04112359136343002\n",
      "Epoch[0], Batch[2618], Train loss: 0.045719701796770096\n",
      "Epoch[0], Val loss: 0.04007028043270111\n",
      "Epoch[0], Batch[2619], Train loss: 0.04351647570729256\n",
      "Epoch[0], Val loss: 0.04168488085269928\n",
      "Epoch[0], Batch[2620], Train loss: 0.045008305460214615\n",
      "Epoch[0], Val loss: 0.04139595478773117\n",
      "Epoch[0], Batch[2621], Train loss: 0.043235599994659424\n",
      "Epoch[0], Val loss: 0.043312836438417435\n",
      "Epoch[0], Batch[2622], Train loss: 0.0421590693295002\n",
      "Epoch[0], Val loss: 0.04154273122549057\n",
      "Epoch[0], Batch[2623], Train loss: 0.041462186723947525\n",
      "Epoch[0], Val loss: 0.043268267065286636\n",
      "Epoch[0], Batch[2624], Train loss: 0.04391128197312355\n",
      "Epoch[0], Val loss: 0.04574088379740715\n",
      "Epoch[0], Batch[2625], Train loss: 0.04592341557145119\n",
      "Epoch[0], Val loss: 0.045187536627054214\n",
      "Epoch[0], Batch[2626], Train loss: 0.041118424385786057\n",
      "Epoch[0], Val loss: 0.03810196742415428\n",
      "Epoch[0], Batch[2627], Train loss: 0.04246202111244202\n",
      "Epoch[0], Val loss: 0.04288945719599724\n",
      "Epoch[0], Batch[2628], Train loss: 0.04096656292676926\n",
      "Epoch[0], Val loss: 0.04301736131310463\n",
      "Epoch[0], Batch[2629], Train loss: 0.04498068615794182\n",
      "Epoch[0], Val loss: 0.03831418231129646\n",
      "Epoch[0], Batch[2630], Train loss: 0.042635027319192886\n",
      "Epoch[0], Val loss: 0.04216183349490166\n",
      "Epoch[0], Batch[2631], Train loss: 0.03973282501101494\n",
      "Epoch[0], Val loss: 0.041102733463048935\n",
      "Epoch[0], Batch[2632], Train loss: 0.046210937201976776\n",
      "Epoch[0], Val loss: 0.04101972654461861\n",
      "Epoch[0], Batch[2633], Train loss: 0.044320784509181976\n",
      "Epoch[0], Val loss: 0.04356791451573372\n",
      "Epoch[0], Batch[2634], Train loss: 0.043386176228523254\n",
      "Epoch[0], Val loss: 0.041393544524908066\n",
      "Epoch[0], Batch[2635], Train loss: 0.047539569437503815\n",
      "Epoch[0], Val loss: 0.04357786104083061\n",
      "Epoch[0], Batch[2636], Train loss: 0.04528236761689186\n",
      "Epoch[0], Val loss: 0.040407728403806686\n",
      "Epoch[0], Batch[2637], Train loss: 0.04288443177938461\n",
      "Epoch[0], Val loss: 0.040531110018491745\n",
      "Epoch[0], Batch[2638], Train loss: 0.042092692106962204\n",
      "Epoch[0], Val loss: 0.04089413955807686\n",
      "Epoch[0], Batch[2639], Train loss: 0.04521540179848671\n",
      "Epoch[0], Val loss: 0.041081055998802185\n",
      "Epoch[0], Batch[2640], Train loss: 0.04042734578251839\n",
      "Epoch[0], Val loss: 0.039453279227018356\n",
      "Epoch[0], Batch[2641], Train loss: 0.04397783428430557\n",
      "Epoch[0], Val loss: 0.03856806829571724\n",
      "Epoch[0], Batch[2642], Train loss: 0.042845483869314194\n",
      "Epoch[0], Val loss: 0.04039971902966499\n",
      "Epoch[0], Batch[2643], Train loss: 0.04057254642248154\n",
      "Epoch[0], Val loss: 0.03975661098957062\n",
      "Epoch[0], Batch[2644], Train loss: 0.04314118251204491\n",
      "Epoch[0], Val loss: 0.039361122995615005\n",
      "Epoch[0], Batch[2645], Train loss: 0.04391865432262421\n",
      "Epoch[0], Val loss: 0.039556872099637985\n",
      "Epoch[0], Batch[2646], Train loss: 0.041862305253744125\n",
      "Epoch[0], Val loss: 0.03822753205895424\n",
      "Epoch[0], Batch[2647], Train loss: 0.04082141071557999\n",
      "Epoch[0], Val loss: 0.0380023717880249\n",
      "Epoch[0], Batch[2648], Train loss: 0.04385756701231003\n",
      "Epoch[0], Val loss: 0.04185379669070244\n",
      "Epoch[0], Batch[2649], Train loss: 0.04457751289010048\n",
      "Epoch[0], Val loss: 0.039642881602048874\n",
      "Epoch[0], Batch[2650], Train loss: 0.045474618673324585\n",
      "Epoch[0], Val loss: 0.042293645441532135\n",
      "Epoch[0], Batch[2651], Train loss: 0.04141588136553764\n",
      "Epoch[0], Val loss: 0.04088939353823662\n",
      "Epoch[0], Batch[2652], Train loss: 0.043646618723869324\n",
      "Epoch[0], Val loss: 0.04028932377696037\n",
      "Epoch[0], Batch[2653], Train loss: 0.04588383436203003\n",
      "Epoch[0], Val loss: 0.03931685537099838\n",
      "Epoch[0], Batch[2654], Train loss: 0.04621630534529686\n",
      "Epoch[0], Val loss: 0.042400456964969635\n",
      "Epoch[0], Batch[2655], Train loss: 0.04285956919193268\n",
      "Epoch[0], Val loss: 0.04020022600889206\n",
      "Epoch[0], Batch[2656], Train loss: 0.04510541632771492\n",
      "Epoch[0], Val loss: 0.0391106903553009\n",
      "Epoch[0], Batch[2657], Train loss: 0.044416338205337524\n",
      "Epoch[0], Val loss: 0.038981806486845016\n",
      "Epoch[0], Batch[2658], Train loss: 0.040219780057668686\n",
      "Epoch[0], Val loss: 0.038962334394454956\n",
      "Epoch[0], Batch[2659], Train loss: 0.04253247380256653\n",
      "Epoch[0], Val loss: 0.044195082038640976\n",
      "Epoch[0], Batch[2660], Train loss: 0.04307316988706589\n",
      "Epoch[0], Val loss: 0.04289235174655914\n",
      "Epoch[0], Batch[2661], Train loss: 0.04110702499747276\n",
      "Epoch[0], Val loss: 0.04194914922118187\n",
      "Epoch[0], Batch[2662], Train loss: 0.04335211589932442\n",
      "Epoch[0], Val loss: 0.04064260423183441\n",
      "Epoch[0], Batch[2663], Train loss: 0.04447997733950615\n",
      "Epoch[0], Val loss: 0.04073835164308548\n",
      "Epoch[0], Batch[2664], Train loss: 0.04374174028635025\n",
      "Epoch[0], Val loss: 0.0403321199119091\n",
      "Epoch[0], Batch[2665], Train loss: 0.04232098534703255\n",
      "Epoch[0], Val loss: 0.03984008729457855\n",
      "Epoch[0], Batch[2666], Train loss: 0.0436343252658844\n",
      "Epoch[0], Val loss: 0.0388711653649807\n",
      "Epoch[0], Batch[2667], Train loss: 0.04167378321290016\n",
      "Epoch[0], Val loss: 0.043072085827589035\n",
      "Epoch[0], Batch[2668], Train loss: 0.042120616883039474\n",
      "Epoch[0], Val loss: 0.04327765479683876\n",
      "Epoch[0], Batch[2669], Train loss: 0.04135949909687042\n",
      "Epoch[0], Val loss: 0.040325820446014404\n",
      "Epoch[0], Batch[2670], Train loss: 0.04265156015753746\n",
      "Epoch[0], Val loss: 0.03999503701925278\n",
      "Epoch[0], Batch[2671], Train loss: 0.041742999106645584\n",
      "Epoch[0], Val loss: 0.039008621126413345\n",
      "Epoch[0], Batch[2672], Train loss: 0.0424685925245285\n",
      "Epoch[0], Val loss: 0.04007412865757942\n",
      "Epoch[0], Batch[2673], Train loss: 0.04281005635857582\n",
      "Epoch[0], Val loss: 0.04145378991961479\n",
      "Epoch[0], Batch[2674], Train loss: 0.04234819859266281\n",
      "Epoch[0], Val loss: 0.03779561072587967\n",
      "Epoch[0], Batch[2675], Train loss: 0.0421556755900383\n",
      "Epoch[0], Val loss: 0.04204718396067619\n",
      "Epoch[0], Batch[2676], Train loss: 0.04234527051448822\n",
      "Epoch[0], Val loss: 0.0420297235250473\n",
      "Epoch[0], Batch[2677], Train loss: 0.04459703713655472\n",
      "Epoch[0], Val loss: 0.042864419519901276\n",
      "Epoch[0], Batch[2678], Train loss: 0.0455041266977787\n",
      "Epoch[0], Val loss: 0.04205944761633873\n",
      "Epoch[0], Batch[2679], Train loss: 0.04352680966258049\n",
      "Epoch[0], Val loss: 0.039145249873399734\n",
      "Epoch[0], Batch[2680], Train loss: 0.04462997987866402\n",
      "Epoch[0], Val loss: 0.03763362020254135\n",
      "Epoch[0], Batch[2681], Train loss: 0.04195450618863106\n",
      "Epoch[0], Val loss: 0.03918994963169098\n",
      "Epoch[0], Batch[2682], Train loss: 0.041291870176792145\n",
      "Epoch[0], Val loss: 0.041067205369472504\n",
      "Epoch[0], Batch[2683], Train loss: 0.04235471785068512\n",
      "Epoch[0], Val loss: 0.04046158492565155\n",
      "Epoch[0], Batch[2684], Train loss: 0.04128998890519142\n",
      "Epoch[0], Val loss: 0.03900590538978577\n",
      "Epoch[0], Batch[2685], Train loss: 0.04232381284236908\n",
      "Epoch[0], Val loss: 0.041409384459257126\n",
      "Epoch[0], Batch[2686], Train loss: 0.04188375174999237\n",
      "Epoch[0], Val loss: 0.0402301587164402\n",
      "Epoch[0], Batch[2687], Train loss: 0.044148046523332596\n",
      "Epoch[0], Val loss: 0.03995807468891144\n",
      "Epoch[0], Batch[2688], Train loss: 0.04171650856733322\n",
      "Epoch[0], Val loss: 0.04226464405655861\n",
      "Epoch[0], Batch[2689], Train loss: 0.041560448706150055\n",
      "Epoch[0], Val loss: 0.03907230868935585\n",
      "Epoch[0], Batch[2690], Train loss: 0.042952265590429306\n",
      "Epoch[0], Val loss: 0.04071391001343727\n",
      "Epoch[0], Batch[2691], Train loss: 0.04383428022265434\n",
      "Epoch[0], Val loss: 0.039008527994155884\n",
      "Epoch[0], Batch[2692], Train loss: 0.04578603804111481\n",
      "Epoch[0], Val loss: 0.040204063057899475\n",
      "Epoch[0], Batch[2693], Train loss: 0.044225215911865234\n",
      "Epoch[0], Val loss: 0.04166267067193985\n",
      "Epoch[0], Batch[2694], Train loss: 0.04175460338592529\n",
      "Epoch[0], Val loss: 0.03653381019830704\n",
      "Epoch[0], Batch[2695], Train loss: 0.04366080462932587\n",
      "Epoch[0], Val loss: 0.042591504752635956\n",
      "Epoch[0], Batch[2696], Train loss: 0.04392952844500542\n",
      "Epoch[0], Val loss: 0.043727073818445206\n",
      "Epoch[0], Batch[2697], Train loss: 0.043685369193553925\n",
      "Epoch[0], Val loss: 0.038717757910490036\n",
      "Epoch[0], Batch[2698], Train loss: 0.044390950351953506\n",
      "Epoch[0], Val loss: 0.03891050070524216\n",
      "Epoch[0], Batch[2699], Train loss: 0.04262150079011917\n",
      "Epoch[0], Val loss: 0.04154444858431816\n",
      "Epoch[0], Batch[2700], Train loss: 0.04064171388745308\n",
      "Epoch[0], Val loss: 0.04081146791577339\n",
      "Epoch[0], Batch[2701], Train loss: 0.04312523081898689\n",
      "Epoch[0], Val loss: 0.04102165624499321\n",
      "Epoch[0], Batch[2702], Train loss: 0.042373575270175934\n",
      "Epoch[0], Val loss: 0.03947175666689873\n",
      "Epoch[0], Batch[2703], Train loss: 0.04043128341436386\n",
      "Epoch[0], Val loss: 0.04266950488090515\n",
      "Epoch[0], Batch[2704], Train loss: 0.03947612643241882\n",
      "Epoch[0], Val loss: 0.03985659405589104\n",
      "Epoch[0], Batch[2705], Train loss: 0.04356039687991142\n",
      "Epoch[0], Val loss: 0.04103821516036987\n",
      "Epoch[0], Batch[2706], Train loss: 0.043423619121313095\n",
      "Epoch[0], Val loss: 0.043360885232686996\n",
      "Epoch[0], Batch[2707], Train loss: 0.04346681386232376\n",
      "Epoch[0], Val loss: 0.041291914880275726\n",
      "Epoch[0], Batch[2708], Train loss: 0.04178101196885109\n",
      "Epoch[0], Val loss: 0.03713204711675644\n",
      "Epoch[0], Batch[2709], Train loss: 0.04160968214273453\n",
      "Epoch[0], Val loss: 0.040183041244745255\n",
      "Epoch[0], Batch[2710], Train loss: 0.0428200326859951\n",
      "Epoch[0], Val loss: 0.03979266434907913\n",
      "Epoch[0], Batch[2711], Train loss: 0.04214329272508621\n",
      "Epoch[0], Val loss: 0.042082779109478\n",
      "Epoch[0], Batch[2712], Train loss: 0.04232407361268997\n",
      "Epoch[0], Val loss: 0.04031400382518768\n",
      "Epoch[0], Batch[2713], Train loss: 0.039108652621507645\n",
      "Epoch[0], Val loss: 0.03814707696437836\n",
      "Epoch[0], Batch[2714], Train loss: 0.044064901769161224\n",
      "Epoch[0], Val loss: 0.043526969850063324\n",
      "Epoch[0], Batch[2715], Train loss: 0.042546335607767105\n",
      "Epoch[0], Val loss: 0.039650313556194305\n",
      "Epoch[0], Batch[2716], Train loss: 0.04209022969007492\n",
      "Epoch[0], Val loss: 0.04017942398786545\n",
      "Epoch[0], Batch[2717], Train loss: 0.04298317804932594\n",
      "Epoch[0], Val loss: 0.0414600595831871\n",
      "Epoch[0], Batch[2718], Train loss: 0.044827770441770554\n",
      "Epoch[0], Val loss: 0.03880872577428818\n",
      "Epoch[0], Batch[2719], Train loss: 0.04215960204601288\n",
      "Epoch[0], Val loss: 0.039272092282772064\n",
      "Epoch[0], Batch[2720], Train loss: 0.04138810560107231\n",
      "Epoch[0], Val loss: 0.0377572737634182\n",
      "Epoch[0], Batch[2721], Train loss: 0.044204991310834885\n",
      "Epoch[0], Val loss: 0.04137561470270157\n",
      "Epoch[0], Batch[2722], Train loss: 0.0433022566139698\n",
      "Epoch[0], Val loss: 0.03966499865055084\n",
      "Epoch[0], Batch[2723], Train loss: 0.04054190218448639\n",
      "Epoch[0], Val loss: 0.040707435458898544\n",
      "Epoch[0], Batch[2724], Train loss: 0.04434159770607948\n",
      "Epoch[0], Val loss: 0.04081280156970024\n",
      "Epoch[0], Batch[2725], Train loss: 0.039920542389154434\n",
      "Epoch[0], Val loss: 0.0429849699139595\n",
      "Epoch[0], Batch[2726], Train loss: 0.04554208740592003\n",
      "Epoch[0], Val loss: 0.04061444476246834\n",
      "Epoch[0], Batch[2727], Train loss: 0.0427207387983799\n",
      "Epoch[0], Val loss: 0.04002636671066284\n",
      "Epoch[0], Batch[2728], Train loss: 0.04202127829194069\n",
      "Epoch[0], Val loss: 0.0398319810628891\n",
      "Epoch[0], Batch[2729], Train loss: 0.0396561324596405\n",
      "Epoch[0], Val loss: 0.04325907304883003\n",
      "Epoch[0], Batch[2730], Train loss: 0.04277941584587097\n",
      "Epoch[0], Val loss: 0.04203023016452789\n",
      "Epoch[0], Batch[2731], Train loss: 0.043861933052539825\n",
      "Epoch[0], Val loss: 0.041199054569005966\n",
      "Epoch[0], Batch[2732], Train loss: 0.039070580154657364\n",
      "Epoch[0], Val loss: 0.039456434547901154\n",
      "Epoch[0], Batch[2733], Train loss: 0.04318603500723839\n",
      "Epoch[0], Val loss: 0.04120684787631035\n",
      "Epoch[0], Batch[2734], Train loss: 0.041009675711393356\n",
      "Epoch[0], Val loss: 0.039574164897203445\n",
      "Epoch[0], Batch[2735], Train loss: 0.040833111852407455\n",
      "Epoch[0], Val loss: 0.04080509766936302\n",
      "Epoch[0], Batch[2736], Train loss: 0.04438268020749092\n",
      "Epoch[0], Val loss: 0.04064159095287323\n",
      "Epoch[0], Batch[2737], Train loss: 0.043765194714069366\n",
      "Epoch[0], Val loss: 0.039183493703603745\n",
      "Epoch[0], Batch[2738], Train loss: 0.04506351053714752\n",
      "Epoch[0], Val loss: 0.03890402987599373\n",
      "Epoch[0], Batch[2739], Train loss: 0.04682379961013794\n",
      "Epoch[0], Val loss: 0.041329525411129\n",
      "Epoch[0], Batch[2740], Train loss: 0.04734223708510399\n",
      "Epoch[0], Val loss: 0.040396299213171005\n",
      "Epoch[0], Batch[2741], Train loss: 0.04380767419934273\n",
      "Epoch[0], Val loss: 0.03934791684150696\n",
      "Epoch[0], Batch[2742], Train loss: 0.04211200773715973\n",
      "Epoch[0], Val loss: 0.040412500500679016\n",
      "Epoch[0], Batch[2743], Train loss: 0.043489038944244385\n",
      "Epoch[0], Val loss: 0.039180319756269455\n",
      "Epoch[0], Batch[2744], Train loss: 0.042743995785713196\n",
      "Epoch[0], Val loss: 0.041697319597005844\n",
      "Epoch[0], Batch[2745], Train loss: 0.040898922830820084\n",
      "Epoch[0], Val loss: 0.038136210292577744\n",
      "Epoch[0], Batch[2746], Train loss: 0.04286793991923332\n",
      "Epoch[0], Val loss: 0.03808748722076416\n",
      "Epoch[0], Batch[2747], Train loss: 0.04152589663863182\n",
      "Epoch[0], Val loss: 0.04050176218152046\n",
      "Epoch[0], Batch[2748], Train loss: 0.0405796617269516\n",
      "Epoch[0], Val loss: 0.03842257708311081\n",
      "Epoch[0], Batch[2749], Train loss: 0.04118133708834648\n",
      "Epoch[0], Val loss: 0.0433117151260376\n",
      "Epoch[0], Batch[2750], Train loss: 0.04162447899580002\n",
      "Epoch[0], Val loss: 0.04198075458407402\n",
      "Epoch[0], Batch[2751], Train loss: 0.0423172265291214\n",
      "Epoch[0], Val loss: 0.03863854706287384\n",
      "Epoch[0], Batch[2752], Train loss: 0.0430830679833889\n",
      "Epoch[0], Val loss: 0.04135970398783684\n",
      "Epoch[0], Batch[2753], Train loss: 0.04180439934134483\n",
      "Epoch[0], Val loss: 0.03819199651479721\n",
      "Epoch[0], Batch[2754], Train loss: 0.04109783470630646\n",
      "Epoch[0], Val loss: 0.0389704629778862\n",
      "Epoch[0], Batch[2755], Train loss: 0.03938253968954086\n",
      "Epoch[0], Val loss: 0.04096616804599762\n",
      "Epoch[0], Batch[2756], Train loss: 0.03960812836885452\n",
      "Epoch[0], Val loss: 0.039004597812891006\n",
      "Epoch[0], Batch[2757], Train loss: 0.040888186544179916\n",
      "Epoch[0], Val loss: 0.04057295620441437\n",
      "Epoch[0], Batch[2758], Train loss: 0.04032563790678978\n",
      "Epoch[0], Val loss: 0.041120268404483795\n",
      "Epoch[0], Batch[2759], Train loss: 0.04192310571670532\n",
      "Epoch[0], Val loss: 0.043410029262304306\n",
      "Epoch[0], Batch[2760], Train loss: 0.042777836322784424\n",
      "Epoch[0], Val loss: 0.041919030249118805\n",
      "Epoch[0], Batch[2761], Train loss: 0.03946724906563759\n",
      "Epoch[0], Val loss: 0.0397326834499836\n",
      "Epoch[0], Batch[2762], Train loss: 0.04357071965932846\n",
      "Epoch[0], Val loss: 0.042329736053943634\n",
      "Epoch[0], Batch[2763], Train loss: 0.04251950606703758\n",
      "Epoch[0], Val loss: 0.04029771685600281\n",
      "Epoch[0], Batch[2764], Train loss: 0.04051909223198891\n",
      "Epoch[0], Val loss: 0.03889639303088188\n",
      "Epoch[0], Batch[2765], Train loss: 0.04474809765815735\n",
      "Epoch[0], Val loss: 0.04083651304244995\n",
      "Epoch[0], Batch[2766], Train loss: 0.0394040085375309\n",
      "Epoch[0], Val loss: 0.04265758767724037\n",
      "Epoch[0], Batch[2767], Train loss: 0.041588205844163895\n",
      "Epoch[0], Val loss: 0.040299054235219955\n",
      "Epoch[0], Batch[2768], Train loss: 0.0440652035176754\n",
      "Epoch[0], Val loss: 0.03944045677781105\n",
      "Epoch[0], Batch[2769], Train loss: 0.04007670655846596\n",
      "Epoch[0], Val loss: 0.040872491896152496\n",
      "Epoch[0], Batch[2770], Train loss: 0.042018819600343704\n",
      "Epoch[0], Val loss: 0.038892727345228195\n",
      "Epoch[0], Batch[2771], Train loss: 0.040828365832567215\n",
      "Epoch[0], Val loss: 0.03951669856905937\n",
      "Epoch[0], Batch[2772], Train loss: 0.042488452047109604\n",
      "Epoch[0], Val loss: 0.04117298498749733\n",
      "Epoch[0], Batch[2773], Train loss: 0.04116165265440941\n",
      "Epoch[0], Val loss: 0.03998769447207451\n",
      "Epoch[0], Batch[2774], Train loss: 0.04541569575667381\n",
      "Epoch[0], Val loss: 0.03837085887789726\n",
      "Epoch[0], Batch[2775], Train loss: 0.040702130645513535\n",
      "Epoch[0], Val loss: 0.03824980929493904\n",
      "Epoch[0], Batch[2776], Train loss: 0.04198198392987251\n",
      "Epoch[0], Val loss: 0.04503302648663521\n",
      "Epoch[0], Batch[2777], Train loss: 0.04247107729315758\n",
      "Epoch[0], Val loss: 0.04203568026423454\n",
      "Epoch[0], Batch[2778], Train loss: 0.04107967019081116\n",
      "Epoch[0], Val loss: 0.0432581752538681\n",
      "Epoch[0], Batch[2779], Train loss: 0.04106102138757706\n",
      "Epoch[0], Val loss: 0.04081249237060547\n",
      "Epoch[0], Batch[2780], Train loss: 0.04118704795837402\n",
      "Epoch[0], Val loss: 0.039160117506980896\n",
      "Epoch[0], Batch[2781], Train loss: 0.04018376022577286\n",
      "Epoch[0], Val loss: 0.03974631801247597\n",
      "Epoch[0], Batch[2782], Train loss: 0.04256497323513031\n",
      "Epoch[0], Val loss: 0.03897039219737053\n",
      "Epoch[0], Batch[2783], Train loss: 0.04396235942840576\n",
      "Epoch[0], Val loss: 0.03939250111579895\n",
      "Epoch[0], Batch[2784], Train loss: 0.04180334508419037\n",
      "Epoch[0], Val loss: 0.04203260317444801\n",
      "Epoch[0], Batch[2785], Train loss: 0.041957397013902664\n",
      "Epoch[0], Val loss: 0.04073890298604965\n",
      "Epoch[0], Batch[2786], Train loss: 0.04112069681286812\n",
      "Epoch[0], Val loss: 0.04255463555455208\n",
      "Epoch[0], Batch[2787], Train loss: 0.039599694311618805\n",
      "Epoch[0], Val loss: 0.04316333308815956\n",
      "Epoch[0], Batch[2788], Train loss: 0.04394296184182167\n",
      "Epoch[0], Val loss: 0.03989420831203461\n",
      "Epoch[0], Batch[2789], Train loss: 0.04534018784761429\n",
      "Epoch[0], Val loss: 0.04339273273944855\n",
      "Epoch[0], Batch[2790], Train loss: 0.044456761330366135\n",
      "Epoch[0], Val loss: 0.04129587486386299\n",
      "Epoch[0], Batch[2791], Train loss: 0.044996678829193115\n",
      "Epoch[0], Val loss: 0.04355039820075035\n",
      "Epoch[0], Batch[2792], Train loss: 0.04423084110021591\n",
      "Epoch[0], Val loss: 0.03997485712170601\n",
      "Epoch[0], Batch[2793], Train loss: 0.0425809770822525\n",
      "Epoch[0], Val loss: 0.04126983880996704\n",
      "Epoch[0], Batch[2794], Train loss: 0.04091551527380943\n",
      "Epoch[0], Val loss: 0.039210960268974304\n",
      "Epoch[0], Batch[2795], Train loss: 0.03764103353023529\n",
      "Epoch[0], Val loss: 0.044335003942251205\n",
      "Epoch[0], Batch[2796], Train loss: 0.03995545580983162\n",
      "Epoch[0], Val loss: 0.04510284215211868\n",
      "Epoch[0], Batch[2797], Train loss: 0.0430896133184433\n",
      "Epoch[0], Val loss: 0.04195486381649971\n",
      "Epoch[0], Batch[2798], Train loss: 0.04358344152569771\n",
      "Epoch[0], Val loss: 0.040851891040802\n",
      "Epoch[0], Batch[2799], Train loss: 0.041377950459718704\n",
      "Epoch[0], Val loss: 0.041892681270837784\n",
      "Epoch[0], Batch[2800], Train loss: 0.04194450005888939\n",
      "Epoch[0], Val loss: 0.03863101825118065\n",
      "Epoch[0], Batch[2801], Train loss: 0.042753733694553375\n",
      "Epoch[0], Val loss: 0.03893307223916054\n",
      "Epoch[0], Batch[2802], Train loss: 0.042607929557561874\n",
      "Epoch[0], Val loss: 0.04067354276776314\n",
      "Epoch[0], Batch[2803], Train loss: 0.04268079623579979\n",
      "Epoch[0], Val loss: 0.040209442377090454\n",
      "Epoch[0], Batch[2804], Train loss: 0.042050160467624664\n",
      "Epoch[0], Val loss: 0.040138937532901764\n",
      "Epoch[0], Batch[2805], Train loss: 0.04170844703912735\n",
      "Epoch[0], Val loss: 0.04106372222304344\n",
      "Epoch[0], Batch[2806], Train loss: 0.0406835675239563\n",
      "Epoch[0], Val loss: 0.04195034131407738\n",
      "Epoch[0], Batch[2807], Train loss: 0.041048891842365265\n",
      "Epoch[0], Val loss: 0.041550375521183014\n",
      "Epoch[0], Batch[2808], Train loss: 0.041956253349781036\n",
      "Epoch[0], Val loss: 0.03946957364678383\n",
      "Epoch[0], Batch[2809], Train loss: 0.04051026701927185\n",
      "Epoch[0], Val loss: 0.037618160247802734\n",
      "Epoch[0], Batch[2810], Train loss: 0.038950543850660324\n",
      "Epoch[0], Val loss: 0.03933834284543991\n",
      "Epoch[0], Batch[2811], Train loss: 0.04439925029873848\n",
      "Epoch[0], Val loss: 0.03909768909215927\n",
      "Epoch[0], Batch[2812], Train loss: 0.041274573653936386\n",
      "Epoch[0], Val loss: 0.03843802586197853\n",
      "Epoch[0], Batch[2813], Train loss: 0.039888910949230194\n",
      "Epoch[0], Val loss: 0.040521055459976196\n",
      "Epoch[0], Batch[2814], Train loss: 0.041326045989990234\n",
      "Epoch[0], Val loss: 0.038453537970781326\n",
      "Epoch[0], Batch[2815], Train loss: 0.03914013132452965\n",
      "Epoch[0], Val loss: 0.039681777358055115\n",
      "Epoch[0], Batch[2816], Train loss: 0.04247806966304779\n",
      "Epoch[0], Val loss: 0.0439332090318203\n",
      "Epoch[0], Batch[2817], Train loss: 0.04011871665716171\n",
      "Epoch[0], Val loss: 0.04123331233859062\n",
      "Epoch[0], Batch[2818], Train loss: 0.04175257682800293\n",
      "Epoch[0], Val loss: 0.03894693776965141\n",
      "Epoch[0], Batch[2819], Train loss: 0.04202037677168846\n",
      "Epoch[0], Val loss: 0.03783312439918518\n",
      "Epoch[0], Batch[2820], Train loss: 0.040502339601516724\n",
      "Epoch[0], Val loss: 0.043503470718860626\n",
      "Epoch[0], Batch[2821], Train loss: 0.04183195158839226\n",
      "Epoch[0], Val loss: 0.04014462232589722\n",
      "Epoch[0], Batch[2822], Train loss: 0.03966137394309044\n",
      "Epoch[0], Val loss: 0.03979594632983208\n",
      "Epoch[0], Batch[2823], Train loss: 0.041819360107183456\n",
      "Epoch[0], Val loss: 0.03898156061768532\n",
      "Epoch[0], Batch[2824], Train loss: 0.041753314435482025\n",
      "Epoch[0], Val loss: 0.03980638459324837\n",
      "Epoch[0], Batch[2825], Train loss: 0.04164067283272743\n",
      "Epoch[0], Val loss: 0.03997601568698883\n",
      "Epoch[0], Batch[2826], Train loss: 0.0441729910671711\n",
      "Epoch[0], Val loss: 0.038424570113420486\n",
      "Epoch[0], Batch[2827], Train loss: 0.043841272592544556\n",
      "Epoch[0], Val loss: 0.03991977870464325\n",
      "Epoch[0], Batch[2828], Train loss: 0.040953461080789566\n",
      "Epoch[0], Val loss: 0.0416950061917305\n",
      "Epoch[0], Batch[2829], Train loss: 0.042415931820869446\n",
      "Epoch[0], Val loss: 0.04061993584036827\n",
      "Epoch[0], Batch[2830], Train loss: 0.041367318481206894\n",
      "Epoch[0], Val loss: 0.03987615555524826\n",
      "Epoch[0], Batch[2831], Train loss: 0.04400995746254921\n",
      "Epoch[0], Val loss: 0.040214311331510544\n",
      "Epoch[0], Batch[2832], Train loss: 0.03965846449136734\n",
      "Epoch[0], Val loss: 0.04062410444021225\n",
      "Epoch[0], Batch[2833], Train loss: 0.04025186598300934\n",
      "Epoch[0], Val loss: 0.040974561125040054\n",
      "Epoch[0], Batch[2834], Train loss: 0.040382690727710724\n",
      "Epoch[0], Val loss: 0.04007532075047493\n",
      "Epoch[0], Batch[2835], Train loss: 0.04207920655608177\n",
      "Epoch[0], Val loss: 0.04129406809806824\n",
      "Epoch[0], Batch[2836], Train loss: 0.04218685254454613\n",
      "Epoch[0], Val loss: 0.0393102653324604\n",
      "Epoch[0], Batch[2837], Train loss: 0.04193897172808647\n",
      "Epoch[0], Val loss: 0.040862735360860825\n",
      "Epoch[0], Batch[2838], Train loss: 0.04109341651201248\n",
      "Epoch[0], Val loss: 0.036794405430555344\n",
      "Epoch[0], Batch[2839], Train loss: 0.04218955338001251\n",
      "Epoch[0], Val loss: 0.04314442723989487\n",
      "Epoch[0], Batch[2840], Train loss: 0.042113762348890305\n",
      "Epoch[0], Val loss: 0.038490179926157\n",
      "Epoch[0], Batch[2841], Train loss: 0.04015020653605461\n",
      "Epoch[0], Val loss: 0.03864904120564461\n",
      "Epoch[0], Batch[2842], Train loss: 0.042046528309583664\n",
      "Epoch[0], Val loss: 0.04076627269387245\n",
      "Epoch[0], Batch[2843], Train loss: 0.042390499264001846\n",
      "Epoch[0], Val loss: 0.03945060074329376\n",
      "Epoch[0], Batch[2844], Train loss: 0.04162776842713356\n",
      "Epoch[0], Val loss: 0.03693199157714844\n",
      "Epoch[0], Batch[2845], Train loss: 0.04319032281637192\n",
      "Epoch[0], Val loss: 0.04093365743756294\n",
      "Epoch[0], Batch[2846], Train loss: 0.039478473365306854\n",
      "Epoch[0], Val loss: 0.03982401266694069\n",
      "Epoch[0], Batch[2847], Train loss: 0.040650445967912674\n",
      "Epoch[0], Val loss: 0.04010690376162529\n",
      "Epoch[0], Batch[2848], Train loss: 0.04027549549937248\n",
      "Epoch[0], Val loss: 0.04045873135328293\n",
      "Epoch[0], Batch[2849], Train loss: 0.039930298924446106\n",
      "Epoch[0], Val loss: 0.03920392692089081\n",
      "Epoch[0], Batch[2850], Train loss: 0.04051414504647255\n",
      "Epoch[0], Val loss: 0.040645137429237366\n",
      "Epoch[0], Batch[2851], Train loss: 0.04014357551932335\n",
      "Epoch[0], Val loss: 0.04011485353112221\n",
      "Epoch[0], Batch[2852], Train loss: 0.040214501321315765\n",
      "Epoch[0], Val loss: 0.03938177600502968\n",
      "Epoch[0], Batch[2853], Train loss: 0.04024834558367729\n",
      "Epoch[0], Val loss: 0.040088482201099396\n",
      "Epoch[0], Batch[2854], Train loss: 0.04257970303297043\n",
      "Epoch[0], Val loss: 0.039499714970588684\n",
      "Epoch[0], Batch[2855], Train loss: 0.03923889994621277\n",
      "Epoch[0], Val loss: 0.0389721505343914\n",
      "Epoch[0], Batch[2856], Train loss: 0.04059351235628128\n",
      "Epoch[0], Val loss: 0.04120320826768875\n",
      "Epoch[0], Batch[2857], Train loss: 0.04208057001233101\n",
      "Epoch[0], Val loss: 0.04021820053458214\n",
      "Epoch[0], Batch[2858], Train loss: 0.0404028445482254\n",
      "Epoch[0], Val loss: 0.039151642471551895\n",
      "Epoch[0], Batch[2859], Train loss: 0.04255804792046547\n",
      "Epoch[0], Val loss: 0.04007350653409958\n",
      "Epoch[0], Batch[2860], Train loss: 0.04231953248381615\n",
      "Epoch[0], Val loss: 0.040088918060064316\n",
      "Epoch[0], Batch[2861], Train loss: 0.041448209434747696\n",
      "Epoch[0], Val loss: 0.03794582188129425\n",
      "Epoch[0], Batch[2862], Train loss: 0.03990296646952629\n",
      "Epoch[0], Val loss: 0.038585495203733444\n",
      "Epoch[0], Batch[2863], Train loss: 0.04106986150145531\n",
      "Epoch[0], Val loss: 0.03947906568646431\n",
      "Epoch[0], Batch[2864], Train loss: 0.04348685219883919\n",
      "Epoch[0], Val loss: 0.04041309282183647\n",
      "Epoch[0], Batch[2865], Train loss: 0.04571177810430527\n",
      "Epoch[0], Val loss: 0.04154429957270622\n",
      "Epoch[0], Batch[2866], Train loss: 0.04380019009113312\n",
      "Epoch[0], Val loss: 0.038497574627399445\n",
      "Epoch[0], Batch[2867], Train loss: 0.03915278986096382\n",
      "Epoch[0], Val loss: 0.039448484778404236\n",
      "Epoch[0], Batch[2868], Train loss: 0.04165026918053627\n",
      "Epoch[0], Val loss: 0.038270190358161926\n",
      "Epoch[0], Batch[2869], Train loss: 0.041347358375787735\n",
      "Epoch[0], Val loss: 0.03904511034488678\n",
      "Epoch[0], Batch[2870], Train loss: 0.0382872000336647\n",
      "Epoch[0], Val loss: 0.037990227341651917\n",
      "Epoch[0], Batch[2871], Train loss: 0.04321916401386261\n",
      "Epoch[0], Val loss: 0.04052094370126724\n",
      "Epoch[0], Batch[2872], Train loss: 0.04055626317858696\n",
      "Epoch[0], Val loss: 0.03805325925350189\n",
      "Epoch[0], Batch[2873], Train loss: 0.042183566838502884\n",
      "Epoch[0], Val loss: 0.03955695405602455\n",
      "Epoch[0], Batch[2874], Train loss: 0.04017472639679909\n",
      "Epoch[0], Val loss: 0.03715399652719498\n",
      "Epoch[0], Batch[2875], Train loss: 0.04052450880408287\n",
      "Epoch[0], Val loss: 0.041203103959560394\n",
      "Epoch[0], Batch[2876], Train loss: 0.04035310074687004\n",
      "Epoch[0], Val loss: 0.039801303297281265\n",
      "Epoch[0], Batch[2877], Train loss: 0.0385453961789608\n",
      "Epoch[0], Val loss: 0.038139522075653076\n",
      "Epoch[0], Batch[2878], Train loss: 0.04206101968884468\n",
      "Epoch[0], Val loss: 0.03837151452898979\n",
      "Epoch[0], Batch[2879], Train loss: 0.0410180389881134\n",
      "Epoch[0], Val loss: 0.040721479803323746\n",
      "Epoch[0], Batch[2880], Train loss: 0.04537850245833397\n",
      "Epoch[0], Val loss: 0.03695234656333923\n",
      "Epoch[0], Batch[2881], Train loss: 0.04255221039056778\n",
      "Epoch[0], Val loss: 0.03998059406876564\n",
      "Epoch[0], Batch[2882], Train loss: 0.04322641342878342\n",
      "Epoch[0], Val loss: 0.04117848724126816\n",
      "Epoch[0], Batch[2883], Train loss: 0.041290607303380966\n",
      "Epoch[0], Val loss: 0.04084620252251625\n",
      "Epoch[0], Batch[2884], Train loss: 0.039549801498651505\n",
      "Epoch[0], Val loss: 0.04031912237405777\n",
      "Epoch[0], Batch[2885], Train loss: 0.040606699883937836\n",
      "Epoch[0], Val loss: 0.0400654673576355\n",
      "Epoch[0], Batch[2886], Train loss: 0.04351392388343811\n",
      "Epoch[0], Val loss: 0.038583703339099884\n",
      "Epoch[0], Batch[2887], Train loss: 0.0413595512509346\n",
      "Epoch[0], Val loss: 0.03819065913558006\n",
      "Epoch[0], Batch[2888], Train loss: 0.03988291323184967\n",
      "Epoch[0], Val loss: 0.038038089871406555\n",
      "Epoch[0], Batch[2889], Train loss: 0.04178726673126221\n",
      "Epoch[0], Val loss: 0.039744701236486435\n",
      "Epoch[0], Batch[2890], Train loss: 0.04162701591849327\n",
      "Epoch[0], Val loss: 0.038156695663928986\n",
      "Epoch[0], Batch[2891], Train loss: 0.0420260988175869\n",
      "Epoch[0], Val loss: 0.03879227489233017\n",
      "Epoch[0], Batch[2892], Train loss: 0.0405079610645771\n",
      "Epoch[0], Val loss: 0.0360223613679409\n",
      "Epoch[0], Batch[2893], Train loss: 0.04066874831914902\n",
      "Epoch[0], Val loss: 0.0373237170279026\n",
      "Epoch[0], Batch[2894], Train loss: 0.042121585458517075\n",
      "Epoch[0], Val loss: 0.037509769201278687\n",
      "Epoch[0], Batch[2895], Train loss: 0.04287104308605194\n",
      "Epoch[0], Val loss: 0.039002541452646255\n",
      "Epoch[0], Batch[2896], Train loss: 0.040881190448999405\n",
      "Epoch[0], Val loss: 0.038458291441202164\n",
      "Epoch[0], Batch[2897], Train loss: 0.04024391248822212\n",
      "Epoch[0], Val loss: 0.039749111980199814\n",
      "Epoch[0], Batch[2898], Train loss: 0.0399901457130909\n",
      "Epoch[0], Val loss: 0.03805490955710411\n",
      "Epoch[0], Batch[2899], Train loss: 0.03988947719335556\n",
      "Epoch[0], Val loss: 0.0385601632297039\n",
      "Epoch[0], Batch[2900], Train loss: 0.04358300194144249\n",
      "Epoch[0], Val loss: 0.03920869901776314\n",
      "Epoch[0], Batch[2901], Train loss: 0.03925958275794983\n",
      "Epoch[0], Val loss: 0.03881581872701645\n",
      "Epoch[0], Batch[2902], Train loss: 0.0437861867249012\n",
      "Epoch[0], Val loss: 0.04178902506828308\n",
      "Epoch[0], Batch[2903], Train loss: 0.042780861258506775\n",
      "Epoch[0], Val loss: 0.041476260870695114\n",
      "Epoch[0], Batch[2904], Train loss: 0.040172167122364044\n",
      "Epoch[0], Val loss: 0.03984244912862778\n",
      "Epoch[0], Batch[2905], Train loss: 0.0425795279443264\n",
      "Epoch[0], Val loss: 0.040807951241731644\n",
      "Epoch[0], Batch[2906], Train loss: 0.04079654440283775\n",
      "Epoch[0], Val loss: 0.03930724784731865\n",
      "Epoch[0], Batch[2907], Train loss: 0.03768336772918701\n",
      "Epoch[0], Val loss: 0.03890104591846466\n",
      "Epoch[0], Batch[2908], Train loss: 0.042064838111400604\n",
      "Epoch[0], Val loss: 0.037054393440485\n",
      "Epoch[0], Batch[2909], Train loss: 0.03969361260533333\n",
      "Epoch[0], Val loss: 0.03805196285247803\n",
      "Epoch[0], Batch[2910], Train loss: 0.04218479245901108\n",
      "Epoch[0], Val loss: 0.03875983506441116\n",
      "Epoch[0], Batch[2911], Train loss: 0.03986130282282829\n",
      "Epoch[0], Val loss: 0.03993130847811699\n",
      "Epoch[0], Batch[2912], Train loss: 0.04199163615703583\n",
      "Epoch[0], Val loss: 0.03836763650178909\n",
      "Epoch[0], Batch[2913], Train loss: 0.039327118545770645\n",
      "Epoch[0], Val loss: 0.039432793855667114\n",
      "Epoch[0], Batch[2914], Train loss: 0.04317047446966171\n",
      "Epoch[0], Val loss: 0.03936438634991646\n",
      "Epoch[0], Batch[2915], Train loss: 0.03933059051632881\n",
      "Epoch[0], Val loss: 0.039934493601322174\n",
      "Epoch[0], Batch[2916], Train loss: 0.04593617841601372\n",
      "Epoch[0], Val loss: 0.041304416954517365\n",
      "Epoch[0], Batch[2917], Train loss: 0.039940766990184784\n",
      "Epoch[0], Val loss: 0.03853355348110199\n",
      "Epoch[0], Batch[2918], Train loss: 0.03949004039168358\n",
      "Epoch[0], Val loss: 0.038424327969551086\n",
      "Epoch[0], Batch[2919], Train loss: 0.04167407378554344\n",
      "Epoch[0], Val loss: 0.04274480417370796\n",
      "Epoch[0], Batch[2920], Train loss: 0.03879912942647934\n",
      "Epoch[0], Val loss: 0.03968983516097069\n",
      "Epoch[0], Batch[2921], Train loss: 0.04198619723320007\n",
      "Epoch[0], Val loss: 0.04364481568336487\n",
      "Epoch[0], Batch[2922], Train loss: 0.03924628719687462\n",
      "Epoch[0], Val loss: 0.03865104913711548\n",
      "Epoch[0], Batch[2923], Train loss: 0.040507834404706955\n",
      "Epoch[0], Val loss: 0.03877570107579231\n",
      "Epoch[0], Batch[2924], Train loss: 0.03952576592564583\n",
      "Epoch[0], Val loss: 0.03934350237250328\n",
      "Epoch[0], Batch[2925], Train loss: 0.03983326628804207\n",
      "Epoch[0], Val loss: 0.039536669850349426\n",
      "Epoch[0], Batch[2926], Train loss: 0.04038708657026291\n",
      "Epoch[0], Val loss: 0.03639847785234451\n",
      "Epoch[0], Batch[2927], Train loss: 0.04360552132129669\n",
      "Epoch[0], Val loss: 0.039122652262449265\n",
      "Epoch[0], Batch[2928], Train loss: 0.03981664776802063\n",
      "Epoch[0], Val loss: 0.03974026441574097\n",
      "Epoch[0], Batch[2929], Train loss: 0.040537986904382706\n",
      "Epoch[0], Val loss: 0.03818749263882637\n",
      "Epoch[0], Batch[2930], Train loss: 0.04298809543251991\n",
      "Epoch[0], Val loss: 0.03814515471458435\n",
      "Epoch[0], Batch[2931], Train loss: 0.040704693645238876\n",
      "Epoch[0], Val loss: 0.041798703372478485\n",
      "Epoch[0], Batch[2932], Train loss: 0.041445422917604446\n",
      "Epoch[0], Val loss: 0.03967979922890663\n",
      "Epoch[0], Batch[2933], Train loss: 0.03825564682483673\n",
      "Epoch[0], Val loss: 0.03904589265584946\n",
      "Epoch[0], Batch[2934], Train loss: 0.03988628834486008\n",
      "Epoch[0], Val loss: 0.03845367580652237\n",
      "Epoch[0], Batch[2935], Train loss: 0.042213767766952515\n",
      "Epoch[0], Val loss: 0.03750842437148094\n",
      "Epoch[0], Batch[2936], Train loss: 0.039146389812231064\n",
      "Epoch[0], Val loss: 0.038581375032663345\n",
      "Epoch[0], Batch[2937], Train loss: 0.04125387594103813\n",
      "Epoch[0], Val loss: 0.03907901793718338\n",
      "Epoch[0], Batch[2938], Train loss: 0.04096511751413345\n",
      "Epoch[0], Val loss: 0.03770240396261215\n",
      "Epoch[0], Batch[2939], Train loss: 0.0378822460770607\n",
      "Epoch[0], Val loss: 0.041680775582790375\n",
      "Epoch[0], Batch[2940], Train loss: 0.03879546374082565\n",
      "Epoch[0], Val loss: 0.03939694166183472\n",
      "Epoch[0], Batch[2941], Train loss: 0.041374415159225464\n",
      "Epoch[0], Val loss: 0.039161499589681625\n",
      "Epoch[0], Batch[2942], Train loss: 0.037841904908418655\n",
      "Epoch[0], Val loss: 0.039878807961940765\n",
      "Epoch[0], Batch[2943], Train loss: 0.040281232446432114\n",
      "Epoch[0], Val loss: 0.03906989470124245\n",
      "Epoch[0], Batch[2944], Train loss: 0.04229511320590973\n",
      "Epoch[0], Val loss: 0.03783426433801651\n",
      "Epoch[0], Batch[2945], Train loss: 0.038763128221035004\n",
      "Epoch[0], Val loss: 0.03757432475686073\n",
      "Epoch[0], Batch[2946], Train loss: 0.04145783185958862\n",
      "Epoch[0], Val loss: 0.04112561419606209\n",
      "Epoch[0], Batch[2947], Train loss: 0.041733626276254654\n",
      "Epoch[0], Val loss: 0.04029536247253418\n",
      "Epoch[0], Batch[2948], Train loss: 0.04147011414170265\n",
      "Epoch[0], Val loss: 0.04082619771361351\n",
      "Epoch[0], Batch[2949], Train loss: 0.039934273809194565\n",
      "Epoch[0], Val loss: 0.04105941578745842\n",
      "Epoch[0], Batch[2950], Train loss: 0.03961857408285141\n",
      "Epoch[0], Val loss: 0.04082278162240982\n",
      "Epoch[0], Batch[2951], Train loss: 0.04298785328865051\n",
      "Epoch[0], Val loss: 0.03747254237532616\n",
      "Epoch[0], Batch[2952], Train loss: 0.042534083127975464\n",
      "Epoch[0], Val loss: 0.03858409821987152\n",
      "Epoch[0], Batch[2953], Train loss: 0.04279216751456261\n",
      "Epoch[0], Val loss: 0.03900200501084328\n",
      "Epoch[0], Batch[2954], Train loss: 0.0419328473508358\n",
      "Epoch[0], Val loss: 0.038654785603284836\n",
      "Epoch[0], Batch[2955], Train loss: 0.041596073657274246\n",
      "Epoch[0], Val loss: 0.03685741126537323\n",
      "Epoch[0], Batch[2956], Train loss: 0.04165240004658699\n",
      "Epoch[0], Val loss: 0.039176102727651596\n",
      "Epoch[0], Batch[2957], Train loss: 0.03922177478671074\n",
      "Epoch[0], Val loss: 0.03840386122465134\n",
      "Epoch[0], Batch[2958], Train loss: 0.04055409133434296\n",
      "Epoch[0], Val loss: 0.04204292222857475\n",
      "Epoch[0], Batch[2959], Train loss: 0.03773655742406845\n",
      "Epoch[0], Val loss: 0.039723917841911316\n",
      "Epoch[0], Batch[2960], Train loss: 0.04151874780654907\n",
      "Epoch[0], Val loss: 0.038843780755996704\n",
      "Epoch[0], Batch[2961], Train loss: 0.041793957352638245\n",
      "Epoch[0], Val loss: 0.04056999459862709\n",
      "Epoch[0], Batch[2962], Train loss: 0.04144354537129402\n",
      "Epoch[0], Val loss: 0.0399324856698513\n",
      "Epoch[0], Batch[2963], Train loss: 0.0415823794901371\n",
      "Epoch[0], Val loss: 0.038832370191812515\n",
      "Epoch[0], Batch[2964], Train loss: 0.04189665615558624\n",
      "Epoch[0], Val loss: 0.03797650337219238\n",
      "Epoch[0], Batch[2965], Train loss: 0.04068256542086601\n",
      "Epoch[0], Val loss: 0.039148006588220596\n",
      "Epoch[0], Batch[2966], Train loss: 0.03934701532125473\n",
      "Epoch[0], Val loss: 0.04042992368340492\n",
      "Epoch[0], Batch[2967], Train loss: 0.042124949395656586\n",
      "Epoch[0], Val loss: 0.03733605891466141\n",
      "Epoch[0], Batch[2968], Train loss: 0.040692754089832306\n",
      "Epoch[0], Val loss: 0.03688531741499901\n",
      "Epoch[0], Batch[2969], Train loss: 0.04195810854434967\n",
      "Epoch[0], Val loss: 0.04095953330397606\n",
      "Epoch[0], Batch[2970], Train loss: 0.04105457291007042\n",
      "Epoch[0], Val loss: 0.037450894713401794\n",
      "Epoch[0], Batch[2971], Train loss: 0.04021668806672096\n",
      "Epoch[0], Val loss: 0.03618023172020912\n",
      "Epoch[0], Batch[2972], Train loss: 0.04253363609313965\n",
      "Epoch[0], Val loss: 0.039691437035799026\n",
      "Epoch[0], Batch[2973], Train loss: 0.04267282038927078\n",
      "Epoch[0], Val loss: 0.0418003611266613\n",
      "Epoch[0], Batch[2974], Train loss: 0.04079001396894455\n",
      "Epoch[0], Val loss: 0.03660018742084503\n",
      "Epoch[0], Batch[2975], Train loss: 0.040817879140377045\n",
      "Epoch[0], Val loss: 0.037600837647914886\n",
      "Epoch[0], Batch[2976], Train loss: 0.03945201262831688\n",
      "Epoch[0], Val loss: 0.03919919580221176\n",
      "Epoch[0], Batch[2977], Train loss: 0.03946319594979286\n",
      "Epoch[0], Val loss: 0.038715463131666183\n",
      "Epoch[0], Batch[2978], Train loss: 0.040737301111221313\n",
      "Epoch[0], Val loss: 0.03992321714758873\n",
      "Epoch[0], Batch[2979], Train loss: 0.04276289418339729\n",
      "Epoch[0], Val loss: 0.03956522420048714\n",
      "Epoch[0], Batch[2980], Train loss: 0.03888923302292824\n",
      "Epoch[0], Val loss: 0.03989449143409729\n",
      "Epoch[0], Batch[2981], Train loss: 0.0393235944211483\n",
      "Epoch[0], Val loss: 0.036528874188661575\n",
      "Epoch[0], Batch[2982], Train loss: 0.03948945924639702\n",
      "Epoch[0], Val loss: 0.03695191442966461\n",
      "Epoch[0], Batch[2983], Train loss: 0.036540646106004715\n",
      "Epoch[0], Val loss: 0.0412815697491169\n",
      "Epoch[0], Batch[2984], Train loss: 0.04053012654185295\n",
      "Epoch[0], Val loss: 0.04714387282729149\n",
      "Epoch[0], Batch[2985], Train loss: 0.043356649577617645\n",
      "Epoch[0], Val loss: 0.037630967795848846\n",
      "Epoch[0], Batch[2986], Train loss: 0.04059472680091858\n",
      "Epoch[0], Val loss: 0.038144733756780624\n",
      "Epoch[0], Batch[2987], Train loss: 0.039081599563360214\n",
      "Epoch[0], Val loss: 0.03850017115473747\n",
      "Epoch[0], Batch[2988], Train loss: 0.04159369319677353\n",
      "Epoch[0], Val loss: 0.03785530477762222\n",
      "Epoch[0], Batch[2989], Train loss: 0.04087819531559944\n",
      "Epoch[0], Val loss: 0.03855522722005844\n",
      "Epoch[0], Batch[2990], Train loss: 0.04366038367152214\n",
      "Epoch[0], Val loss: 0.03936583176255226\n",
      "Epoch[0], Batch[2991], Train loss: 0.04119037091732025\n",
      "Epoch[0], Val loss: 0.03893105685710907\n",
      "Epoch[0], Batch[2992], Train loss: 0.038722921162843704\n",
      "Epoch[0], Val loss: 0.03994501754641533\n",
      "Epoch[0], Batch[2993], Train loss: 0.04006621614098549\n",
      "Epoch[0], Val loss: 0.03949199616909027\n",
      "Epoch[0], Batch[2994], Train loss: 0.04146162420511246\n",
      "Epoch[0], Val loss: 0.036161430180072784\n",
      "Epoch[0], Batch[2995], Train loss: 0.037980128079652786\n",
      "Epoch[0], Val loss: 0.04260021448135376\n",
      "Epoch[0], Batch[2996], Train loss: 0.04097304120659828\n",
      "Epoch[0], Val loss: 0.03815822675824165\n",
      "Epoch[0], Batch[2997], Train loss: 0.0415729321539402\n",
      "Epoch[0], Val loss: 0.03929436579346657\n",
      "Epoch[0], Batch[2998], Train loss: 0.04029248282313347\n",
      "Epoch[0], Val loss: 0.038938868790864944\n",
      "Epoch[0], Batch[2999], Train loss: 0.04122423008084297\n",
      "Epoch[0], Val loss: 0.03830672800540924\n",
      "Epoch[0], Batch[3000], Train loss: 0.0399179682135582\n",
      "Epoch[0], Val loss: 0.038342271000146866\n",
      "Epoch[0], Batch[3001], Train loss: 0.0374571867287159\n",
      "Epoch[0], Val loss: 0.03885563090443611\n",
      "Epoch[0], Batch[3002], Train loss: 0.04139244183897972\n",
      "Epoch[0], Val loss: 0.03856963664293289\n",
      "Epoch[0], Batch[3003], Train loss: 0.04081549867987633\n",
      "Epoch[0], Val loss: 0.03969964012503624\n",
      "Epoch[0], Batch[3004], Train loss: 0.038990601897239685\n",
      "Epoch[0], Val loss: 0.04032019153237343\n",
      "Epoch[0], Batch[3005], Train loss: 0.03833799064159393\n",
      "Epoch[0], Val loss: 0.038773052394390106\n",
      "Epoch[0], Batch[3006], Train loss: 0.03840017318725586\n",
      "Epoch[0], Val loss: 0.03996971994638443\n",
      "Epoch[0], Batch[3007], Train loss: 0.04035954922437668\n",
      "Epoch[0], Val loss: 0.04182596877217293\n",
      "Epoch[0], Batch[3008], Train loss: 0.04210438206791878\n",
      "Epoch[0], Val loss: 0.03797585889697075\n",
      "Epoch[0], Batch[3009], Train loss: 0.038769129663705826\n",
      "Epoch[0], Val loss: 0.03575020655989647\n",
      "Epoch[0], Batch[3010], Train loss: 0.04007324203848839\n",
      "Epoch[0], Val loss: 0.04201198369264603\n",
      "Epoch[0], Batch[3011], Train loss: 0.04101478308439255\n",
      "Epoch[0], Val loss: 0.03948846459388733\n",
      "Epoch[0], Batch[3012], Train loss: 0.04113955795764923\n",
      "Epoch[0], Val loss: 0.041899263858795166\n",
      "Epoch[0], Batch[3013], Train loss: 0.04014158248901367\n",
      "Epoch[0], Val loss: 0.04038139432668686\n",
      "Epoch[0], Batch[3014], Train loss: 0.0404348187148571\n",
      "Epoch[0], Val loss: 0.0365774892270565\n",
      "Epoch[0], Batch[3015], Train loss: 0.04020466282963753\n",
      "Epoch[0], Val loss: 0.03963738679885864\n",
      "Epoch[0], Batch[3016], Train loss: 0.041802242398262024\n",
      "Epoch[0], Val loss: 0.037144411355257034\n",
      "Epoch[0], Batch[3017], Train loss: 0.04154522344470024\n",
      "Epoch[0], Val loss: 0.03857581317424774\n",
      "Epoch[0], Batch[3018], Train loss: 0.044457025825977325\n",
      "Epoch[0], Val loss: 0.03913474828004837\n",
      "Epoch[0], Batch[3019], Train loss: 0.04189698398113251\n",
      "Epoch[0], Val loss: 0.041142694652080536\n",
      "Epoch[0], Batch[3020], Train loss: 0.03840500861406326\n",
      "Epoch[0], Val loss: 0.03669958561658859\n",
      "Epoch[0], Batch[3021], Train loss: 0.04204469919204712\n",
      "Epoch[0], Val loss: 0.04007112979888916\n",
      "Epoch[0], Batch[3022], Train loss: 0.04129816219210625\n",
      "Epoch[0], Val loss: 0.037370774894952774\n",
      "Epoch[0], Batch[3023], Train loss: 0.04253442585468292\n",
      "Epoch[0], Val loss: 0.038287390023469925\n",
      "Epoch[0], Batch[3024], Train loss: 0.04233650863170624\n",
      "Epoch[0], Val loss: 0.039069246500730515\n",
      "Epoch[0], Batch[3025], Train loss: 0.04140409082174301\n",
      "Epoch[0], Val loss: 0.03935271129012108\n",
      "Epoch[0], Batch[3026], Train loss: 0.04071469604969025\n",
      "Epoch[0], Val loss: 0.0407828763127327\n",
      "Epoch[0], Batch[3027], Train loss: 0.039163343608379364\n",
      "Epoch[0], Val loss: 0.03717184439301491\n",
      "Epoch[0], Batch[3028], Train loss: 0.041172176599502563\n",
      "Epoch[0], Val loss: 0.03986555337905884\n",
      "Epoch[0], Batch[3029], Train loss: 0.038814835250377655\n",
      "Epoch[0], Val loss: 0.0403163842856884\n",
      "Epoch[0], Batch[3030], Train loss: 0.03841939941048622\n",
      "Epoch[0], Val loss: 0.03944462910294533\n",
      "Epoch[0], Batch[3031], Train loss: 0.03957369923591614\n",
      "Epoch[0], Val loss: 0.0389791876077652\n",
      "Epoch[0], Batch[3032], Train loss: 0.041301511228084564\n",
      "Epoch[0], Val loss: 0.03844822943210602\n",
      "Epoch[0], Batch[3033], Train loss: 0.04313967004418373\n",
      "Epoch[0], Val loss: 0.04062439873814583\n",
      "Epoch[0], Batch[3034], Train loss: 0.04154123365879059\n",
      "Epoch[0], Val loss: 0.036768823862075806\n",
      "Epoch[0], Batch[3035], Train loss: 0.04167354479432106\n",
      "Epoch[0], Val loss: 0.03512716293334961\n",
      "Epoch[0], Batch[3036], Train loss: 0.04075419530272484\n",
      "Epoch[0], Val loss: 0.0379793606698513\n",
      "Epoch[0], Batch[3037], Train loss: 0.0386907197535038\n",
      "Epoch[0], Val loss: 0.03881588578224182\n",
      "Epoch[0], Batch[3038], Train loss: 0.04218079894781113\n",
      "Epoch[0], Val loss: 0.03764072805643082\n",
      "Epoch[0], Batch[3039], Train loss: 0.042638324201107025\n",
      "Epoch[0], Val loss: 0.03865149989724159\n",
      "Epoch[0], Batch[3040], Train loss: 0.041838712990283966\n",
      "Epoch[0], Val loss: 0.03713477775454521\n",
      "Epoch[0], Batch[3041], Train loss: 0.03879430890083313\n",
      "Epoch[0], Val loss: 0.038964297622442245\n",
      "Epoch[0], Batch[3042], Train loss: 0.04083835333585739\n",
      "Epoch[0], Val loss: 0.0385390967130661\n",
      "Epoch[0], Batch[3043], Train loss: 0.04205382987856865\n",
      "Epoch[0], Val loss: 0.03887149319052696\n",
      "Epoch[0], Batch[3044], Train loss: 0.042682766914367676\n",
      "Epoch[0], Val loss: 0.04234654828906059\n",
      "Epoch[0], Batch[3045], Train loss: 0.042959243059158325\n",
      "Epoch[0], Val loss: 0.038432516157627106\n",
      "Epoch[0], Batch[3046], Train loss: 0.03965076804161072\n",
      "Epoch[0], Val loss: 0.03636186942458153\n",
      "Epoch[0], Batch[3047], Train loss: 0.0400896891951561\n",
      "Epoch[0], Val loss: 0.03827129304409027\n",
      "Epoch[0], Batch[3048], Train loss: 0.03942699730396271\n",
      "Epoch[0], Val loss: 0.038787223398685455\n",
      "Epoch[0], Batch[3049], Train loss: 0.03924831375479698\n",
      "Epoch[0], Val loss: 0.03877490386366844\n",
      "Epoch[0], Batch[3050], Train loss: 0.042130935937166214\n",
      "Epoch[0], Val loss: 0.03694963827729225\n",
      "Epoch[0], Batch[3051], Train loss: 0.040991950780153275\n",
      "Epoch[0], Val loss: 0.036620378494262695\n",
      "Epoch[0], Batch[3052], Train loss: 0.0393935963511467\n",
      "Epoch[0], Val loss: 0.03734242171049118\n",
      "Epoch[0], Batch[3053], Train loss: 0.042341362684965134\n",
      "Epoch[0], Val loss: 0.03881088271737099\n",
      "Epoch[0], Batch[3054], Train loss: 0.040892984718084335\n",
      "Epoch[0], Val loss: 0.03604007139801979\n",
      "Epoch[0], Batch[3055], Train loss: 0.0425681546330452\n",
      "Epoch[0], Val loss: 0.03840657323598862\n",
      "Epoch[0], Batch[3056], Train loss: 0.0407092310488224\n",
      "Epoch[0], Val loss: 0.03791455924510956\n",
      "Epoch[0], Batch[3057], Train loss: 0.04134095460176468\n",
      "Epoch[0], Val loss: 0.03852713480591774\n",
      "Epoch[0], Batch[3058], Train loss: 0.03802800551056862\n",
      "Epoch[0], Val loss: 0.03708028048276901\n",
      "Epoch[0], Batch[3059], Train loss: 0.03876971825957298\n",
      "Epoch[0], Val loss: 0.040773455053567886\n",
      "Epoch[0], Batch[3060], Train loss: 0.03960470110177994\n",
      "Epoch[0], Val loss: 0.03732606768608093\n",
      "Epoch[0], Batch[3061], Train loss: 0.04068474844098091\n",
      "Epoch[0], Val loss: 0.0376654751598835\n",
      "Epoch[0], Batch[3062], Train loss: 0.04163353145122528\n",
      "Epoch[0], Val loss: 0.03611019253730774\n",
      "Epoch[0], Batch[3063], Train loss: 0.037482086569070816\n",
      "Epoch[0], Val loss: 0.037038885056972504\n",
      "Epoch[0], Batch[3064], Train loss: 0.03942873701453209\n",
      "Epoch[0], Val loss: 0.038597602397203445\n",
      "Epoch[0], Batch[3065], Train loss: 0.042165786027908325\n",
      "Epoch[0], Val loss: 0.036500051617622375\n",
      "Epoch[0], Batch[3066], Train loss: 0.03994080796837807\n",
      "Epoch[0], Val loss: 0.03934766352176666\n",
      "Epoch[0], Batch[3067], Train loss: 0.04147734120488167\n",
      "Epoch[0], Val loss: 0.039638299494981766\n",
      "Epoch[0], Batch[3068], Train loss: 0.040493194013834\n",
      "Epoch[0], Val loss: 0.038588572293519974\n",
      "Epoch[0], Batch[3069], Train loss: 0.03901618346571922\n",
      "Epoch[0], Val loss: 0.04145084694027901\n",
      "Epoch[0], Batch[3070], Train loss: 0.038501329720020294\n",
      "Epoch[0], Val loss: 0.03819514438509941\n",
      "Epoch[0], Batch[3071], Train loss: 0.04121822863817215\n",
      "Epoch[0], Val loss: 0.03810027614235878\n",
      "Epoch[0], Batch[3072], Train loss: 0.039805494248867035\n",
      "Epoch[0], Val loss: 0.0378606878221035\n",
      "Epoch[0], Batch[3073], Train loss: 0.0415659137070179\n",
      "Epoch[0], Val loss: 0.038157228380441666\n",
      "Epoch[0], Batch[3074], Train loss: 0.04095865413546562\n",
      "Epoch[0], Val loss: 0.03926568850874901\n",
      "Epoch[0], Batch[3075], Train loss: 0.041191648691892624\n",
      "Epoch[0], Val loss: 0.03928755968809128\n",
      "Epoch[0], Batch[3076], Train loss: 0.04193831607699394\n",
      "Epoch[0], Val loss: 0.037609636783599854\n",
      "Epoch[0], Batch[3077], Train loss: 0.04134969785809517\n",
      "Epoch[0], Val loss: 0.03892994299530983\n",
      "Epoch[0], Batch[3078], Train loss: 0.03941892087459564\n",
      "Epoch[0], Val loss: 0.039108067750930786\n",
      "Epoch[0], Batch[3079], Train loss: 0.041586507111787796\n",
      "Epoch[0], Val loss: 0.0393042117357254\n",
      "Epoch[0], Batch[3080], Train loss: 0.03879841789603233\n",
      "Epoch[0], Val loss: 0.03882824629545212\n",
      "Epoch[0], Batch[3081], Train loss: 0.03909068927168846\n",
      "Epoch[0], Val loss: 0.041029296815395355\n",
      "Epoch[0], Batch[3082], Train loss: 0.03928882256150246\n",
      "Epoch[0], Val loss: 0.0401017889380455\n",
      "Epoch[0], Batch[3083], Train loss: 0.041763659566640854\n",
      "Epoch[0], Val loss: 0.037405334413051605\n",
      "Epoch[0], Batch[3084], Train loss: 0.03911779448390007\n",
      "Epoch[0], Val loss: 0.03984085097908974\n",
      "Epoch[0], Batch[3085], Train loss: 0.042514070868492126\n",
      "Epoch[0], Val loss: 0.037964899092912674\n",
      "Epoch[0], Batch[3086], Train loss: 0.03827245160937309\n",
      "Epoch[0], Val loss: 0.03694940358400345\n",
      "Epoch[0], Batch[3087], Train loss: 0.041066382080316544\n",
      "Epoch[0], Val loss: 0.0413735993206501\n",
      "Epoch[0], Batch[3088], Train loss: 0.04076654091477394\n",
      "Epoch[0], Val loss: 0.03968905285000801\n",
      "Epoch[0], Batch[3089], Train loss: 0.040579721331596375\n",
      "Epoch[0], Val loss: 0.03982776775956154\n",
      "Epoch[0], Batch[3090], Train loss: 0.040032416582107544\n",
      "Epoch[0], Val loss: 0.039418503642082214\n",
      "Epoch[0], Batch[3091], Train loss: 0.04078429937362671\n",
      "Epoch[0], Val loss: 0.03965966776013374\n",
      "Epoch[0], Batch[3092], Train loss: 0.04064840450882912\n",
      "Epoch[0], Val loss: 0.038904543966054916\n",
      "Epoch[0], Batch[3093], Train loss: 0.041687410324811935\n",
      "Epoch[0], Val loss: 0.039489418268203735\n",
      "Epoch[0], Batch[3094], Train loss: 0.042289916425943375\n",
      "Epoch[0], Val loss: 0.03942735493183136\n",
      "Epoch[0], Batch[3095], Train loss: 0.03993445634841919\n",
      "Epoch[0], Val loss: 0.04088367894291878\n",
      "Epoch[0], Batch[3096], Train loss: 0.03978104516863823\n",
      "Epoch[0], Val loss: 0.04013852775096893\n",
      "Epoch[0], Batch[3097], Train loss: 0.0390266589820385\n",
      "Epoch[0], Val loss: 0.038043808192014694\n",
      "Epoch[0], Batch[3098], Train loss: 0.039143770933151245\n",
      "Epoch[0], Val loss: 0.036839790642261505\n",
      "Epoch[0], Batch[3099], Train loss: 0.03882807493209839\n",
      "Epoch[0], Val loss: 0.03865785151720047\n",
      "Epoch[0], Batch[3100], Train loss: 0.03871603682637215\n",
      "Epoch[0], Val loss: 0.03627515211701393\n",
      "Epoch[0], Batch[3101], Train loss: 0.04185939580202103\n",
      "Epoch[0], Val loss: 0.040431227535009384\n",
      "Epoch[0], Batch[3102], Train loss: 0.04057066887617111\n",
      "Epoch[0], Val loss: 0.037419382482767105\n",
      "Epoch[0], Batch[3103], Train loss: 0.03822971507906914\n",
      "Epoch[0], Val loss: 0.03805430233478546\n",
      "Epoch[0], Batch[3104], Train loss: 0.039472587406635284\n",
      "Epoch[0], Val loss: 0.0376630574464798\n",
      "Epoch[0], Batch[3105], Train loss: 0.040999576449394226\n",
      "Epoch[0], Val loss: 0.03720417618751526\n",
      "Epoch[0], Batch[3106], Train loss: 0.042955730110406876\n",
      "Epoch[0], Val loss: 0.03812534734606743\n",
      "Epoch[0], Batch[3107], Train loss: 0.039388708770275116\n",
      "Epoch[0], Val loss: 0.039253901690244675\n",
      "Epoch[0], Batch[3108], Train loss: 0.039231594651937485\n",
      "Epoch[0], Val loss: 0.037514809519052505\n",
      "Epoch[0], Batch[3109], Train loss: 0.04216882213950157\n",
      "Epoch[0], Val loss: 0.0376342348754406\n",
      "Epoch[0], Batch[3110], Train loss: 0.0397103950381279\n",
      "Epoch[0], Val loss: 0.03788900002837181\n",
      "Epoch[0], Batch[3111], Train loss: 0.04098948463797569\n",
      "Epoch[0], Val loss: 0.039261601865291595\n",
      "Epoch[0], Batch[3112], Train loss: 0.03966667130589485\n",
      "Epoch[0], Val loss: 0.039428506046533585\n",
      "Epoch[0], Batch[3113], Train loss: 0.04267941787838936\n",
      "Epoch[0], Val loss: 0.0371178463101387\n",
      "Epoch[0], Batch[3114], Train loss: 0.039277903735637665\n",
      "Epoch[0], Val loss: 0.0368836373090744\n",
      "Epoch[0], Batch[3115], Train loss: 0.03855782374739647\n",
      "Epoch[0], Val loss: 0.03655068948864937\n",
      "Epoch[0], Batch[3116], Train loss: 0.040135130286216736\n",
      "Epoch[0], Val loss: 0.03889646381139755\n",
      "Epoch[0], Batch[3117], Train loss: 0.038295187056064606\n",
      "Epoch[0], Val loss: 0.03813408315181732\n",
      "Epoch[0], Batch[3118], Train loss: 0.038588520139455795\n",
      "Epoch[0], Val loss: 0.03879500925540924\n",
      "Epoch[0], Batch[3119], Train loss: 0.04076852276921272\n",
      "Epoch[0], Val loss: 0.03958137333393097\n",
      "Epoch[0], Batch[3120], Train loss: 0.038475628942251205\n",
      "Epoch[0], Val loss: 0.037185993045568466\n",
      "Epoch[0], Batch[3121], Train loss: 0.03885782137513161\n",
      "Epoch[0], Val loss: 0.0377327986061573\n",
      "Epoch[0], Batch[3122], Train loss: 0.039752982556819916\n",
      "Epoch[0], Val loss: 0.038687922060489655\n",
      "Epoch[0], Batch[3123], Train loss: 0.039801981300115585\n",
      "Epoch[0], Val loss: 0.03946976736187935\n",
      "Epoch[0], Batch[3124], Train loss: 0.037112873047590256\n",
      "Epoch[0], Val loss: 0.03579830005764961\n",
      "Epoch[0], Batch[3125], Train loss: 0.03783559054136276\n",
      "Epoch[0], Val loss: 0.03749500587582588\n",
      "Epoch[0], Batch[3126], Train loss: 0.03917428478598595\n",
      "Epoch[0], Val loss: 0.038143090903759\n",
      "Epoch[0], Batch[3127], Train loss: 0.043095145374536514\n",
      "Epoch[0], Val loss: 0.03782176226377487\n",
      "Epoch[0], Batch[3128], Train loss: 0.038561273366212845\n",
      "Epoch[0], Val loss: 0.03634113818407059\n",
      "Epoch[0], Batch[3129], Train loss: 0.0393645316362381\n",
      "Epoch[0], Val loss: 0.03764306753873825\n",
      "Epoch[0], Batch[3130], Train loss: 0.042023543268442154\n",
      "Epoch[0], Val loss: 0.038754772394895554\n",
      "Epoch[0], Batch[3131], Train loss: 0.0397193469107151\n",
      "Epoch[0], Val loss: 0.03756660223007202\n",
      "Epoch[0], Batch[3132], Train loss: 0.04092935472726822\n",
      "Epoch[0], Val loss: 0.03914939984679222\n",
      "Epoch[0], Batch[3133], Train loss: 0.040971044450998306\n",
      "Epoch[0], Val loss: 0.038062144070863724\n",
      "Epoch[0], Batch[3134], Train loss: 0.037978172302246094\n",
      "Epoch[0], Val loss: 0.04064667224884033\n",
      "Epoch[0], Batch[3135], Train loss: 0.04151330515742302\n",
      "Epoch[0], Val loss: 0.03899982199072838\n",
      "Epoch[0], Batch[3136], Train loss: 0.042081937193870544\n",
      "Epoch[0], Val loss: 0.040512584149837494\n",
      "Epoch[0], Batch[3137], Train loss: 0.03959529101848602\n",
      "Epoch[0], Val loss: 0.03815719112753868\n",
      "Epoch[0], Batch[3138], Train loss: 0.04048469662666321\n",
      "Epoch[0], Val loss: 0.0369734987616539\n",
      "Epoch[0], Batch[3139], Train loss: 0.040310174226760864\n",
      "Epoch[0], Val loss: 0.03665982559323311\n",
      "Epoch[0], Batch[3140], Train loss: 0.041351206600666046\n",
      "Epoch[0], Val loss: 0.0371023491024971\n",
      "Epoch[0], Batch[3141], Train loss: 0.03589633107185364\n",
      "Epoch[0], Val loss: 0.03949689865112305\n",
      "Epoch[0], Batch[3142], Train loss: 0.040163278579711914\n",
      "Epoch[0], Val loss: 0.038515664637088776\n",
      "Epoch[0], Batch[3143], Train loss: 0.03887190669775009\n",
      "Epoch[0], Val loss: 0.03706754744052887\n",
      "Epoch[0], Batch[3144], Train loss: 0.039661914110183716\n",
      "Epoch[0], Val loss: 0.03902176395058632\n",
      "Epoch[0], Batch[3145], Train loss: 0.03946802765130997\n",
      "Epoch[0], Val loss: 0.038839567452669144\n",
      "Epoch[0], Batch[3146], Train loss: 0.04020798206329346\n",
      "Epoch[0], Val loss: 0.04006873071193695\n",
      "Epoch[0], Batch[3147], Train loss: 0.03965264558792114\n",
      "Epoch[0], Val loss: 0.037146419286727905\n",
      "Epoch[0], Batch[3148], Train loss: 0.03966906666755676\n",
      "Epoch[0], Val loss: 0.03881252184510231\n",
      "Epoch[0], Batch[3149], Train loss: 0.04113521799445152\n",
      "Epoch[0], Val loss: 0.03739658743143082\n",
      "Epoch[0], Batch[3150], Train loss: 0.041134126484394073\n",
      "Epoch[0], Val loss: 0.03822970762848854\n",
      "Epoch[0], Batch[3151], Train loss: 0.040130987763404846\n",
      "Epoch[0], Val loss: 0.03779321536421776\n",
      "Epoch[0], Batch[3152], Train loss: 0.040565766394138336\n",
      "Epoch[0], Val loss: 0.03966618701815605\n",
      "Epoch[0], Batch[3153], Train loss: 0.039011579006910324\n",
      "Epoch[0], Val loss: 0.03865983709692955\n",
      "Epoch[0], Batch[3154], Train loss: 0.03851558268070221\n",
      "Epoch[0], Val loss: 0.039575330913066864\n",
      "Epoch[0], Batch[3155], Train loss: 0.040696077048778534\n",
      "Epoch[0], Val loss: 0.03905407339334488\n",
      "Epoch[0], Batch[3156], Train loss: 0.039491038769483566\n",
      "Epoch[0], Val loss: 0.03772579878568649\n",
      "Epoch[0], Batch[3157], Train loss: 0.03898392617702484\n",
      "Epoch[0], Val loss: 0.038763582706451416\n",
      "Epoch[0], Batch[3158], Train loss: 0.037181220948696136\n",
      "Epoch[0], Val loss: 0.03872359171509743\n",
      "Epoch[0], Batch[3159], Train loss: 0.039085932075977325\n",
      "Epoch[0], Val loss: 0.03985022380948067\n",
      "Epoch[0], Batch[3160], Train loss: 0.03952834755182266\n",
      "Epoch[0], Val loss: 0.0366949625313282\n",
      "Epoch[0], Batch[3161], Train loss: 0.040824390947818756\n",
      "Epoch[0], Val loss: 0.037268709391355515\n",
      "Epoch[0], Batch[3162], Train loss: 0.037814684212207794\n",
      "Epoch[0], Val loss: 0.03705061599612236\n",
      "Epoch[0], Batch[3163], Train loss: 0.038992829620838165\n",
      "Epoch[0], Val loss: 0.03864345699548721\n",
      "Epoch[0], Batch[3164], Train loss: 0.04151998087763786\n",
      "Epoch[0], Val loss: 0.038079243153333664\n",
      "Epoch[0], Batch[3165], Train loss: 0.037936724722385406\n",
      "Epoch[0], Val loss: 0.03785938769578934\n",
      "Epoch[0], Batch[3166], Train loss: 0.03821270540356636\n",
      "Epoch[0], Val loss: 0.037308212369680405\n",
      "Epoch[0], Batch[3167], Train loss: 0.03832637518644333\n",
      "Epoch[0], Val loss: 0.04406728595495224\n",
      "Epoch[0], Batch[3168], Train loss: 0.038821469992399216\n",
      "Epoch[0], Val loss: 0.03825627267360687\n",
      "Epoch[0], Batch[3169], Train loss: 0.03829890117049217\n",
      "Epoch[0], Val loss: 0.03780006244778633\n",
      "Epoch[0], Batch[3170], Train loss: 0.03847423195838928\n",
      "Epoch[0], Val loss: 0.039454638957977295\n",
      "Epoch[0], Batch[3171], Train loss: 0.04039761796593666\n",
      "Epoch[0], Val loss: 0.038314614444971085\n",
      "Epoch[0], Batch[3172], Train loss: 0.0393546000123024\n",
      "Epoch[0], Val loss: 0.03842616081237793\n",
      "Epoch[0], Batch[3173], Train loss: 0.036397214978933334\n",
      "Epoch[0], Val loss: 0.039204489439725876\n",
      "Epoch[0], Batch[3174], Train loss: 0.044559892266988754\n",
      "Epoch[0], Val loss: 0.036011260002851486\n",
      "Epoch[0], Batch[3175], Train loss: 0.04078984260559082\n",
      "Epoch[0], Val loss: 0.03617225959897041\n",
      "Epoch[0], Batch[3176], Train loss: 0.04155506566166878\n",
      "Epoch[0], Val loss: 0.039861999452114105\n",
      "Epoch[0], Batch[3177], Train loss: 0.04197414964437485\n",
      "Epoch[0], Val loss: 0.04045317694544792\n",
      "Epoch[0], Batch[3178], Train loss: 0.03993791341781616\n",
      "Epoch[0], Val loss: 0.03815710172057152\n",
      "Epoch[0], Batch[3179], Train loss: 0.04120148718357086\n",
      "Epoch[0], Val loss: 0.039443500339984894\n",
      "Epoch[0], Batch[3180], Train loss: 0.03905005753040314\n",
      "Epoch[0], Val loss: 0.04074588045477867\n",
      "Epoch[0], Batch[3181], Train loss: 0.0404413603246212\n",
      "Epoch[0], Val loss: 0.0374646931886673\n",
      "Epoch[0], Batch[3182], Train loss: 0.03954176604747772\n",
      "Epoch[0], Val loss: 0.037449050694704056\n",
      "Epoch[0], Batch[3183], Train loss: 0.039555273950099945\n",
      "Epoch[0], Val loss: 0.03901680186390877\n",
      "Epoch[0], Batch[3184], Train loss: 0.040008097887039185\n",
      "Epoch[0], Val loss: 0.03880787268280983\n",
      "Epoch[0], Batch[3185], Train loss: 0.03691231086850166\n",
      "Epoch[0], Val loss: 0.037921905517578125\n",
      "Epoch[0], Batch[3186], Train loss: 0.0392972007393837\n",
      "Epoch[0], Val loss: 0.036450695246458054\n",
      "Epoch[0], Batch[3187], Train loss: 0.03930863365530968\n",
      "Epoch[0], Val loss: 0.03683512657880783\n",
      "Epoch[0], Batch[3188], Train loss: 0.03914032131433487\n",
      "Epoch[0], Val loss: 0.03804268315434456\n",
      "Epoch[0], Batch[3189], Train loss: 0.03970422223210335\n",
      "Epoch[0], Val loss: 0.03855033963918686\n",
      "Epoch[0], Batch[3190], Train loss: 0.041240494698286057\n",
      "Epoch[0], Val loss: 0.036439310759305954\n",
      "Epoch[0], Batch[3191], Train loss: 0.04111570864915848\n",
      "Epoch[0], Val loss: 0.038707464933395386\n",
      "Epoch[0], Batch[3192], Train loss: 0.04190301522612572\n",
      "Epoch[0], Val loss: 0.040118131786584854\n",
      "Epoch[0], Batch[3193], Train loss: 0.04105218127369881\n",
      "Epoch[0], Val loss: 0.03723270818591118\n",
      "Epoch[0], Batch[3194], Train loss: 0.03931370750069618\n",
      "Epoch[0], Val loss: 0.03638701140880585\n",
      "Epoch[0], Batch[3195], Train loss: 0.040218595415353775\n",
      "Epoch[0], Val loss: 0.037317756563425064\n",
      "Epoch[0], Batch[3196], Train loss: 0.04046640545129776\n",
      "Epoch[0], Val loss: 0.03864886984229088\n",
      "Epoch[0], Batch[3197], Train loss: 0.03865808993577957\n",
      "Epoch[0], Val loss: 0.03634032979607582\n",
      "Epoch[0], Batch[3198], Train loss: 0.03829187899827957\n",
      "Epoch[0], Val loss: 0.039407674223184586\n",
      "Epoch[0], Batch[3199], Train loss: 0.04102509841322899\n",
      "Epoch[0], Val loss: 0.03975154459476471\n",
      "Epoch[0], Batch[3200], Train loss: 0.04124998673796654\n",
      "Epoch[0], Val loss: 0.039053767919540405\n",
      "Epoch[0], Batch[3201], Train loss: 0.03700544312596321\n",
      "Epoch[0], Val loss: 0.038446687161922455\n",
      "Epoch[0], Batch[3202], Train loss: 0.039085596799850464\n",
      "Epoch[0], Val loss: 0.039120256900787354\n",
      "Epoch[0], Batch[3203], Train loss: 0.04084021970629692\n",
      "Epoch[0], Val loss: 0.036114614456892014\n",
      "Epoch[0], Batch[3204], Train loss: 0.04011727124452591\n",
      "Epoch[0], Val loss: 0.038869548588991165\n",
      "Epoch[0], Batch[3205], Train loss: 0.04123435169458389\n",
      "Epoch[0], Val loss: 0.03820999711751938\n",
      "Epoch[0], Batch[3206], Train loss: 0.04081520438194275\n",
      "Epoch[0], Val loss: 0.03857544809579849\n",
      "Epoch[0], Batch[3207], Train loss: 0.03802017122507095\n",
      "Epoch[0], Val loss: 0.04091678932309151\n",
      "Epoch[0], Batch[3208], Train loss: 0.04301424324512482\n",
      "Epoch[0], Val loss: 0.039214059710502625\n",
      "Epoch[0], Batch[3209], Train loss: 0.034737735986709595\n",
      "Epoch[0], Val loss: 0.03796875476837158\n",
      "Epoch[0], Batch[3210], Train loss: 0.04014573618769646\n",
      "Epoch[0], Val loss: 0.038067493587732315\n",
      "Epoch[0], Batch[3211], Train loss: 0.0398280955851078\n",
      "Epoch[0], Val loss: 0.03849286213517189\n",
      "Epoch[0], Batch[3212], Train loss: 0.043304622173309326\n",
      "Epoch[0], Val loss: 0.03778776898980141\n",
      "Epoch[0], Batch[3213], Train loss: 0.03732820227742195\n",
      "Epoch[0], Val loss: 0.03727155178785324\n",
      "Epoch[0], Batch[3214], Train loss: 0.03931122645735741\n",
      "Epoch[0], Val loss: 0.03622305020689964\n",
      "Epoch[0], Batch[3215], Train loss: 0.040809936821460724\n",
      "Epoch[0], Val loss: 0.038406357169151306\n",
      "Epoch[0], Batch[3216], Train loss: 0.03873267397284508\n",
      "Epoch[0], Val loss: 0.0348801352083683\n",
      "Epoch[0], Batch[3217], Train loss: 0.036662328988313675\n",
      "Epoch[0], Val loss: 0.038466114550828934\n",
      "Epoch[0], Batch[3218], Train loss: 0.03940818831324577\n",
      "Epoch[0], Val loss: 0.036398883908987045\n",
      "Epoch[0], Batch[3219], Train loss: 0.039570290595293045\n",
      "Epoch[0], Val loss: 0.035479482263326645\n",
      "Epoch[0], Batch[3220], Train loss: 0.0392288975417614\n",
      "Epoch[0], Val loss: 0.03470245376229286\n",
      "Epoch[0], Batch[3221], Train loss: 0.04132372885942459\n",
      "Epoch[0], Val loss: 0.039041195064783096\n",
      "Epoch[0], Batch[3222], Train loss: 0.039287783205509186\n",
      "Epoch[0], Val loss: 0.0399346798658371\n",
      "Epoch[0], Batch[3223], Train loss: 0.03853854909539223\n",
      "Epoch[0], Val loss: 0.038896288722753525\n",
      "Epoch[0], Batch[3224], Train loss: 0.03978434205055237\n",
      "Epoch[0], Val loss: 0.039514265954494476\n",
      "Epoch[0], Batch[3225], Train loss: 0.038554318249225616\n",
      "Epoch[0], Val loss: 0.037412405014038086\n",
      "Epoch[0], Batch[3226], Train loss: 0.038324516266584396\n",
      "Epoch[0], Val loss: 0.038079556077718735\n",
      "Epoch[0], Batch[3227], Train loss: 0.041007641702890396\n",
      "Epoch[0], Val loss: 0.0380336157977581\n",
      "Epoch[0], Batch[3228], Train loss: 0.03779422491788864\n",
      "Epoch[0], Val loss: 0.03838343918323517\n",
      "Epoch[0], Batch[3229], Train loss: 0.040697045624256134\n",
      "Epoch[0], Val loss: 0.03805705904960632\n",
      "Epoch[0], Batch[3230], Train loss: 0.039292335510253906\n",
      "Epoch[0], Val loss: 0.04002545401453972\n",
      "Epoch[0], Batch[3231], Train loss: 0.038653917610645294\n",
      "Epoch[0], Val loss: 0.039047665894031525\n",
      "Epoch[0], Batch[3232], Train loss: 0.037782058119773865\n",
      "Epoch[0], Val loss: 0.03814614564180374\n",
      "Epoch[0], Batch[3233], Train loss: 0.039159078150987625\n",
      "Epoch[0], Val loss: 0.03757615387439728\n",
      "Epoch[0], Batch[3234], Train loss: 0.040716689079999924\n",
      "Epoch[0], Val loss: 0.03799914941191673\n",
      "Epoch[0], Batch[3235], Train loss: 0.03963751345872879\n",
      "Epoch[0], Val loss: 0.038121581077575684\n",
      "Epoch[0], Batch[3236], Train loss: 0.04002115875482559\n",
      "Epoch[0], Val loss: 0.0409524105489254\n",
      "Epoch[0], Batch[3237], Train loss: 0.03849548101425171\n",
      "Epoch[0], Val loss: 0.03786119818687439\n",
      "Epoch[0], Batch[3238], Train loss: 0.03884362429380417\n",
      "Epoch[0], Val loss: 0.039174262434244156\n",
      "Epoch[0], Batch[3239], Train loss: 0.03867306187748909\n",
      "Epoch[0], Val loss: 0.04172342270612717\n",
      "Epoch[0], Batch[3240], Train loss: 0.040452055633068085\n",
      "Epoch[0], Val loss: 0.03533164784312248\n",
      "Epoch[0], Batch[3241], Train loss: 0.03969588875770569\n",
      "Epoch[0], Val loss: 0.038815561681985855\n",
      "Epoch[0], Batch[3242], Train loss: 0.03890099748969078\n",
      "Epoch[0], Val loss: 0.03956713527441025\n",
      "Epoch[0], Batch[3243], Train loss: 0.03849951922893524\n",
      "Epoch[0], Val loss: 0.037111006677150726\n",
      "Epoch[0], Batch[3244], Train loss: 0.038099050521850586\n",
      "Epoch[0], Val loss: 0.03871266543865204\n",
      "Epoch[0], Batch[3245], Train loss: 0.03808366879820824\n",
      "Epoch[0], Val loss: 0.03632863983511925\n",
      "Epoch[0], Batch[3246], Train loss: 0.04123790189623833\n",
      "Epoch[0], Val loss: 0.03826664388179779\n",
      "Epoch[0], Batch[3247], Train loss: 0.03959430009126663\n",
      "Epoch[0], Val loss: 0.03825456276535988\n",
      "Epoch[0], Batch[3248], Train loss: 0.04081357643008232\n",
      "Epoch[0], Val loss: 0.03648175671696663\n",
      "Epoch[0], Batch[3249], Train loss: 0.03715271130204201\n",
      "Epoch[0], Val loss: 0.037612248212099075\n",
      "Epoch[0], Batch[3250], Train loss: 0.04047062620520592\n",
      "Epoch[0], Val loss: 0.039064932614564896\n",
      "Epoch[0], Batch[3251], Train loss: 0.043106891214847565\n",
      "Epoch[0], Val loss: 0.039376068860292435\n",
      "Epoch[0], Batch[3252], Train loss: 0.04261577129364014\n",
      "Epoch[0], Val loss: 0.0420118011534214\n",
      "Epoch[0], Batch[3253], Train loss: 0.041483525186777115\n",
      "Epoch[0], Val loss: 0.0397268682718277\n",
      "Epoch[0], Batch[3254], Train loss: 0.039087191224098206\n",
      "Epoch[0], Val loss: 0.03839762508869171\n",
      "Epoch[0], Batch[3255], Train loss: 0.03924498334527016\n",
      "Epoch[0], Val loss: 0.038792770355939865\n",
      "Epoch[0], Batch[3256], Train loss: 0.0400538332760334\n",
      "Epoch[0], Val loss: 0.03942529484629631\n",
      "Epoch[0], Batch[3257], Train loss: 0.03904859721660614\n",
      "Epoch[0], Val loss: 0.036704111844301224\n",
      "Epoch[0], Batch[3258], Train loss: 0.0413583368062973\n",
      "Epoch[0], Val loss: 0.037554483860731125\n",
      "Epoch[0], Batch[3259], Train loss: 0.039156749844551086\n",
      "Epoch[0], Val loss: 0.03925085440278053\n",
      "Epoch[0], Batch[3260], Train loss: 0.03874170780181885\n",
      "Epoch[0], Val loss: 0.03787367418408394\n",
      "Epoch[0], Batch[3261], Train loss: 0.04106082767248154\n",
      "Epoch[0], Val loss: 0.03950366750359535\n",
      "Epoch[0], Batch[3262], Train loss: 0.04114474356174469\n",
      "Epoch[0], Val loss: 0.0378546267747879\n",
      "Epoch[0], Batch[3263], Train loss: 0.04193204268813133\n",
      "Epoch[0], Val loss: 0.039708804339170456\n",
      "Epoch[0], Batch[3264], Train loss: 0.03794783353805542\n",
      "Epoch[0], Val loss: 0.03659951686859131\n",
      "Epoch[0], Batch[3265], Train loss: 0.039726436138153076\n",
      "Epoch[0], Val loss: 0.03813488408923149\n",
      "Epoch[0], Batch[3266], Train loss: 0.03850381448864937\n",
      "Epoch[0], Val loss: 0.04045858979225159\n",
      "Epoch[0], Batch[3267], Train loss: 0.04121515154838562\n",
      "Epoch[0], Val loss: 0.03877405822277069\n",
      "Epoch[0], Batch[3268], Train loss: 0.03901239484548569\n",
      "Epoch[0], Val loss: 0.03714365512132645\n",
      "Epoch[0], Batch[3269], Train loss: 0.04180382192134857\n",
      "Epoch[0], Val loss: 0.037056419998407364\n",
      "Epoch[0], Batch[3270], Train loss: 0.03893209248781204\n",
      "Epoch[0], Val loss: 0.03879885375499725\n",
      "Epoch[0], Batch[3271], Train loss: 0.03809027001261711\n",
      "Epoch[0], Val loss: 0.03887046128511429\n",
      "Epoch[0], Batch[3272], Train loss: 0.03974943608045578\n",
      "Epoch[0], Val loss: 0.040612850338220596\n",
      "Epoch[0], Batch[3273], Train loss: 0.0413295179605484\n",
      "Epoch[0], Val loss: 0.03761797398328781\n",
      "Epoch[0], Batch[3274], Train loss: 0.03946147859096527\n",
      "Epoch[0], Val loss: 0.03712413087487221\n",
      "Epoch[0], Batch[3275], Train loss: 0.03813532367348671\n",
      "Epoch[0], Val loss: 0.03621417284011841\n",
      "Epoch[0], Batch[3276], Train loss: 0.038893479853868484\n",
      "Epoch[0], Val loss: 0.041153475642204285\n",
      "Epoch[0], Batch[3277], Train loss: 0.04039211571216583\n",
      "Epoch[0], Val loss: 0.03651503473520279\n",
      "Epoch[0], Batch[3278], Train loss: 0.04041633754968643\n",
      "Epoch[0], Val loss: 0.03894726559519768\n",
      "Epoch[0], Batch[3279], Train loss: 0.039706502109766006\n",
      "Epoch[0], Val loss: 0.040362026542425156\n",
      "Epoch[0], Batch[3280], Train loss: 0.037414342164993286\n",
      "Epoch[0], Val loss: 0.04160158336162567\n",
      "Epoch[0], Batch[3281], Train loss: 0.0404740534722805\n",
      "Epoch[0], Val loss: 0.040387723594903946\n",
      "Epoch[0], Batch[3282], Train loss: 0.04013173654675484\n",
      "Epoch[0], Val loss: 0.03643743321299553\n",
      "Epoch[0], Batch[3283], Train loss: 0.039929620921611786\n",
      "Epoch[0], Val loss: 0.03871415928006172\n",
      "Epoch[0], Batch[3284], Train loss: 0.03967968747019768\n",
      "Epoch[0], Val loss: 0.03919539228081703\n",
      "Epoch[0], Batch[3285], Train loss: 0.03891299292445183\n",
      "Epoch[0], Val loss: 0.03811816871166229\n",
      "Epoch[0], Batch[3286], Train loss: 0.04075033590197563\n",
      "Epoch[0], Val loss: 0.03706918656826019\n",
      "Epoch[0], Batch[3287], Train loss: 0.0401998870074749\n",
      "Epoch[0], Val loss: 0.03690067678689957\n",
      "Epoch[0], Batch[3288], Train loss: 0.040147505700588226\n",
      "Epoch[0], Val loss: 0.03976310044527054\n",
      "Epoch[0], Batch[3289], Train loss: 0.040326230227947235\n",
      "Epoch[0], Val loss: 0.037868887186050415\n",
      "Epoch[0], Batch[3290], Train loss: 0.03869020193815231\n",
      "Epoch[0], Val loss: 0.03603198006749153\n",
      "Epoch[0], Batch[3291], Train loss: 0.03823594003915787\n",
      "Epoch[0], Val loss: 0.04066285863518715\n",
      "Epoch[0], Batch[3292], Train loss: 0.03730529546737671\n",
      "Epoch[0], Val loss: 0.041224535554647446\n",
      "Epoch[0], Batch[3293], Train loss: 0.038961924612522125\n",
      "Epoch[0], Val loss: 0.03604989871382713\n",
      "Epoch[0], Batch[3294], Train loss: 0.039239007979631424\n",
      "Epoch[0], Val loss: 0.03936347737908363\n",
      "Epoch[0], Batch[3295], Train loss: 0.03902551904320717\n",
      "Epoch[0], Val loss: 0.03874434903264046\n",
      "Epoch[0], Batch[3296], Train loss: 0.03944183886051178\n",
      "Epoch[0], Val loss: 0.035173896700143814\n",
      "Epoch[0], Batch[3297], Train loss: 0.04217216372489929\n",
      "Epoch[0], Val loss: 0.03684569522738457\n",
      "Epoch[0], Batch[3298], Train loss: 0.03727886825799942\n",
      "Epoch[0], Val loss: 0.03943509981036186\n",
      "Epoch[0], Batch[3299], Train loss: 0.04233276844024658\n",
      "Epoch[0], Val loss: 0.03879979997873306\n",
      "Epoch[0], Batch[3300], Train loss: 0.037286825478076935\n",
      "Epoch[0], Val loss: 0.036786388605833054\n",
      "Epoch[0], Batch[3301], Train loss: 0.04091197997331619\n",
      "Epoch[0], Val loss: 0.03976096957921982\n",
      "Epoch[0], Batch[3302], Train loss: 0.04064501076936722\n",
      "Epoch[0], Val loss: 0.038212306797504425\n",
      "Epoch[0], Batch[3303], Train loss: 0.0389445461332798\n",
      "Epoch[0], Val loss: 0.0338829904794693\n",
      "Epoch[0], Batch[3304], Train loss: 0.03981325030326843\n",
      "Epoch[0], Val loss: 0.03769034519791603\n",
      "Epoch[0], Batch[3305], Train loss: 0.03901240974664688\n",
      "Epoch[0], Val loss: 0.038599688559770584\n",
      "Epoch[0], Batch[3306], Train loss: 0.041400328278541565\n",
      "Epoch[0], Val loss: 0.038707245141267776\n",
      "Epoch[0], Batch[3307], Train loss: 0.03755549341440201\n",
      "Epoch[0], Val loss: 0.03688563033938408\n",
      "Epoch[0], Batch[3308], Train loss: 0.040037933737039566\n",
      "Epoch[0], Val loss: 0.0385550856590271\n",
      "Epoch[0], Batch[3309], Train loss: 0.038722068071365356\n",
      "Epoch[0], Val loss: 0.03907228633761406\n",
      "Epoch[0], Batch[3310], Train loss: 0.03797607123851776\n",
      "Epoch[0], Val loss: 0.03640975058078766\n",
      "Epoch[0], Batch[3311], Train loss: 0.04011871665716171\n",
      "Epoch[0], Val loss: 0.03751131147146225\n",
      "Epoch[0], Batch[3312], Train loss: 0.042240358889102936\n",
      "Epoch[0], Val loss: 0.03696581348776817\n",
      "Epoch[0], Batch[3313], Train loss: 0.0382300540804863\n",
      "Epoch[0], Val loss: 0.039609573781490326\n",
      "Epoch[0], Batch[3314], Train loss: 0.04093066602945328\n",
      "Epoch[0], Val loss: 0.0395403690636158\n",
      "Epoch[0], Batch[3315], Train loss: 0.04127933830022812\n",
      "Epoch[0], Val loss: 0.03594015911221504\n",
      "Epoch[0], Batch[3316], Train loss: 0.03715118393301964\n",
      "Epoch[0], Val loss: 0.035723719745874405\n",
      "Epoch[0], Batch[3317], Train loss: 0.03946639969944954\n",
      "Epoch[0], Val loss: 0.03870174661278725\n",
      "Epoch[0], Batch[3318], Train loss: 0.04245627671480179\n",
      "Epoch[0], Val loss: 0.03705838695168495\n",
      "Epoch[0], Batch[3319], Train loss: 0.04040171205997467\n",
      "Epoch[0], Val loss: 0.03612097352743149\n",
      "Epoch[0], Batch[3320], Train loss: 0.03722288832068443\n",
      "Epoch[0], Val loss: 0.038527946919202805\n",
      "Epoch[0], Batch[3321], Train loss: 0.039157845079898834\n",
      "Epoch[0], Val loss: 0.038264933973550797\n",
      "Epoch[0], Batch[3322], Train loss: 0.03977980464696884\n",
      "Epoch[0], Val loss: 0.03963335603475571\n",
      "Epoch[0], Batch[3323], Train loss: 0.036896005272865295\n",
      "Epoch[0], Val loss: 0.03833834454417229\n",
      "Epoch[0], Batch[3324], Train loss: 0.04012565314769745\n",
      "Epoch[0], Val loss: 0.03591978922486305\n",
      "Epoch[0], Batch[3325], Train loss: 0.040708836168050766\n",
      "Epoch[0], Val loss: 0.038149695843458176\n",
      "Epoch[0], Batch[3326], Train loss: 0.03918720781803131\n",
      "Epoch[0], Val loss: 0.03845498710870743\n",
      "Epoch[0], Batch[3327], Train loss: 0.04178719222545624\n",
      "Epoch[0], Val loss: 0.03954393416643143\n",
      "Epoch[0], Batch[3328], Train loss: 0.04127141833305359\n",
      "Epoch[0], Val loss: 0.03738577291369438\n",
      "Epoch[0], Batch[3329], Train loss: 0.04043829068541527\n",
      "Epoch[0], Val loss: 0.04037301614880562\n",
      "Epoch[0], Batch[3330], Train loss: 0.03636422008275986\n",
      "Epoch[0], Val loss: 0.041945651173591614\n",
      "Epoch[0], Batch[3331], Train loss: 0.03932341933250427\n",
      "Epoch[0], Val loss: 0.04020652547478676\n",
      "Epoch[0], Batch[3332], Train loss: 0.0400368794798851\n",
      "Epoch[0], Val loss: 0.03712977096438408\n",
      "Epoch[0], Batch[3333], Train loss: 0.04123732075095177\n",
      "Epoch[0], Val loss: 0.03663316369056702\n",
      "Epoch[0], Batch[3334], Train loss: 0.0397542379796505\n",
      "Epoch[0], Val loss: 0.03844405710697174\n",
      "Epoch[0], Batch[3335], Train loss: 0.039380308240652084\n",
      "Epoch[0], Val loss: 0.041132114827632904\n",
      "Epoch[0], Batch[3336], Train loss: 0.037225984036922455\n",
      "Epoch[0], Val loss: 0.03816315159201622\n",
      "Epoch[0], Batch[3337], Train loss: 0.041387058794498444\n",
      "Epoch[0], Val loss: 0.04081694036722183\n",
      "Epoch[0], Batch[3338], Train loss: 0.039935361593961716\n",
      "Epoch[0], Val loss: 0.037277497351169586\n",
      "Epoch[0], Batch[3339], Train loss: 0.0390922985970974\n",
      "Epoch[0], Val loss: 0.0414431095123291\n",
      "Epoch[0], Batch[3340], Train loss: 0.03540502488613129\n",
      "Epoch[0], Val loss: 0.03982757031917572\n",
      "Epoch[0], Batch[3341], Train loss: 0.039287228137254715\n",
      "Epoch[0], Val loss: 0.037402667105197906\n",
      "Epoch[0], Batch[3342], Train loss: 0.042068466544151306\n",
      "Epoch[0], Val loss: 0.03648250922560692\n",
      "Epoch[0], Batch[3343], Train loss: 0.039007507264614105\n",
      "Epoch[0], Val loss: 0.03739424794912338\n",
      "Epoch[0], Batch[3344], Train loss: 0.042158093303442\n",
      "Epoch[0], Val loss: 0.04061540588736534\n",
      "Epoch[0], Batch[3345], Train loss: 0.03888891637325287\n",
      "Epoch[0], Val loss: 0.037479218095541\n",
      "Epoch[0], Batch[3346], Train loss: 0.04164709150791168\n",
      "Epoch[0], Val loss: 0.03799416869878769\n",
      "Epoch[0], Batch[3347], Train loss: 0.035648852586746216\n",
      "Epoch[0], Val loss: 0.03783728927373886\n",
      "Epoch[0], Batch[3348], Train loss: 0.0396883524954319\n",
      "Epoch[0], Val loss: 0.04124755412340164\n",
      "Epoch[0], Batch[3349], Train loss: 0.040473710745573044\n",
      "Epoch[0], Val loss: 0.03777743875980377\n",
      "Epoch[0], Batch[3350], Train loss: 0.04385744407773018\n",
      "Epoch[0], Val loss: 0.03712312504649162\n",
      "Epoch[0], Batch[3351], Train loss: 0.03847237303853035\n",
      "Epoch[0], Val loss: 0.03677942231297493\n",
      "Epoch[0], Batch[3352], Train loss: 0.04113783314824104\n",
      "Epoch[0], Val loss: 0.03833336755633354\n",
      "Epoch[0], Batch[3353], Train loss: 0.03752550482749939\n",
      "Epoch[0], Val loss: 0.03694726899266243\n",
      "Epoch[0], Batch[3354], Train loss: 0.039255689829587936\n",
      "Epoch[0], Val loss: 0.039101749658584595\n",
      "Epoch[0], Batch[3355], Train loss: 0.04002609848976135\n",
      "Epoch[0], Val loss: 0.03812268748879433\n",
      "Epoch[0], Batch[3356], Train loss: 0.03968176990747452\n",
      "Epoch[0], Val loss: 0.039997268468141556\n",
      "Epoch[0], Batch[3357], Train loss: 0.039788153022527695\n",
      "Epoch[0], Val loss: 0.03607208654284477\n",
      "Epoch[0], Batch[3358], Train loss: 0.03897527605295181\n",
      "Epoch[0], Val loss: 0.03767642006278038\n",
      "Epoch[0], Batch[3359], Train loss: 0.03661727160215378\n",
      "Epoch[0], Val loss: 0.038150932639837265\n",
      "Epoch[0], Batch[3360], Train loss: 0.0392560251057148\n",
      "Epoch[0], Val loss: 0.03748898208141327\n",
      "Epoch[0], Batch[3361], Train loss: 0.03924383595585823\n",
      "Epoch[0], Val loss: 0.0380735918879509\n",
      "Epoch[0], Batch[3362], Train loss: 0.04060431942343712\n",
      "Epoch[0], Val loss: 0.04104067012667656\n",
      "Epoch[0], Batch[3363], Train loss: 0.040448300540447235\n",
      "Epoch[0], Val loss: 0.037772685289382935\n",
      "Epoch[0], Batch[3364], Train loss: 0.03806218504905701\n",
      "Epoch[0], Val loss: 0.03731556981801987\n",
      "Epoch[0], Batch[3365], Train loss: 0.03761610388755798\n",
      "Epoch[0], Val loss: 0.03878897801041603\n",
      "Epoch[0], Batch[3366], Train loss: 0.039121113717556\n",
      "Epoch[0], Val loss: 0.039219826459884644\n",
      "Epoch[0], Batch[3367], Train loss: 0.039776984602212906\n",
      "Epoch[0], Val loss: 0.038980163633823395\n",
      "Epoch[0], Batch[3368], Train loss: 0.03701731562614441\n",
      "Epoch[0], Val loss: 0.03726378455758095\n",
      "Epoch[0], Batch[3369], Train loss: 0.038241006433963776\n",
      "Epoch[0], Val loss: 0.03691478818655014\n",
      "Epoch[0], Batch[3370], Train loss: 0.03974700719118118\n",
      "Epoch[0], Val loss: 0.03878307715058327\n",
      "Epoch[0], Batch[3371], Train loss: 0.03918454796075821\n",
      "Epoch[0], Val loss: 0.03733314946293831\n",
      "Epoch[0], Batch[3372], Train loss: 0.03677063062787056\n",
      "Epoch[0], Val loss: 0.03447580337524414\n",
      "Epoch[0], Batch[3373], Train loss: 0.03800781071186066\n",
      "Epoch[0], Val loss: 0.036497581750154495\n",
      "Epoch[0], Batch[3374], Train loss: 0.03811132162809372\n",
      "Epoch[0], Val loss: 0.03816854581236839\n",
      "Epoch[0], Batch[3375], Train loss: 0.04067915305495262\n",
      "Epoch[0], Val loss: 0.03868670389056206\n",
      "Epoch[0], Batch[3376], Train loss: 0.03948964551091194\n",
      "Epoch[0], Val loss: 0.0393034927546978\n",
      "Epoch[0], Batch[3377], Train loss: 0.03752944990992546\n",
      "Epoch[0], Val loss: 0.03704697638750076\n",
      "Epoch[0], Batch[3378], Train loss: 0.03788880631327629\n",
      "Epoch[0], Val loss: 0.03820963203907013\n",
      "Epoch[0], Batch[3379], Train loss: 0.039639923721551895\n",
      "Epoch[0], Val loss: 0.04028373211622238\n",
      "Epoch[0], Batch[3380], Train loss: 0.03917265683412552\n",
      "Epoch[0], Val loss: 0.03731195628643036\n",
      "Epoch[0], Batch[3381], Train loss: 0.04001160338521004\n",
      "Epoch[0], Val loss: 0.038985755294561386\n",
      "Epoch[0], Batch[3382], Train loss: 0.03979023918509483\n",
      "Epoch[0], Val loss: 0.03817921504378319\n",
      "Epoch[0], Batch[3383], Train loss: 0.039155181497335434\n",
      "Epoch[0], Val loss: 0.03860611468553543\n",
      "Epoch[0], Batch[3384], Train loss: 0.039650559425354004\n",
      "Epoch[0], Val loss: 0.038135282695293427\n",
      "Epoch[0], Batch[3385], Train loss: 0.03775164857506752\n",
      "Epoch[0], Val loss: 0.038019075989723206\n",
      "Epoch[0], Batch[3386], Train loss: 0.04001577943563461\n",
      "Epoch[0], Val loss: 0.03448092192411423\n",
      "Epoch[0], Batch[3387], Train loss: 0.040205925703048706\n",
      "Epoch[0], Val loss: 0.04055846482515335\n",
      "Epoch[0], Batch[3388], Train loss: 0.041775818914175034\n",
      "Epoch[0], Val loss: 0.03967287018895149\n",
      "Epoch[0], Batch[3389], Train loss: 0.03949754685163498\n",
      "Epoch[0], Val loss: 0.03908708691596985\n",
      "Epoch[0], Batch[3390], Train loss: 0.04071216657757759\n",
      "Epoch[0], Val loss: 0.03837123140692711\n",
      "Epoch[0], Batch[3391], Train loss: 0.03948390483856201\n",
      "Epoch[0], Val loss: 0.03886640444397926\n",
      "Epoch[0], Batch[3392], Train loss: 0.04077022522687912\n",
      "Epoch[0], Val loss: 0.03809154033660889\n",
      "Epoch[0], Batch[3393], Train loss: 0.03636614605784416\n",
      "Epoch[0], Val loss: 0.03646847605705261\n",
      "Epoch[0], Batch[3394], Train loss: 0.03994514048099518\n",
      "Epoch[0], Val loss: 0.03781319782137871\n",
      "Epoch[0], Batch[3395], Train loss: 0.04374557361006737\n",
      "Epoch[0], Val loss: 0.037243302911520004\n",
      "Epoch[0], Batch[3396], Train loss: 0.039271529763936996\n",
      "Epoch[0], Val loss: 0.03524528071284294\n",
      "Epoch[0], Batch[3397], Train loss: 0.03969142585992813\n",
      "Epoch[0], Val loss: 0.03768802806735039\n",
      "Epoch[0], Batch[3398], Train loss: 0.03956819698214531\n",
      "Epoch[0], Val loss: 0.035983018577098846\n",
      "Epoch[0], Batch[3399], Train loss: 0.0391477569937706\n",
      "Epoch[0], Val loss: 0.037808582186698914\n",
      "Epoch[0], Batch[3400], Train loss: 0.04033622890710831\n",
      "Epoch[0], Val loss: 0.03743768855929375\n",
      "Epoch[0], Batch[3401], Train loss: 0.038851298391819\n",
      "Epoch[0], Val loss: 0.03881259635090828\n",
      "Epoch[0], Batch[3402], Train loss: 0.0410880520939827\n",
      "Epoch[0], Val loss: 0.03956294059753418\n",
      "Epoch[0], Batch[3403], Train loss: 0.04017335921525955\n",
      "Epoch[0], Val loss: 0.036756351590156555\n",
      "Epoch[0], Batch[3404], Train loss: 0.03779119625687599\n",
      "Epoch[0], Val loss: 0.03639329597353935\n",
      "Epoch[0], Batch[3405], Train loss: 0.03988843411207199\n",
      "Epoch[0], Val loss: 0.03927469253540039\n",
      "Epoch[0], Batch[3406], Train loss: 0.03937221318483353\n",
      "Epoch[0], Val loss: 0.037864845246076584\n",
      "Epoch[0], Batch[3407], Train loss: 0.04243113473057747\n",
      "Epoch[0], Val loss: 0.037460800260305405\n",
      "Epoch[0], Batch[3408], Train loss: 0.04028534144163132\n",
      "Epoch[0], Val loss: 0.037556253373622894\n",
      "Epoch[0], Batch[3409], Train loss: 0.037457820028066635\n",
      "Epoch[0], Val loss: 0.03767674043774605\n",
      "Epoch[0], Batch[3410], Train loss: 0.03614563122391701\n",
      "Epoch[0], Val loss: 0.038449086248874664\n",
      "Epoch[0], Batch[3411], Train loss: 0.03831394761800766\n",
      "Epoch[0], Val loss: 0.03638571500778198\n",
      "Epoch[0], Batch[3412], Train loss: 0.04004548117518425\n",
      "Epoch[0], Val loss: 0.037508275359869\n",
      "Epoch[0], Batch[3413], Train loss: 0.03611895069479942\n",
      "Epoch[0], Val loss: 0.037534814327955246\n",
      "Epoch[0], Batch[3414], Train loss: 0.03966977447271347\n",
      "Epoch[0], Val loss: 0.03495335578918457\n",
      "Epoch[0], Batch[3415], Train loss: 0.038742028176784515\n",
      "Epoch[0], Val loss: 0.03859355300664902\n",
      "Epoch[0], Batch[3416], Train loss: 0.037741389125585556\n",
      "Epoch[0], Val loss: 0.04102993384003639\n",
      "Epoch[0], Batch[3417], Train loss: 0.03956597298383713\n",
      "Epoch[0], Val loss: 0.03783085197210312\n",
      "Epoch[0], Batch[3418], Train loss: 0.03710455074906349\n",
      "Epoch[0], Val loss: 0.03630725294351578\n",
      "Epoch[0], Batch[3419], Train loss: 0.03804054856300354\n",
      "Epoch[0], Val loss: 0.0397474430501461\n",
      "Epoch[0], Batch[3420], Train loss: 0.04049387946724892\n",
      "Epoch[0], Val loss: 0.034955885261297226\n",
      "Epoch[0], Batch[3421], Train loss: 0.0392712838947773\n",
      "Epoch[0], Val loss: 0.039215296506881714\n",
      "Epoch[0], Batch[3422], Train loss: 0.03945399820804596\n",
      "Epoch[0], Val loss: 0.03965303674340248\n",
      "Epoch[0], Batch[3423], Train loss: 0.038352180272340775\n",
      "Epoch[0], Val loss: 0.037597477436065674\n",
      "Epoch[0], Batch[3424], Train loss: 0.038158901035785675\n",
      "Epoch[0], Val loss: 0.03785807266831398\n",
      "Epoch[0], Batch[3425], Train loss: 0.03947575390338898\n",
      "Epoch[0], Val loss: 0.03566117212176323\n",
      "Epoch[0], Batch[3426], Train loss: 0.03926864266395569\n",
      "Epoch[0], Val loss: 0.03888873755931854\n",
      "Epoch[0], Batch[3427], Train loss: 0.03837361931800842\n",
      "Epoch[0], Val loss: 0.040232397615909576\n",
      "Epoch[0], Batch[3428], Train loss: 0.042076803743839264\n",
      "Epoch[0], Val loss: 0.03882012143731117\n",
      "Epoch[0], Batch[3429], Train loss: 0.04060361534357071\n",
      "Epoch[0], Val loss: 0.040666818618774414\n",
      "Epoch[0], Batch[3430], Train loss: 0.039733197540044785\n",
      "Epoch[0], Val loss: 0.04132246971130371\n",
      "Epoch[0], Batch[3431], Train loss: 0.04010765254497528\n",
      "Epoch[0], Val loss: 0.03728177770972252\n",
      "Epoch[0], Batch[3432], Train loss: 0.043700505048036575\n",
      "Epoch[0], Val loss: 0.04071826860308647\n",
      "Epoch[0], Batch[3433], Train loss: 0.040989313274621964\n",
      "Epoch[0], Val loss: 0.03808670490980148\n",
      "Epoch[0], Batch[3434], Train loss: 0.04151296615600586\n",
      "Epoch[0], Val loss: 0.039516739547252655\n",
      "Epoch[0], Batch[3435], Train loss: 0.03742250055074692\n",
      "Epoch[0], Val loss: 0.03926810622215271\n",
      "Epoch[0], Batch[3436], Train loss: 0.038684725761413574\n",
      "Epoch[0], Val loss: 0.03801729157567024\n",
      "Epoch[0], Batch[3437], Train loss: 0.03977647423744202\n",
      "Epoch[0], Val loss: 0.04030653461813927\n",
      "Epoch[0], Batch[3438], Train loss: 0.03975515440106392\n",
      "Epoch[0], Val loss: 0.03783618286252022\n",
      "Epoch[0], Batch[3439], Train loss: 0.03834892809391022\n",
      "Epoch[0], Val loss: 0.03957202658057213\n",
      "Epoch[0], Batch[3440], Train loss: 0.038190655410289764\n",
      "Epoch[0], Val loss: 0.03688318654894829\n",
      "Epoch[0], Batch[3441], Train loss: 0.040310900658369064\n",
      "Epoch[0], Val loss: 0.04000404104590416\n",
      "Epoch[0], Batch[3442], Train loss: 0.040150925517082214\n",
      "Epoch[0], Val loss: 0.038167864084243774\n",
      "Epoch[0], Batch[3443], Train loss: 0.04020044952630997\n",
      "Epoch[0], Val loss: 0.03950667381286621\n",
      "Epoch[0], Batch[3444], Train loss: 0.040742792189121246\n",
      "Epoch[0], Val loss: 0.039798229932785034\n",
      "Epoch[0], Batch[3445], Train loss: 0.04099978506565094\n",
      "Epoch[0], Val loss: 0.037969302386045456\n",
      "Epoch[0], Batch[3446], Train loss: 0.03827014937996864\n",
      "Epoch[0], Val loss: 0.036785226315259933\n",
      "Epoch[0], Batch[3447], Train loss: 0.039238568395376205\n",
      "Epoch[0], Val loss: 0.03746877610683441\n",
      "Epoch[0], Batch[3448], Train loss: 0.039896510541439056\n",
      "Epoch[0], Val loss: 0.038350872695446014\n",
      "Epoch[0], Batch[3449], Train loss: 0.039193544536828995\n",
      "Epoch[0], Val loss: 0.037858571857213974\n",
      "Epoch[0], Batch[3450], Train loss: 0.038397371768951416\n",
      "Epoch[0], Val loss: 0.03773568943142891\n",
      "Epoch[0], Batch[3451], Train loss: 0.039749618619680405\n",
      "Epoch[0], Val loss: 0.039185795933008194\n",
      "Epoch[0], Batch[3452], Train loss: 0.0392606183886528\n",
      "Epoch[0], Val loss: 0.039050377905368805\n",
      "Epoch[0], Batch[3453], Train loss: 0.03623722121119499\n",
      "Epoch[0], Val loss: 0.035888172686100006\n",
      "Epoch[0], Batch[3454], Train loss: 0.037142910063266754\n",
      "Epoch[0], Val loss: 0.03590857610106468\n",
      "Epoch[0], Batch[3455], Train loss: 0.039001259952783585\n",
      "Epoch[0], Val loss: 0.037608321756124496\n",
      "Epoch[0], Batch[3456], Train loss: 0.0382222980260849\n",
      "Epoch[0], Val loss: 0.037782810628414154\n",
      "Epoch[0], Batch[3457], Train loss: 0.04029090702533722\n",
      "Epoch[0], Val loss: 0.03769254684448242\n",
      "Epoch[0], Batch[3458], Train loss: 0.04265085980296135\n",
      "Epoch[0], Val loss: 0.038661446422338486\n",
      "Epoch[0], Batch[3459], Train loss: 0.04120947793126106\n",
      "Epoch[0], Val loss: 0.03697900101542473\n",
      "Epoch[0], Batch[3460], Train loss: 0.03954144939780235\n",
      "Epoch[0], Val loss: 0.0371972918510437\n",
      "Epoch[0], Batch[3461], Train loss: 0.04064754396677017\n",
      "Epoch[0], Val loss: 0.04050753265619278\n",
      "Epoch[0], Batch[3462], Train loss: 0.04236475005745888\n",
      "Epoch[0], Val loss: 0.03513593599200249\n",
      "Epoch[0], Batch[3463], Train loss: 0.03790724277496338\n",
      "Epoch[0], Val loss: 0.037366848438978195\n",
      "Epoch[0], Batch[3464], Train loss: 0.03704244643449783\n",
      "Epoch[0], Val loss: 0.036564577370882034\n",
      "Epoch[0], Batch[3465], Train loss: 0.03888368234038353\n",
      "Epoch[0], Val loss: 0.038640096783638\n",
      "Epoch[0], Batch[3466], Train loss: 0.03608233109116554\n",
      "Epoch[0], Val loss: 0.037978947162628174\n",
      "Epoch[0], Batch[3467], Train loss: 0.038147084414958954\n",
      "Epoch[0], Val loss: 0.038060080260038376\n",
      "Epoch[0], Batch[3468], Train loss: 0.041108906269073486\n",
      "Epoch[0], Val loss: 0.03716256842017174\n",
      "Epoch[0], Batch[3469], Train loss: 0.040637873113155365\n",
      "Epoch[0], Val loss: 0.03714754059910774\n",
      "Epoch[0], Batch[3470], Train loss: 0.03852133825421333\n",
      "Epoch[0], Val loss: 0.03828290477395058\n",
      "Epoch[0], Batch[3471], Train loss: 0.03818855807185173\n",
      "Epoch[0], Val loss: 0.03858962655067444\n",
      "Epoch[0], Batch[3472], Train loss: 0.03807169944047928\n",
      "Epoch[0], Val loss: 0.03607970103621483\n",
      "Epoch[0], Batch[3473], Train loss: 0.037478815764188766\n",
      "Epoch[0], Val loss: 0.03798571601510048\n",
      "Epoch[0], Batch[3474], Train loss: 0.037476856261491776\n",
      "Epoch[0], Val loss: 0.0360267274081707\n",
      "Epoch[0], Batch[3475], Train loss: 0.03985856845974922\n",
      "Epoch[0], Val loss: 0.037629347294569016\n",
      "Epoch[0], Batch[3476], Train loss: 0.038228679448366165\n",
      "Epoch[0], Val loss: 0.039536140859127045\n",
      "Epoch[0], Batch[3477], Train loss: 0.0354832224547863\n",
      "Epoch[0], Val loss: 0.03943699598312378\n",
      "Epoch[0], Batch[3478], Train loss: 0.03824206814169884\n",
      "Epoch[0], Val loss: 0.041658055037260056\n",
      "Epoch[0], Batch[3479], Train loss: 0.0384664349257946\n",
      "Epoch[0], Val loss: 0.03937456011772156\n",
      "Epoch[0], Batch[3480], Train loss: 0.0399726964533329\n",
      "Epoch[0], Val loss: 0.0384250245988369\n",
      "Epoch[0], Batch[3481], Train loss: 0.03806931897997856\n",
      "Epoch[0], Val loss: 0.040547631680965424\n",
      "Epoch[0], Batch[3482], Train loss: 0.040139660239219666\n",
      "Epoch[0], Val loss: 0.0390593446791172\n",
      "Epoch[0], Batch[3483], Train loss: 0.04126419499516487\n",
      "Epoch[0], Val loss: 0.03641336038708687\n",
      "Epoch[0], Batch[3484], Train loss: 0.0393354706466198\n",
      "Epoch[0], Val loss: 0.039528440684080124\n",
      "Epoch[0], Batch[3485], Train loss: 0.038271818310022354\n",
      "Epoch[0], Val loss: 0.03608178719878197\n",
      "Epoch[0], Batch[3486], Train loss: 0.03812255710363388\n",
      "Epoch[0], Val loss: 0.04011888802051544\n",
      "Epoch[0], Batch[3487], Train loss: 0.04187154397368431\n",
      "Epoch[0], Val loss: 0.03884294256567955\n",
      "Epoch[0], Batch[3488], Train loss: 0.04076508432626724\n",
      "Epoch[0], Val loss: 0.03766284137964249\n",
      "Epoch[0], Batch[3489], Train loss: 0.037011124193668365\n",
      "Epoch[0], Val loss: 0.03869134187698364\n",
      "Epoch[0], Batch[3490], Train loss: 0.042312141507864\n",
      "Epoch[0], Val loss: 0.043675974011421204\n",
      "Epoch[0], Batch[3491], Train loss: 0.03895030915737152\n",
      "Epoch[0], Val loss: 0.04017885401844978\n",
      "Epoch[0], Batch[3492], Train loss: 0.03662269935011864\n",
      "Epoch[0], Val loss: 0.03595002740621567\n",
      "Epoch[0], Batch[3493], Train loss: 0.037845201790332794\n",
      "Epoch[0], Val loss: 0.03627735748887062\n",
      "Epoch[0], Batch[3494], Train loss: 0.03852694481611252\n",
      "Epoch[0], Val loss: 0.03832731023430824\n",
      "Epoch[0], Batch[3495], Train loss: 0.039547041058540344\n",
      "Epoch[0], Val loss: 0.037815578281879425\n",
      "Epoch[0], Batch[3496], Train loss: 0.04001738503575325\n",
      "Epoch[0], Val loss: 0.03911207988858223\n",
      "Epoch[0], Batch[3497], Train loss: 0.03921181708574295\n",
      "Epoch[0], Val loss: 0.038994863629341125\n",
      "Epoch[0], Batch[3498], Train loss: 0.03801892697811127\n",
      "Epoch[0], Val loss: 0.038809772580862045\n",
      "Epoch[0], Batch[3499], Train loss: 0.03890558332204819\n",
      "Epoch[0], Val loss: 0.03748412802815437\n",
      "Epoch[0], Batch[3500], Train loss: 0.04174429923295975\n",
      "Epoch[0], Val loss: 0.034491825848817825\n",
      "Epoch[0], Batch[3501], Train loss: 0.03979375958442688\n",
      "Epoch[0], Val loss: 0.03676001727581024\n",
      "Epoch[0], Batch[3502], Train loss: 0.040003903210163116\n",
      "Epoch[0], Val loss: 0.03828711807727814\n",
      "Epoch[0], Batch[3503], Train loss: 0.03770558163523674\n",
      "Epoch[0], Val loss: 0.035193778574466705\n",
      "Epoch[0], Batch[3504], Train loss: 0.038773685693740845\n",
      "Epoch[0], Val loss: 0.03941771388053894\n",
      "Epoch[0], Batch[3505], Train loss: 0.03894350677728653\n",
      "Epoch[0], Val loss: 0.039494309574365616\n",
      "Epoch[0], Batch[3506], Train loss: 0.03792732208967209\n",
      "Epoch[0], Val loss: 0.03667839616537094\n",
      "Epoch[0], Batch[3507], Train loss: 0.03815772384405136\n",
      "Epoch[0], Val loss: 0.03731947019696236\n",
      "Epoch[0], Batch[3508], Train loss: 0.04272886738181114\n",
      "Epoch[0], Val loss: 0.037444133311510086\n",
      "Epoch[0], Batch[3509], Train loss: 0.03897161781787872\n",
      "Epoch[0], Val loss: 0.03693623095750809\n",
      "Epoch[0], Batch[3510], Train loss: 0.037910737097263336\n",
      "Epoch[0], Val loss: 0.03860018029808998\n",
      "Epoch[0], Batch[3511], Train loss: 0.03951987251639366\n",
      "Epoch[0], Val loss: 0.036140840500593185\n",
      "Epoch[0], Batch[3512], Train loss: 0.03986196964979172\n",
      "Epoch[0], Val loss: 0.03539694845676422\n",
      "Epoch[0], Batch[3513], Train loss: 0.03884734585881233\n",
      "Epoch[0], Val loss: 0.03779475763440132\n",
      "Epoch[0], Batch[3514], Train loss: 0.03954359143972397\n",
      "Epoch[0], Val loss: 0.03692971542477608\n",
      "Epoch[0], Batch[3515], Train loss: 0.03750266879796982\n",
      "Epoch[0], Val loss: 0.03719641640782356\n",
      "Epoch[0], Batch[3516], Train loss: 0.03833087533712387\n",
      "Epoch[0], Val loss: 0.03729349002242088\n",
      "Epoch[0], Batch[3517], Train loss: 0.03881920501589775\n",
      "Epoch[0], Val loss: 0.03678347170352936\n",
      "Epoch[0], Batch[3518], Train loss: 0.037261392921209335\n",
      "Epoch[0], Val loss: 0.03614523261785507\n",
      "Epoch[0], Batch[3519], Train loss: 0.04007403925061226\n",
      "Epoch[0], Val loss: 0.0395146869122982\n",
      "Epoch[0], Batch[3520], Train loss: 0.04049647971987724\n",
      "Epoch[0], Val loss: 0.03734668344259262\n",
      "Epoch[0], Batch[3521], Train loss: 0.03553557023406029\n",
      "Epoch[0], Val loss: 0.037245795130729675\n",
      "Epoch[0], Batch[3522], Train loss: 0.03723528981208801\n",
      "Epoch[0], Val loss: 0.03867604210972786\n",
      "Epoch[0], Batch[3523], Train loss: 0.039275139570236206\n",
      "Epoch[0], Val loss: 0.0377318374812603\n",
      "Epoch[0], Batch[3524], Train loss: 0.039710648357868195\n",
      "Epoch[0], Val loss: 0.03940780460834503\n",
      "Epoch[0], Batch[3525], Train loss: 0.03874453902244568\n",
      "Epoch[0], Val loss: 0.0388554185628891\n",
      "Epoch[0], Batch[3526], Train loss: 0.03684602305293083\n",
      "Epoch[0], Val loss: 0.03680817782878876\n",
      "Epoch[0], Batch[3527], Train loss: 0.03820830583572388\n",
      "Epoch[0], Val loss: 0.03662336617708206\n",
      "Epoch[0], Batch[3528], Train loss: 0.039790935814380646\n",
      "Epoch[0], Val loss: 0.04220445454120636\n",
      "Epoch[0], Batch[3529], Train loss: 0.03934607282280922\n",
      "Epoch[0], Val loss: 0.038891274482011795\n",
      "Epoch[0], Batch[3530], Train loss: 0.038981661200523376\n",
      "Epoch[0], Val loss: 0.03625638037919998\n",
      "Epoch[0], Batch[3531], Train loss: 0.040776576846838\n",
      "Epoch[0], Val loss: 0.03638535737991333\n",
      "Epoch[0], Batch[3532], Train loss: 0.0374688096344471\n",
      "Epoch[0], Val loss: 0.038092002272605896\n",
      "Epoch[0], Batch[3533], Train loss: 0.04058726504445076\n",
      "Epoch[0], Val loss: 0.0363599956035614\n",
      "Epoch[0], Batch[3534], Train loss: 0.03698502480983734\n",
      "Epoch[0], Val loss: 0.03607800230383873\n",
      "Epoch[0], Batch[3535], Train loss: 0.04016612097620964\n",
      "Epoch[0], Val loss: 0.03778720274567604\n",
      "Epoch[0], Batch[3536], Train loss: 0.038041554391384125\n",
      "Epoch[0], Val loss: 0.037236250936985016\n",
      "Epoch[0], Batch[3537], Train loss: 0.04150744900107384\n",
      "Epoch[0], Val loss: 0.03561265021562576\n",
      "Epoch[0], Batch[3538], Train loss: 0.03902006521821022\n",
      "Epoch[0], Val loss: 0.03490221127867699\n",
      "Epoch[0], Batch[3539], Train loss: 0.03911123797297478\n",
      "Epoch[0], Val loss: 0.037296563386917114\n",
      "Epoch[0], Batch[3540], Train loss: 0.04142441600561142\n",
      "Epoch[0], Val loss: 0.038063906133174896\n",
      "Epoch[0], Batch[3541], Train loss: 0.03688150644302368\n",
      "Epoch[0], Val loss: 0.03500467911362648\n",
      "Epoch[0], Batch[3542], Train loss: 0.03873825445771217\n",
      "Epoch[0], Val loss: 0.03690030053257942\n",
      "Epoch[0], Batch[3543], Train loss: 0.03957369923591614\n",
      "Epoch[0], Val loss: 0.03910030052065849\n",
      "Epoch[0], Batch[3544], Train loss: 0.03743943199515343\n",
      "Epoch[0], Val loss: 0.037476830184459686\n",
      "Epoch[0], Batch[3545], Train loss: 0.03945155069231987\n",
      "Epoch[0], Val loss: 0.037291161715984344\n",
      "Epoch[0], Batch[3546], Train loss: 0.040753256529569626\n",
      "Epoch[0], Val loss: 0.0364990308880806\n",
      "Epoch[0], Batch[3547], Train loss: 0.04205826297402382\n",
      "Epoch[0], Val loss: 0.035545822232961655\n",
      "Epoch[0], Batch[3548], Train loss: 0.039075445383787155\n",
      "Epoch[0], Val loss: 0.03608347848057747\n",
      "Epoch[0], Batch[3549], Train loss: 0.037785109132528305\n",
      "Epoch[0], Val loss: 0.03821689635515213\n",
      "Epoch[0], Batch[3550], Train loss: 0.0390658974647522\n",
      "Epoch[0], Val loss: 0.03821298107504845\n",
      "Epoch[0], Batch[3551], Train loss: 0.039878007024526596\n",
      "Epoch[0], Val loss: 0.038237329572439194\n",
      "Epoch[0], Batch[3552], Train loss: 0.039443690329790115\n",
      "Epoch[0], Val loss: 0.0370657816529274\n",
      "Epoch[0], Batch[3553], Train loss: 0.035799890756607056\n",
      "Epoch[0], Val loss: 0.036462534219026566\n",
      "Epoch[0], Batch[3554], Train loss: 0.03929515555500984\n",
      "Epoch[0], Val loss: 0.036948610097169876\n",
      "Epoch[0], Batch[3555], Train loss: 0.037134479731321335\n",
      "Epoch[0], Val loss: 0.037631966173648834\n",
      "Epoch[0], Batch[3556], Train loss: 0.03869098052382469\n",
      "Epoch[0], Val loss: 0.036659497767686844\n",
      "Epoch[0], Batch[3557], Train loss: 0.0371888242661953\n",
      "Epoch[0], Val loss: 0.03623952716588974\n",
      "Epoch[0], Batch[3558], Train loss: 0.03953056037425995\n",
      "Epoch[0], Val loss: 0.037705354392528534\n",
      "Epoch[0], Batch[3559], Train loss: 0.04111584648489952\n",
      "Epoch[0], Val loss: 0.03486601263284683\n",
      "Epoch[0], Batch[3560], Train loss: 0.03621682524681091\n",
      "Epoch[0], Val loss: 0.035879988223314285\n",
      "Epoch[0], Batch[3561], Train loss: 0.0411858893930912\n",
      "Epoch[0], Val loss: 0.03808292746543884\n",
      "Epoch[0], Batch[3562], Train loss: 0.037894733250141144\n",
      "Epoch[0], Val loss: 0.04046621173620224\n",
      "Epoch[0], Batch[3563], Train loss: 0.040327247232198715\n",
      "Epoch[0], Val loss: 0.03931165859103203\n",
      "Epoch[0], Batch[3564], Train loss: 0.03969506174325943\n",
      "Epoch[0], Val loss: 0.036622386425733566\n",
      "Epoch[0], Batch[3565], Train loss: 0.03852607682347298\n",
      "Epoch[0], Val loss: 0.038894932717084885\n",
      "Epoch[0], Batch[3566], Train loss: 0.03998683765530586\n",
      "Epoch[0], Val loss: 0.037618380039930344\n",
      "Epoch[0], Batch[3567], Train loss: 0.04079318419098854\n",
      "Epoch[0], Val loss: 0.03573787584900856\n",
      "Epoch[0], Batch[3568], Train loss: 0.04197414964437485\n",
      "Epoch[0], Val loss: 0.040139324963092804\n",
      "Epoch[0], Batch[3569], Train loss: 0.03941632807254791\n",
      "Epoch[0], Val loss: 0.040609486401081085\n",
      "Epoch[0], Batch[3570], Train loss: 0.03806176036596298\n",
      "Epoch[0], Val loss: 0.03828584775328636\n",
      "Epoch[0], Batch[3571], Train loss: 0.0375681072473526\n",
      "Epoch[0], Val loss: 0.03851734474301338\n",
      "Epoch[0], Batch[3572], Train loss: 0.040124908089637756\n",
      "Epoch[0], Val loss: 0.03883634880185127\n",
      "Epoch[0], Batch[3573], Train loss: 0.03842639550566673\n",
      "Epoch[0], Val loss: 0.03829634562134743\n",
      "Epoch[0], Batch[3574], Train loss: 0.039881058037281036\n",
      "Epoch[0], Val loss: 0.03670421615242958\n",
      "Epoch[0], Batch[3575], Train loss: 0.03961638733744621\n",
      "Epoch[0], Val loss: 0.03672575205564499\n",
      "Epoch[0], Batch[3576], Train loss: 0.04074309393763542\n",
      "Epoch[0], Val loss: 0.03437322378158569\n",
      "Epoch[0], Batch[3577], Train loss: 0.03711959719657898\n",
      "Epoch[0], Val loss: 0.03795392066240311\n",
      "Epoch[0], Batch[3578], Train loss: 0.039272893220186234\n",
      "Epoch[0], Val loss: 0.03807152435183525\n",
      "Epoch[0], Batch[3579], Train loss: 0.038715261965990067\n",
      "Epoch[0], Val loss: 0.035124897956848145\n",
      "Epoch[0], Batch[3580], Train loss: 0.04109424352645874\n",
      "Epoch[0], Val loss: 0.038307249546051025\n",
      "Epoch[0], Batch[3581], Train loss: 0.04043639451265335\n",
      "Epoch[0], Val loss: 0.03601498529314995\n",
      "Epoch[0], Batch[3582], Train loss: 0.039657142013311386\n",
      "Epoch[0], Val loss: 0.03625520318746567\n",
      "Epoch[0], Batch[3583], Train loss: 0.038084667176008224\n",
      "Epoch[0], Val loss: 0.040183719247579575\n",
      "Epoch[0], Batch[3584], Train loss: 0.04125223308801651\n",
      "Epoch[0], Val loss: 0.0351199135184288\n",
      "Epoch[0], Batch[3585], Train loss: 0.03924540430307388\n",
      "Epoch[0], Val loss: 0.038567330688238144\n",
      "Epoch[0], Batch[3586], Train loss: 0.040852565318346024\n",
      "Epoch[0], Val loss: 0.03481840342283249\n",
      "Epoch[0], Batch[3587], Train loss: 0.03722347319126129\n",
      "Epoch[0], Val loss: 0.03910357505083084\n",
      "Epoch[0], Batch[3588], Train loss: 0.03746768459677696\n",
      "Epoch[0], Val loss: 0.03903447836637497\n",
      "Epoch[0], Batch[3589], Train loss: 0.039552848786115646\n",
      "Epoch[0], Val loss: 0.03760537505149841\n",
      "Epoch[0], Batch[3590], Train loss: 0.03690802678465843\n",
      "Epoch[0], Val loss: 0.0363972969353199\n",
      "Epoch[0], Batch[3591], Train loss: 0.03873461112380028\n",
      "Epoch[0], Val loss: 0.037039533257484436\n",
      "Epoch[0], Batch[3592], Train loss: 0.035622384399175644\n",
      "Epoch[0], Val loss: 0.03535865247249603\n",
      "Epoch[0], Batch[3593], Train loss: 0.03911086171865463\n",
      "Epoch[0], Val loss: 0.036856818944215775\n",
      "Epoch[0], Batch[3594], Train loss: 0.038294535130262375\n",
      "Epoch[0], Val loss: 0.03675113990902901\n",
      "Epoch[0], Batch[3595], Train loss: 0.03704584762454033\n",
      "Epoch[0], Val loss: 0.037064068019390106\n",
      "Epoch[0], Batch[3596], Train loss: 0.04024958983063698\n",
      "Epoch[0], Val loss: 0.03673157840967178\n",
      "Epoch[0], Batch[3597], Train loss: 0.03776515647768974\n",
      "Epoch[0], Val loss: 0.03761550784111023\n",
      "Epoch[0], Batch[3598], Train loss: 0.03528909012675285\n",
      "Epoch[0], Val loss: 0.039241183549165726\n",
      "Epoch[0], Batch[3599], Train loss: 0.03961935266852379\n",
      "Epoch[0], Val loss: 0.03700343519449234\n",
      "Epoch[0], Batch[3600], Train loss: 0.039530690759420395\n",
      "Epoch[0], Val loss: 0.03897271677851677\n",
      "Epoch[0], Batch[3601], Train loss: 0.0375906266272068\n",
      "Epoch[0], Val loss: 0.03779924660921097\n",
      "Epoch[0], Batch[3602], Train loss: 0.03871439769864082\n",
      "Epoch[0], Val loss: 0.03789098188281059\n",
      "Epoch[0], Batch[3603], Train loss: 0.03783056139945984\n",
      "Epoch[0], Val loss: 0.037871167063713074\n",
      "Epoch[0], Batch[3604], Train loss: 0.039656709879636765\n",
      "Epoch[0], Val loss: 0.035976674407720566\n",
      "Epoch[0], Batch[3605], Train loss: 0.03987810015678406\n",
      "Epoch[0], Val loss: 0.03866063058376312\n",
      "Epoch[0], Batch[3606], Train loss: 0.04071991518139839\n",
      "Epoch[0], Val loss: 0.0377487987279892\n",
      "Epoch[0], Batch[3607], Train loss: 0.037066247314214706\n",
      "Epoch[0], Val loss: 0.037672847509384155\n",
      "Epoch[0], Batch[3608], Train loss: 0.04016702622175217\n",
      "Epoch[0], Val loss: 0.03848615288734436\n",
      "Epoch[0], Batch[3609], Train loss: 0.037380099296569824\n",
      "Epoch[0], Val loss: 0.038088731467723846\n",
      "Epoch[0], Batch[3610], Train loss: 0.038430240005254745\n",
      "Epoch[0], Val loss: 0.038303278386592865\n",
      "Epoch[0], Batch[3611], Train loss: 0.04228846728801727\n",
      "Epoch[0], Val loss: 0.03722907602787018\n",
      "Epoch[0], Batch[3612], Train loss: 0.038577549159526825\n",
      "Epoch[0], Val loss: 0.03631386160850525\n",
      "Epoch[0], Batch[3613], Train loss: 0.03875824809074402\n",
      "Epoch[0], Val loss: 0.03751680627465248\n",
      "Epoch[0], Batch[3614], Train loss: 0.03613211587071419\n",
      "Epoch[0], Val loss: 0.03533191233873367\n",
      "Epoch[0], Batch[3615], Train loss: 0.03620472922921181\n",
      "Epoch[0], Val loss: 0.03592758625745773\n",
      "Epoch[0], Batch[3616], Train loss: 0.038646478205919266\n",
      "Epoch[0], Val loss: 0.03949108347296715\n",
      "Epoch[0], Batch[3617], Train loss: 0.039725545793771744\n",
      "Epoch[0], Val loss: 0.03954101726412773\n",
      "Epoch[0], Batch[3618], Train loss: 0.03800101950764656\n",
      "Epoch[0], Val loss: 0.03567269816994667\n",
      "Epoch[0], Batch[3619], Train loss: 0.039803653955459595\n",
      "Epoch[0], Val loss: 0.036416519433259964\n",
      "Epoch[0], Batch[3620], Train loss: 0.03781059384346008\n",
      "Epoch[0], Val loss: 0.03615126013755798\n",
      "Epoch[0], Batch[3621], Train loss: 0.03768453747034073\n",
      "Epoch[0], Val loss: 0.035544637590646744\n",
      "Epoch[0], Batch[3622], Train loss: 0.04023526981472969\n",
      "Epoch[0], Val loss: 0.03517812862992287\n",
      "Epoch[0], Batch[3623], Train loss: 0.040253426879644394\n",
      "Epoch[0], Val loss: 0.0388658232986927\n",
      "Epoch[0], Batch[3624], Train loss: 0.03878716379404068\n",
      "Epoch[0], Val loss: 0.04121850058436394\n",
      "Epoch[0], Batch[3625], Train loss: 0.03809875622391701\n",
      "Epoch[0], Val loss: 0.0354771763086319\n",
      "Epoch[0], Batch[3626], Train loss: 0.038114070892333984\n",
      "Epoch[0], Val loss: 0.0372547022998333\n",
      "Epoch[0], Batch[3627], Train loss: 0.03846222534775734\n",
      "Epoch[0], Val loss: 0.03906307741999626\n",
      "Epoch[0], Batch[3628], Train loss: 0.03807244077324867\n",
      "Epoch[0], Val loss: 0.03444800153374672\n",
      "Epoch[0], Batch[3629], Train loss: 0.03750445693731308\n",
      "Epoch[0], Val loss: 0.03832317143678665\n",
      "Epoch[0], Batch[3630], Train loss: 0.03974476829171181\n",
      "Epoch[0], Val loss: 0.036355454474687576\n",
      "Epoch[0], Batch[3631], Train loss: 0.03866273909807205\n",
      "Epoch[0], Val loss: 0.038028012961149216\n",
      "Epoch[0], Batch[3632], Train loss: 0.03836866468191147\n",
      "Epoch[0], Val loss: 0.03794863820075989\n",
      "Epoch[0], Batch[3633], Train loss: 0.037631019949913025\n",
      "Epoch[0], Val loss: 0.035787247121334076\n",
      "Epoch[0], Batch[3634], Train loss: 0.038257330656051636\n",
      "Epoch[0], Val loss: 0.03968016430735588\n",
      "Epoch[0], Batch[3635], Train loss: 0.0403154194355011\n",
      "Epoch[0], Val loss: 0.039096198976039886\n",
      "Epoch[0], Batch[3636], Train loss: 0.03992745280265808\n",
      "Epoch[0], Val loss: 0.03605561703443527\n",
      "Epoch[0], Batch[3637], Train loss: 0.03891456127166748\n",
      "Epoch[0], Val loss: 0.03857961297035217\n",
      "Epoch[0], Batch[3638], Train loss: 0.03926192224025726\n",
      "Epoch[0], Val loss: 0.036174822598695755\n",
      "Epoch[0], Batch[3639], Train loss: 0.03761066868901253\n",
      "Epoch[0], Val loss: 0.036726076155900955\n",
      "Epoch[0], Batch[3640], Train loss: 0.03848959878087044\n",
      "Epoch[0], Val loss: 0.039608970284461975\n",
      "Epoch[0], Batch[3641], Train loss: 0.038023997098207474\n",
      "Epoch[0], Val loss: 0.0368889719247818\n",
      "Epoch[0], Batch[3642], Train loss: 0.03695656731724739\n",
      "Epoch[0], Val loss: 0.03522785007953644\n",
      "Epoch[0], Batch[3643], Train loss: 0.03713596239686012\n",
      "Epoch[0], Val loss: 0.040717463940382004\n",
      "Epoch[0], Batch[3644], Train loss: 0.04058685153722763\n",
      "Epoch[0], Val loss: 0.034945763647556305\n",
      "Epoch[0], Batch[3645], Train loss: 0.038498278707265854\n",
      "Epoch[0], Val loss: 0.03795524686574936\n",
      "Epoch[0], Batch[3646], Train loss: 0.038039352744817734\n",
      "Epoch[0], Val loss: 0.034951113164424896\n",
      "Epoch[0], Batch[3647], Train loss: 0.04263050854206085\n",
      "Epoch[0], Val loss: 0.03682207688689232\n",
      "Epoch[0], Batch[3648], Train loss: 0.03803948685526848\n",
      "Epoch[0], Val loss: 0.036739639937877655\n",
      "Epoch[0], Batch[3649], Train loss: 0.03886440768837929\n",
      "Epoch[0], Val loss: 0.039002444595098495\n",
      "Epoch[0], Batch[3650], Train loss: 0.03761128708720207\n",
      "Epoch[0], Val loss: 0.03642026707530022\n",
      "Epoch[0], Batch[3651], Train loss: 0.039537280797958374\n",
      "Epoch[0], Val loss: 0.03671446442604065\n",
      "Epoch[0], Batch[3652], Train loss: 0.03817012533545494\n",
      "Epoch[0], Val loss: 0.03531944379210472\n",
      "Epoch[0], Batch[3653], Train loss: 0.03753446787595749\n",
      "Epoch[0], Val loss: 0.03769291937351227\n",
      "Epoch[0], Batch[3654], Train loss: 0.0370856449007988\n",
      "Epoch[0], Val loss: 0.04016812890768051\n",
      "Epoch[0], Batch[3655], Train loss: 0.03752148523926735\n",
      "Epoch[0], Val loss: 0.03607752174139023\n",
      "Epoch[0], Batch[3656], Train loss: 0.03708086907863617\n",
      "Epoch[0], Val loss: 0.03664858266711235\n",
      "Epoch[0], Batch[3657], Train loss: 0.04040102660655975\n",
      "Epoch[0], Val loss: 0.03760649636387825\n",
      "Epoch[0], Batch[3658], Train loss: 0.03643008694052696\n",
      "Epoch[0], Val loss: 0.036525171250104904\n",
      "Epoch[0], Batch[3659], Train loss: 0.0358707495033741\n",
      "Epoch[0], Val loss: 0.0382964201271534\n",
      "Epoch[0], Batch[3660], Train loss: 0.038009919226169586\n",
      "Epoch[0], Val loss: 0.03798637539148331\n",
      "Epoch[0], Batch[3661], Train loss: 0.04063619673252106\n",
      "Epoch[0], Val loss: 0.038432132452726364\n",
      "Epoch[0], Batch[3662], Train loss: 0.03826579079031944\n",
      "Epoch[0], Val loss: 0.03642161563038826\n",
      "Epoch[0], Batch[3663], Train loss: 0.03881998732686043\n",
      "Epoch[0], Val loss: 0.03644649311900139\n",
      "Epoch[0], Batch[3664], Train loss: 0.038031935691833496\n",
      "Epoch[0], Val loss: 0.037478141486644745\n",
      "Epoch[0], Batch[3665], Train loss: 0.03667166456580162\n",
      "Epoch[0], Val loss: 0.03699270263314247\n",
      "Epoch[0], Batch[3666], Train loss: 0.0391358882188797\n",
      "Epoch[0], Val loss: 0.03818386793136597\n",
      "Epoch[0], Batch[3667], Train loss: 0.03720371052622795\n",
      "Epoch[0], Val loss: 0.03510129451751709\n",
      "Epoch[0], Batch[3668], Train loss: 0.03679715469479561\n",
      "Epoch[0], Val loss: 0.03619908168911934\n",
      "Epoch[0], Batch[3669], Train loss: 0.038150135427713394\n",
      "Epoch[0], Val loss: 0.037284255027770996\n",
      "Epoch[0], Batch[3670], Train loss: 0.03814668953418732\n",
      "Epoch[0], Val loss: 0.036854520440101624\n",
      "Epoch[0], Batch[3671], Train loss: 0.0406542606651783\n",
      "Epoch[0], Val loss: 0.03991394117474556\n",
      "Epoch[0], Batch[3672], Train loss: 0.03797328099608421\n",
      "Epoch[0], Val loss: 0.03516919165849686\n",
      "Epoch[0], Batch[3673], Train loss: 0.038059305399656296\n",
      "Epoch[0], Val loss: 0.03874364495277405\n",
      "Epoch[0], Batch[3674], Train loss: 0.038707297295331955\n",
      "Epoch[0], Val loss: 0.03677628934383392\n",
      "Epoch[0], Batch[3675], Train loss: 0.03659233823418617\n",
      "Epoch[0], Val loss: 0.03676619753241539\n",
      "Epoch[0], Batch[3676], Train loss: 0.03860727697610855\n",
      "Epoch[0], Val loss: 0.03484887629747391\n",
      "Epoch[0], Batch[3677], Train loss: 0.041112951934337616\n",
      "Epoch[0], Val loss: 0.03672721982002258\n",
      "Epoch[0], Batch[3678], Train loss: 0.03827279433608055\n",
      "Epoch[0], Val loss: 0.036845613270998\n",
      "Epoch[0], Batch[3679], Train loss: 0.03828896954655647\n",
      "Epoch[0], Val loss: 0.0364820696413517\n",
      "Epoch[0], Batch[3680], Train loss: 0.038978349417448044\n",
      "Epoch[0], Val loss: 0.037372052669525146\n",
      "Epoch[0], Batch[3681], Train loss: 0.03747688978910446\n",
      "Epoch[0], Val loss: 0.03493982553482056\n",
      "Epoch[0], Batch[3682], Train loss: 0.040669143199920654\n",
      "Epoch[0], Val loss: 0.0374813936650753\n",
      "Epoch[0], Batch[3683], Train loss: 0.03971279785037041\n",
      "Epoch[0], Val loss: 0.035761408507823944\n",
      "Epoch[0], Batch[3684], Train loss: 0.03574434295296669\n",
      "Epoch[0], Val loss: 0.038324374705553055\n",
      "Epoch[0], Batch[3685], Train loss: 0.03763522952795029\n",
      "Epoch[0], Val loss: 0.03501298278570175\n",
      "Epoch[0], Batch[3686], Train loss: 0.03971360996365547\n",
      "Epoch[0], Val loss: 0.038992639631032944\n",
      "Epoch[0], Batch[3687], Train loss: 0.03777945041656494\n",
      "Epoch[0], Val loss: 0.03889809921383858\n",
      "Epoch[0], Batch[3688], Train loss: 0.037999048829078674\n",
      "Epoch[0], Val loss: 0.03722751885652542\n",
      "Epoch[0], Batch[3689], Train loss: 0.04092685878276825\n",
      "Epoch[0], Val loss: 0.04002353921532631\n",
      "Epoch[0], Batch[3690], Train loss: 0.039375510066747665\n",
      "Epoch[0], Val loss: 0.038755763322114944\n",
      "Epoch[0], Batch[3691], Train loss: 0.0395660325884819\n",
      "Epoch[0], Val loss: 0.04066883400082588\n",
      "Epoch[0], Batch[3692], Train loss: 0.037967339158058167\n",
      "Epoch[0], Val loss: 0.03775680065155029\n",
      "Epoch[0], Batch[3693], Train loss: 0.03878166899085045\n",
      "Epoch[0], Val loss: 0.034753818064928055\n",
      "Epoch[0], Batch[3694], Train loss: 0.038225434720516205\n",
      "Epoch[0], Val loss: 0.0385797880589962\n",
      "Epoch[0], Batch[3695], Train loss: 0.03825004771351814\n",
      "Epoch[0], Val loss: 0.038954492658376694\n",
      "Epoch[0], Batch[3696], Train loss: 0.04004210606217384\n",
      "Epoch[0], Val loss: 0.03914261981844902\n",
      "Epoch[0], Batch[3697], Train loss: 0.03794312849640846\n",
      "Epoch[0], Val loss: 0.03600810095667839\n",
      "Epoch[0], Batch[3698], Train loss: 0.03839988633990288\n",
      "Epoch[0], Val loss: 0.04112676531076431\n",
      "Epoch[0], Batch[3699], Train loss: 0.03667959198355675\n",
      "Epoch[0], Val loss: 0.037625644356012344\n",
      "Epoch[0], Batch[3700], Train loss: 0.03997291624546051\n",
      "Epoch[0], Val loss: 0.03606807440519333\n",
      "Epoch[0], Batch[3701], Train loss: 0.039499763399362564\n",
      "Epoch[0], Val loss: 0.03911843150854111\n",
      "Epoch[0], Batch[3702], Train loss: 0.03714582696557045\n",
      "Epoch[0], Val loss: 0.036900028586387634\n",
      "Epoch[0], Batch[3703], Train loss: 0.03723258525133133\n",
      "Epoch[0], Val loss: 0.035880785435438156\n",
      "Epoch[0], Batch[3704], Train loss: 0.038326576352119446\n",
      "Epoch[0], Val loss: 0.03615666925907135\n",
      "Epoch[0], Batch[3705], Train loss: 0.0379636324942112\n",
      "Epoch[0], Val loss: 0.03775087371468544\n",
      "Epoch[0], Batch[3706], Train loss: 0.038738057017326355\n",
      "Epoch[0], Val loss: 0.037391867488622665\n",
      "Epoch[0], Batch[3707], Train loss: 0.039101600646972656\n",
      "Epoch[0], Val loss: 0.038147494196891785\n",
      "Epoch[0], Batch[3708], Train loss: 0.038721777498722076\n",
      "Epoch[0], Val loss: 0.03768409416079521\n",
      "Epoch[0], Batch[3709], Train loss: 0.03716154024004936\n",
      "Epoch[0], Val loss: 0.03790345415472984\n",
      "Epoch[0], Batch[3710], Train loss: 0.037099439650774\n",
      "Epoch[0], Val loss: 0.038017965853214264\n",
      "Epoch[0], Batch[3711], Train loss: 0.03703873232007027\n",
      "Epoch[0], Val loss: 0.03622795268893242\n",
      "Epoch[0], Batch[3712], Train loss: 0.03799137473106384\n",
      "Epoch[0], Val loss: 0.038752343505620956\n",
      "Epoch[0], Batch[3713], Train loss: 0.04021837189793587\n",
      "Epoch[0], Val loss: 0.0386633463203907\n",
      "Epoch[0], Batch[3714], Train loss: 0.03943195939064026\n",
      "Epoch[0], Val loss: 0.03852803260087967\n",
      "Epoch[0], Batch[3715], Train loss: 0.037263695150613785\n",
      "Epoch[0], Val loss: 0.03436903655529022\n",
      "Epoch[0], Batch[3716], Train loss: 0.03935674950480461\n",
      "Epoch[0], Val loss: 0.038222428411245346\n",
      "Epoch[0], Batch[3717], Train loss: 0.03609481081366539\n",
      "Epoch[0], Val loss: 0.03558799996972084\n",
      "Epoch[0], Batch[3718], Train loss: 0.03937353938817978\n",
      "Epoch[0], Val loss: 0.039189938455820084\n",
      "Epoch[0], Batch[3719], Train loss: 0.0379650704562664\n",
      "Epoch[0], Val loss: 0.03533194214105606\n",
      "Epoch[0], Batch[3720], Train loss: 0.04024551436305046\n",
      "Epoch[0], Val loss: 0.03937680646777153\n",
      "Epoch[0], Batch[3721], Train loss: 0.037981707602739334\n",
      "Epoch[0], Val loss: 0.038920871913433075\n",
      "Epoch[0], Batch[3722], Train loss: 0.03859565779566765\n",
      "Epoch[0], Val loss: 0.03528127819299698\n",
      "Epoch[0], Batch[3723], Train loss: 0.037367936223745346\n",
      "Epoch[0], Val loss: 0.03695318475365639\n",
      "Epoch[0], Batch[3724], Train loss: 0.037606947124004364\n",
      "Epoch[0], Val loss: 0.03775305673480034\n",
      "Epoch[0], Batch[3725], Train loss: 0.038469258695840836\n",
      "Epoch[0], Val loss: 0.03438328951597214\n",
      "Epoch[0], Batch[3726], Train loss: 0.04041508585214615\n",
      "Epoch[0], Val loss: 0.03707873448729515\n",
      "Epoch[0], Batch[3727], Train loss: 0.036639418452978134\n",
      "Epoch[0], Val loss: 0.03606666624546051\n",
      "Epoch[0], Batch[3728], Train loss: 0.038225408643484116\n",
      "Epoch[0], Val loss: 0.03887748345732689\n",
      "Epoch[0], Batch[3729], Train loss: 0.036913178861141205\n",
      "Epoch[0], Val loss: 0.034197062253952026\n",
      "Epoch[0], Batch[3730], Train loss: 0.039744868874549866\n",
      "Epoch[0], Val loss: 0.038143694400787354\n",
      "Epoch[0], Batch[3731], Train loss: 0.043966058641672134\n",
      "Epoch[0], Val loss: 0.035786524415016174\n",
      "Epoch[0], Batch[3732], Train loss: 0.03864136338233948\n",
      "Epoch[0], Val loss: 0.03769831731915474\n",
      "Epoch[0], Batch[3733], Train loss: 0.03662915527820587\n",
      "Epoch[0], Val loss: 0.037524256855249405\n",
      "Epoch[0], Batch[3734], Train loss: 0.03860282525420189\n",
      "Epoch[0], Val loss: 0.03795453533530235\n",
      "Epoch[0], Batch[3735], Train loss: 0.03682834282517433\n",
      "Epoch[0], Val loss: 0.03667571768164635\n",
      "Epoch[0], Batch[3736], Train loss: 0.03953534737229347\n",
      "Epoch[0], Val loss: 0.035388581454753876\n",
      "Epoch[0], Batch[3737], Train loss: 0.03894110023975372\n",
      "Epoch[0], Val loss: 0.03641984984278679\n",
      "Epoch[0], Batch[3738], Train loss: 0.03953190892934799\n",
      "Epoch[0], Val loss: 0.037576138973236084\n",
      "Epoch[0], Batch[3739], Train loss: 0.0384044349193573\n",
      "Epoch[0], Val loss: 0.03719237074255943\n",
      "Epoch[0], Batch[3740], Train loss: 0.0392419770359993\n",
      "Epoch[0], Val loss: 0.03819381073117256\n",
      "Epoch[0], Batch[3741], Train loss: 0.038030803203582764\n",
      "Epoch[0], Val loss: 0.035879455506801605\n",
      "Epoch[0], Batch[3742], Train loss: 0.03850904479622841\n",
      "Epoch[0], Val loss: 0.037687722593545914\n",
      "Epoch[0], Batch[3743], Train loss: 0.03703876957297325\n",
      "Epoch[0], Val loss: 0.0385994054377079\n",
      "Epoch[0], Batch[3744], Train loss: 0.03774324059486389\n",
      "Epoch[0], Val loss: 0.03879503160715103\n",
      "Epoch[0], Batch[3745], Train loss: 0.039038486778736115\n",
      "Epoch[0], Val loss: 0.03749430924654007\n",
      "Epoch[0], Batch[3746], Train loss: 0.03579200431704521\n",
      "Epoch[0], Val loss: 0.041344452649354935\n",
      "Epoch[0], Batch[3747], Train loss: 0.03856324777007103\n",
      "Epoch[0], Val loss: 0.037553682923316956\n",
      "Epoch[0], Batch[3748], Train loss: 0.04008195921778679\n",
      "Epoch[0], Val loss: 0.035962849855422974\n",
      "Epoch[0], Batch[3749], Train loss: 0.040061093866825104\n",
      "Epoch[0], Val loss: 0.03709614649415016\n",
      "Epoch[0], Batch[3750], Train loss: 0.03704440966248512\n",
      "Epoch[0], Val loss: 0.036064036190509796\n",
      "Epoch[0], Batch[3751], Train loss: 0.039905861020088196\n",
      "Epoch[0], Val loss: 0.03702542930841446\n",
      "Epoch[0], Batch[3752], Train loss: 0.03630939871072769\n",
      "Epoch[0], Val loss: 0.03813346102833748\n",
      "Epoch[0], Batch[3753], Train loss: 0.040795717388391495\n",
      "Epoch[0], Val loss: 0.0384710468351841\n",
      "Epoch[0], Batch[3754], Train loss: 0.03974222019314766\n",
      "Epoch[0], Val loss: 0.03954325243830681\n",
      "Epoch[0], Batch[3755], Train loss: 0.03613699972629547\n",
      "Epoch[0], Val loss: 0.0353059284389019\n",
      "Epoch[0], Batch[3756], Train loss: 0.036091577261686325\n",
      "Epoch[0], Val loss: 0.037491049617528915\n",
      "Epoch[0], Batch[3757], Train loss: 0.038573507219552994\n",
      "Epoch[0], Val loss: 0.03659241273999214\n",
      "Epoch[0], Batch[3758], Train loss: 0.03665073961019516\n",
      "Epoch[0], Val loss: 0.03942031413316727\n",
      "Epoch[0], Batch[3759], Train loss: 0.037322863936424255\n",
      "Epoch[0], Val loss: 0.0375995934009552\n",
      "Epoch[0], Batch[3760], Train loss: 0.038637157529592514\n",
      "Epoch[0], Val loss: 0.03784545138478279\n",
      "Epoch[0], Batch[3761], Train loss: 0.0413215197622776\n",
      "Epoch[0], Val loss: 0.037455253303050995\n",
      "Epoch[0], Batch[3762], Train loss: 0.03817898780107498\n",
      "Epoch[0], Val loss: 0.04057817906141281\n",
      "Epoch[0], Batch[3763], Train loss: 0.04141426086425781\n",
      "Epoch[0], Val loss: 0.03897655010223389\n",
      "Epoch[0], Batch[3764], Train loss: 0.03892040252685547\n",
      "Epoch[0], Val loss: 0.03653177246451378\n",
      "Epoch[0], Batch[3765], Train loss: 0.03594142943620682\n",
      "Epoch[0], Val loss: 0.0377424955368042\n",
      "Epoch[0], Batch[3766], Train loss: 0.039042625576257706\n",
      "Epoch[0], Val loss: 0.03592289611697197\n",
      "Epoch[0], Batch[3767], Train loss: 0.039996568113565445\n",
      "Epoch[0], Val loss: 0.037324730306863785\n",
      "Epoch[0], Batch[3768], Train loss: 0.036785051226615906\n",
      "Epoch[0], Val loss: 0.0388229601085186\n",
      "Epoch[0], Batch[3769], Train loss: 0.039446402341127396\n",
      "Epoch[0], Val loss: 0.03831427916884422\n",
      "Epoch[0], Batch[3770], Train loss: 0.040838491171598434\n",
      "Epoch[0], Val loss: 0.037272993475198746\n",
      "Epoch[0], Batch[3771], Train loss: 0.03872370347380638\n",
      "Epoch[0], Val loss: 0.035467810928821564\n",
      "Epoch[0], Batch[3772], Train loss: 0.039046403020620346\n",
      "Epoch[0], Val loss: 0.03617051988840103\n",
      "Epoch[0], Batch[3773], Train loss: 0.037826307117938995\n",
      "Epoch[0], Val loss: 0.03582089766860008\n",
      "Epoch[0], Batch[3774], Train loss: 0.04084746912121773\n",
      "Epoch[0], Val loss: 0.03636462241411209\n",
      "Epoch[0], Batch[3775], Train loss: 0.03910895809531212\n",
      "Epoch[0], Val loss: 0.03557462617754936\n",
      "Epoch[0], Batch[3776], Train loss: 0.03834444284439087\n",
      "Epoch[0], Val loss: 0.039199348539114\n",
      "Epoch[0], Batch[3777], Train loss: 0.03856596350669861\n",
      "Epoch[0], Val loss: 0.03769088536500931\n",
      "Epoch[0], Batch[3778], Train loss: 0.03905963897705078\n",
      "Epoch[0], Val loss: 0.03700277581810951\n",
      "Epoch[0], Batch[3779], Train loss: 0.037367384880781174\n",
      "Epoch[0], Val loss: 0.037330567836761475\n",
      "Epoch[0], Batch[3780], Train loss: 0.03592615947127342\n",
      "Epoch[0], Val loss: 0.036635614931583405\n",
      "Epoch[0], Batch[3781], Train loss: 0.036154855042696\n",
      "Epoch[0], Val loss: 0.03542860969901085\n",
      "Epoch[0], Batch[3782], Train loss: 0.03783401846885681\n",
      "Epoch[0], Val loss: 0.035279981791973114\n",
      "Epoch[0], Batch[3783], Train loss: 0.03819706290960312\n",
      "Epoch[0], Val loss: 0.034496109932661057\n",
      "Epoch[0], Batch[3784], Train loss: 0.0393068790435791\n",
      "Epoch[0], Val loss: 0.0363844595849514\n",
      "Epoch[0], Batch[3785], Train loss: 0.03535490110516548\n",
      "Epoch[0], Val loss: 0.036461859941482544\n",
      "Epoch[0], Batch[3786], Train loss: 0.039743777364492416\n",
      "Epoch[0], Val loss: 0.033350132405757904\n",
      "Epoch[0], Batch[3787], Train loss: 0.03601899743080139\n",
      "Epoch[0], Val loss: 0.035854458808898926\n",
      "Epoch[0], Batch[3788], Train loss: 0.0389045812189579\n",
      "Epoch[0], Val loss: 0.035744909197092056\n",
      "Epoch[0], Batch[3789], Train loss: 0.03903203085064888\n",
      "Epoch[0], Val loss: 0.0375887006521225\n",
      "Epoch[0], Batch[3790], Train loss: 0.03663979470729828\n",
      "Epoch[0], Val loss: 0.03748251870274544\n",
      "Epoch[0], Batch[3791], Train loss: 0.0382997952401638\n",
      "Epoch[0], Val loss: 0.03972891718149185\n",
      "Epoch[0], Batch[3792], Train loss: 0.036724913865327835\n",
      "Epoch[0], Val loss: 0.0356813445687294\n",
      "Epoch[0], Batch[3793], Train loss: 0.03418738394975662\n",
      "Epoch[0], Val loss: 0.03702743723988533\n",
      "Epoch[0], Batch[3794], Train loss: 0.03752119466662407\n",
      "Epoch[0], Val loss: 0.03616872802376747\n",
      "Epoch[0], Batch[3795], Train loss: 0.03834487870335579\n",
      "Epoch[0], Val loss: 0.04065961390733719\n",
      "Epoch[0], Batch[3796], Train loss: 0.03716477379202843\n",
      "Epoch[0], Val loss: 0.03834434598684311\n",
      "Epoch[0], Batch[3797], Train loss: 0.036250319331884384\n",
      "Epoch[0], Val loss: 0.0382051058113575\n",
      "Epoch[0], Batch[3798], Train loss: 0.03871932625770569\n",
      "Epoch[0], Val loss: 0.0374983586370945\n",
      "Epoch[0], Batch[3799], Train loss: 0.036843571811914444\n",
      "Epoch[0], Val loss: 0.03564883768558502\n",
      "Epoch[0], Batch[3800], Train loss: 0.03966571018099785\n",
      "Epoch[0], Val loss: 0.039319828152656555\n",
      "Epoch[0], Batch[3801], Train loss: 0.04060729965567589\n",
      "Epoch[0], Val loss: 0.03344430401921272\n",
      "Epoch[0], Batch[3802], Train loss: 0.037861667573451996\n",
      "Epoch[0], Val loss: 0.03898393362760544\n",
      "Epoch[0], Batch[3803], Train loss: 0.03994738310575485\n",
      "Epoch[0], Val loss: 0.036196302622556686\n",
      "Epoch[0], Batch[3804], Train loss: 0.03820851817727089\n",
      "Epoch[0], Val loss: 0.03653216361999512\n",
      "Epoch[0], Batch[3805], Train loss: 0.040963198989629745\n",
      "Epoch[0], Val loss: 0.03806053474545479\n",
      "Epoch[0], Batch[3806], Train loss: 0.03694845736026764\n",
      "Epoch[0], Val loss: 0.033785074949264526\n",
      "Epoch[0], Batch[3807], Train loss: 0.03691300377249718\n",
      "Epoch[0], Val loss: 0.03837459906935692\n",
      "Epoch[0], Batch[3808], Train loss: 0.03673181310296059\n",
      "Epoch[0], Val loss: 0.03644503280520439\n",
      "Epoch[0], Batch[3809], Train loss: 0.0376279391348362\n",
      "Epoch[0], Val loss: 0.03585537523031235\n",
      "Epoch[0], Batch[3810], Train loss: 0.0374741367995739\n",
      "Epoch[0], Val loss: 0.03729582577943802\n",
      "Epoch[0], Batch[3811], Train loss: 0.03843716159462929\n",
      "Epoch[0], Val loss: 0.03558776527643204\n",
      "Epoch[0], Batch[3812], Train loss: 0.03956964239478111\n",
      "Epoch[0], Val loss: 0.03765103593468666\n",
      "Epoch[0], Batch[3813], Train loss: 0.037454839795827866\n",
      "Epoch[0], Val loss: 0.036051731556653976\n",
      "Epoch[0], Batch[3814], Train loss: 0.03931107744574547\n",
      "Epoch[0], Val loss: 0.03824001923203468\n",
      "Epoch[0], Batch[3815], Train loss: 0.04198504611849785\n",
      "Epoch[0], Val loss: 0.03507621958851814\n",
      "Epoch[0], Batch[3816], Train loss: 0.03713580593466759\n",
      "Epoch[0], Val loss: 0.03706743195652962\n",
      "Epoch[0], Batch[3817], Train loss: 0.03673083707690239\n",
      "Epoch[0], Val loss: 0.039618171751499176\n",
      "Epoch[0], Batch[3818], Train loss: 0.0388965979218483\n",
      "Epoch[0], Val loss: 0.03718368336558342\n",
      "Epoch[0], Batch[3819], Train loss: 0.03929884359240532\n",
      "Epoch[0], Val loss: 0.038440246134996414\n",
      "Epoch[0], Batch[3820], Train loss: 0.039129145443439484\n",
      "Epoch[0], Val loss: 0.03808984160423279\n",
      "Epoch[0], Batch[3821], Train loss: 0.03608468174934387\n",
      "Epoch[0], Val loss: 0.03537939488887787\n",
      "Epoch[0], Batch[3822], Train loss: 0.038038335740566254\n",
      "Epoch[0], Val loss: 0.0352497361600399\n",
      "Epoch[0], Batch[3823], Train loss: 0.037666205316782\n",
      "Epoch[0], Val loss: 0.03645015507936478\n",
      "Epoch[0], Batch[3824], Train loss: 0.03765321150422096\n",
      "Epoch[0], Val loss: 0.034997813403606415\n",
      "Epoch[0], Batch[3825], Train loss: 0.03627574443817139\n",
      "Epoch[0], Val loss: 0.03489472344517708\n",
      "Epoch[0], Batch[3826], Train loss: 0.03756977617740631\n",
      "Epoch[0], Val loss: 0.039400577545166016\n",
      "Epoch[0], Batch[3827], Train loss: 0.041006192564964294\n",
      "Epoch[0], Val loss: 0.03429009020328522\n",
      "Epoch[0], Batch[3828], Train loss: 0.03885291516780853\n",
      "Epoch[0], Val loss: 0.03748956322669983\n",
      "Epoch[0], Batch[3829], Train loss: 0.03657836094498634\n",
      "Epoch[0], Val loss: 0.039227887988090515\n",
      "Epoch[0], Batch[3830], Train loss: 0.04048481583595276\n",
      "Epoch[0], Val loss: 0.03858498856425285\n",
      "Epoch[0], Batch[3831], Train loss: 0.036674097180366516\n",
      "Epoch[0], Val loss: 0.037469830363988876\n",
      "Epoch[0], Batch[3832], Train loss: 0.036814723163843155\n",
      "Epoch[0], Val loss: 0.036520346999168396\n",
      "Epoch[0], Batch[3833], Train loss: 0.03706211596727371\n",
      "Epoch[0], Val loss: 0.03806779161095619\n",
      "Epoch[0], Batch[3834], Train loss: 0.03678452596068382\n",
      "Epoch[0], Val loss: 0.04036534205079079\n",
      "Epoch[0], Batch[3835], Train loss: 0.04212610051035881\n",
      "Epoch[0], Val loss: 0.03743475675582886\n",
      "Epoch[0], Batch[3836], Train loss: 0.037973713129758835\n",
      "Epoch[0], Val loss: 0.03642917424440384\n",
      "Epoch[0], Batch[3837], Train loss: 0.0395420640707016\n",
      "Epoch[0], Val loss: 0.03558134287595749\n",
      "Epoch[0], Batch[3838], Train loss: 0.03954493626952171\n",
      "Epoch[0], Val loss: 0.037623077630996704\n",
      "Epoch[0], Batch[3839], Train loss: 0.03711214289069176\n",
      "Epoch[0], Val loss: 0.036469966173172\n",
      "Epoch[0], Batch[3840], Train loss: 0.036430492997169495\n",
      "Epoch[0], Val loss: 0.03726658597588539\n",
      "Epoch[0], Batch[3841], Train loss: 0.038598328828811646\n",
      "Epoch[0], Val loss: 0.035651225596666336\n",
      "Epoch[0], Batch[3842], Train loss: 0.037497378885746\n",
      "Epoch[0], Val loss: 0.03974340111017227\n",
      "Epoch[0], Batch[3843], Train loss: 0.0410383976995945\n",
      "Epoch[0], Val loss: 0.036705754697322845\n",
      "Epoch[0], Batch[3844], Train loss: 0.03907445818185806\n",
      "Epoch[0], Val loss: 0.03680587559938431\n",
      "Epoch[0], Batch[3845], Train loss: 0.04018179699778557\n",
      "Epoch[0], Val loss: 0.0369013249874115\n",
      "Epoch[0], Batch[3846], Train loss: 0.036802951246500015\n",
      "Epoch[0], Val loss: 0.03995507210493088\n",
      "Epoch[0], Batch[3847], Train loss: 0.03981959447264671\n",
      "Epoch[0], Val loss: 0.040908265858888626\n",
      "Epoch[0], Batch[3848], Train loss: 0.0419306755065918\n",
      "Epoch[0], Val loss: 0.03702252730727196\n",
      "Epoch[0], Batch[3849], Train loss: 0.036832407116889954\n",
      "Epoch[0], Val loss: 0.03727366030216217\n",
      "Epoch[0], Batch[3850], Train loss: 0.038647957146167755\n",
      "Epoch[0], Val loss: 0.03638001158833504\n",
      "Epoch[0], Batch[3851], Train loss: 0.03951483592391014\n",
      "Epoch[0], Val loss: 0.03680279105901718\n",
      "Epoch[0], Batch[3852], Train loss: 0.03859888017177582\n",
      "Epoch[0], Val loss: 0.0348266065120697\n",
      "Epoch[0], Batch[3853], Train loss: 0.038450539112091064\n",
      "Epoch[0], Val loss: 0.03886514902114868\n",
      "Epoch[0], Batch[3854], Train loss: 0.036118924617767334\n",
      "Epoch[0], Val loss: 0.04006040096282959\n",
      "Epoch[0], Batch[3855], Train loss: 0.03868885710835457\n",
      "Epoch[0], Val loss: 0.03774179890751839\n",
      "Epoch[0], Batch[3856], Train loss: 0.04039532691240311\n",
      "Epoch[0], Val loss: 0.03532937541604042\n",
      "Epoch[0], Batch[3857], Train loss: 0.04094566032290459\n",
      "Epoch[0], Val loss: 0.03780120238661766\n",
      "Epoch[0], Batch[3858], Train loss: 0.03972950950264931\n",
      "Epoch[0], Val loss: 0.03633924946188927\n",
      "Epoch[0], Batch[3859], Train loss: 0.036158546805381775\n",
      "Epoch[0], Val loss: 0.03935648128390312\n",
      "Epoch[0], Batch[3860], Train loss: 0.03855722025036812\n",
      "Epoch[0], Val loss: 0.03397658094763756\n",
      "Epoch[0], Batch[3861], Train loss: 0.03687469661235809\n",
      "Epoch[0], Val loss: 0.036987561732530594\n",
      "Epoch[0], Batch[3862], Train loss: 0.03915955498814583\n",
      "Epoch[0], Val loss: 0.035417065024375916\n",
      "Epoch[0], Batch[3863], Train loss: 0.03854421526193619\n",
      "Epoch[0], Val loss: 0.03610982000827789\n",
      "Epoch[0], Batch[3864], Train loss: 0.03782413899898529\n",
      "Epoch[0], Val loss: 0.03780660778284073\n",
      "Epoch[0], Batch[3865], Train loss: 0.03790590539574623\n",
      "Epoch[0], Val loss: 0.035465218126773834\n",
      "Epoch[0], Batch[3866], Train loss: 0.03867745399475098\n",
      "Epoch[0], Val loss: 0.03575471416115761\n",
      "Epoch[0], Batch[3867], Train loss: 0.039461586624383926\n",
      "Epoch[0], Val loss: 0.03546722233295441\n",
      "Epoch[0], Batch[3868], Train loss: 0.039615076035261154\n",
      "Epoch[0], Val loss: 0.035651206970214844\n",
      "Epoch[0], Batch[3869], Train loss: 0.03842078521847725\n",
      "Epoch[0], Val loss: 0.03912000358104706\n",
      "Epoch[0], Batch[3870], Train loss: 0.03841644152998924\n",
      "Epoch[0], Val loss: 0.035731419920921326\n",
      "Epoch[0], Batch[3871], Train loss: 0.04084860906004906\n",
      "Epoch[0], Val loss: 0.03660614788532257\n",
      "Epoch[0], Batch[3872], Train loss: 0.03969999775290489\n",
      "Epoch[0], Val loss: 0.03843098133802414\n",
      "Epoch[0], Batch[3873], Train loss: 0.03725973144173622\n",
      "Epoch[0], Val loss: 0.03812828287482262\n",
      "Epoch[0], Batch[3874], Train loss: 0.03744178265333176\n",
      "Epoch[0], Val loss: 0.037663452327251434\n",
      "Epoch[0], Batch[3875], Train loss: 0.03802337124943733\n",
      "Epoch[0], Val loss: 0.03820544108748436\n",
      "Epoch[0], Batch[3876], Train loss: 0.03811142221093178\n",
      "Epoch[0], Val loss: 0.03772205114364624\n",
      "Epoch[0], Batch[3877], Train loss: 0.038290344178676605\n",
      "Epoch[0], Val loss: 0.034458983689546585\n",
      "Epoch[0], Batch[3878], Train loss: 0.03782397508621216\n",
      "Epoch[0], Val loss: 0.03829844668507576\n",
      "Epoch[0], Batch[3879], Train loss: 0.03740231692790985\n",
      "Epoch[0], Val loss: 0.03793761506676674\n",
      "Epoch[0], Batch[3880], Train loss: 0.03887307643890381\n",
      "Epoch[0], Val loss: 0.0354757085442543\n",
      "Epoch[0], Batch[3881], Train loss: 0.04042217880487442\n",
      "Epoch[0], Val loss: 0.037071678787469864\n",
      "Epoch[0], Batch[3882], Train loss: 0.037308238446712494\n",
      "Epoch[0], Val loss: 0.03599837049841881\n",
      "Epoch[0], Batch[3883], Train loss: 0.04107961058616638\n",
      "Epoch[0], Val loss: 0.03675239533185959\n",
      "Epoch[0], Batch[3884], Train loss: 0.03867528215050697\n",
      "Epoch[0], Val loss: 0.04012942314147949\n",
      "Epoch[0], Batch[3885], Train loss: 0.03906326740980148\n",
      "Epoch[0], Val loss: 0.037646472454071045\n",
      "Epoch[0], Batch[3886], Train loss: 0.03756416216492653\n",
      "Epoch[0], Val loss: 0.03454407677054405\n",
      "Epoch[0], Batch[3887], Train loss: 0.040730662643909454\n",
      "Epoch[0], Val loss: 0.03743135556578636\n",
      "Epoch[0], Batch[3888], Train loss: 0.03932254761457443\n",
      "Epoch[0], Val loss: 0.03523673862218857\n",
      "Epoch[0], Batch[3889], Train loss: 0.04037868231534958\n",
      "Epoch[0], Val loss: 0.03808990493416786\n",
      "Epoch[0], Batch[3890], Train loss: 0.03776882216334343\n",
      "Epoch[0], Val loss: 0.03643965721130371\n",
      "Epoch[0], Batch[3891], Train loss: 0.03999090939760208\n",
      "Epoch[0], Val loss: 0.03688826411962509\n",
      "Epoch[0], Batch[3892], Train loss: 0.03845968097448349\n",
      "Epoch[0], Val loss: 0.037151824682950974\n",
      "Epoch[0], Batch[3893], Train loss: 0.03681616112589836\n",
      "Epoch[0], Val loss: 0.03627147898077965\n",
      "Epoch[0], Batch[3894], Train loss: 0.038263339549303055\n",
      "Epoch[0], Val loss: 0.03734824061393738\n",
      "Epoch[0], Batch[3895], Train loss: 0.03738531470298767\n",
      "Epoch[0], Val loss: 0.037435732781887054\n",
      "Epoch[0], Batch[3896], Train loss: 0.03736606612801552\n",
      "Epoch[0], Val loss: 0.0369960255920887\n",
      "Epoch[0], Batch[3897], Train loss: 0.036801911890506744\n",
      "Epoch[0], Val loss: 0.03505583107471466\n",
      "Epoch[0], Batch[3898], Train loss: 0.03623279929161072\n",
      "Epoch[0], Val loss: 0.037429649382829666\n",
      "Epoch[0], Batch[3899], Train loss: 0.03855779021978378\n",
      "Epoch[0], Val loss: 0.036793164908885956\n",
      "Epoch[0], Batch[3900], Train loss: 0.03745444491505623\n",
      "Epoch[0], Val loss: 0.04009482264518738\n",
      "Epoch[0], Batch[3901], Train loss: 0.04115763306617737\n",
      "Epoch[0], Val loss: 0.03507525473833084\n",
      "Epoch[0], Batch[3902], Train loss: 0.041318245232105255\n",
      "Epoch[0], Val loss: 0.03617632016539574\n",
      "Epoch[0], Batch[3903], Train loss: 0.038075655698776245\n",
      "Epoch[0], Val loss: 0.03614301607012749\n",
      "Epoch[0], Batch[3904], Train loss: 0.038050804287195206\n",
      "Epoch[0], Val loss: 0.03645225241780281\n",
      "Epoch[0], Batch[3905], Train loss: 0.04029778763651848\n",
      "Epoch[0], Val loss: 0.03732854500412941\n",
      "Epoch[0], Batch[3906], Train loss: 0.0376850962638855\n",
      "Epoch[0], Val loss: 0.036573465913534164\n",
      "Epoch[0], Batch[3907], Train loss: 0.03899608179926872\n",
      "Epoch[0], Val loss: 0.03489360213279724\n",
      "Epoch[0], Batch[3908], Train loss: 0.038848645985126495\n",
      "Epoch[0], Val loss: 0.037465259432792664\n",
      "Epoch[0], Batch[3909], Train loss: 0.03777068853378296\n",
      "Epoch[0], Val loss: 0.03796752914786339\n",
      "Epoch[0], Batch[3910], Train loss: 0.03807062655687332\n",
      "Epoch[0], Val loss: 0.03664059191942215\n",
      "Epoch[0], Batch[3911], Train loss: 0.037621304392814636\n",
      "Epoch[0], Val loss: 0.036016784608364105\n",
      "Epoch[0], Batch[3912], Train loss: 0.03722275048494339\n",
      "Epoch[0], Val loss: 0.03738532215356827\n",
      "Epoch[0], Batch[3913], Train loss: 0.03810134902596474\n",
      "Epoch[0], Val loss: 0.03768859803676605\n",
      "Epoch[0], Batch[3914], Train loss: 0.03723503276705742\n",
      "Epoch[0], Val loss: 0.03734235838055611\n",
      "Epoch[0], Batch[3915], Train loss: 0.03803350776433945\n",
      "Epoch[0], Val loss: 0.03519604727625847\n",
      "Epoch[0], Batch[3916], Train loss: 0.037724483758211136\n",
      "Epoch[0], Val loss: 0.035781823098659515\n",
      "Epoch[0], Batch[3917], Train loss: 0.03840784728527069\n",
      "Epoch[0], Val loss: 0.0381176732480526\n",
      "Epoch[0], Batch[3918], Train loss: 0.040352653712034225\n",
      "Epoch[0], Val loss: 0.03828456997871399\n",
      "Epoch[0], Batch[3919], Train loss: 0.03444975987076759\n",
      "Epoch[0], Val loss: 0.0356580875813961\n",
      "Epoch[0], Batch[3920], Train loss: 0.03824186325073242\n",
      "Epoch[0], Val loss: 0.036539364606142044\n",
      "Epoch[0], Batch[3921], Train loss: 0.036785103380680084\n",
      "Epoch[0], Val loss: 0.03649916127324104\n",
      "Epoch[0], Batch[3922], Train loss: 0.03953259438276291\n",
      "Epoch[0], Val loss: 0.03590858355164528\n",
      "Epoch[0], Batch[3923], Train loss: 0.039260417222976685\n",
      "Epoch[0], Val loss: 0.03824041038751602\n",
      "Epoch[0], Batch[3924], Train loss: 0.03552648425102234\n",
      "Epoch[0], Val loss: 0.03786526247859001\n",
      "Epoch[0], Batch[3925], Train loss: 0.037475936114788055\n",
      "Epoch[0], Val loss: 0.03625038266181946\n",
      "Epoch[0], Batch[3926], Train loss: 0.03659825772047043\n",
      "Epoch[0], Val loss: 0.034909069538116455\n",
      "Epoch[0], Batch[3927], Train loss: 0.040008191019296646\n",
      "Epoch[0], Val loss: 0.03691946342587471\n",
      "Epoch[0], Batch[3928], Train loss: 0.037375301122665405\n",
      "Epoch[0], Val loss: 0.034936174750328064\n",
      "Epoch[0], Batch[3929], Train loss: 0.03767723590135574\n",
      "Epoch[0], Val loss: 0.03726671636104584\n",
      "Epoch[0], Batch[3930], Train loss: 0.03590947389602661\n",
      "Epoch[0], Val loss: 0.0366506464779377\n",
      "Epoch[0], Batch[3931], Train loss: 0.035635072737932205\n",
      "Epoch[0], Val loss: 0.03541746363043785\n",
      "Epoch[0], Batch[3932], Train loss: 0.03778466954827309\n",
      "Epoch[0], Val loss: 0.035976771265268326\n",
      "Epoch[0], Batch[3933], Train loss: 0.03878018260002136\n",
      "Epoch[0], Val loss: 0.03786567971110344\n",
      "Epoch[0], Batch[3934], Train loss: 0.03902161493897438\n",
      "Epoch[0], Val loss: 0.03612648695707321\n",
      "Epoch[0], Batch[3935], Train loss: 0.037413619458675385\n",
      "Epoch[0], Val loss: 0.03533179685473442\n",
      "Epoch[0], Batch[3936], Train loss: 0.03878720849752426\n",
      "Epoch[0], Val loss: 0.035659197717905045\n",
      "Epoch[0], Batch[3937], Train loss: 0.03650644049048424\n",
      "Epoch[0], Val loss: 0.03540416434407234\n",
      "Epoch[0], Batch[3938], Train loss: 0.037664562463760376\n",
      "Epoch[0], Val loss: 0.034845687448978424\n",
      "Epoch[0], Batch[3939], Train loss: 0.03542044013738632\n",
      "Epoch[0], Val loss: 0.03793206810951233\n",
      "Epoch[0], Batch[3940], Train loss: 0.0385468453168869\n",
      "Epoch[0], Val loss: 0.0352645181119442\n",
      "Epoch[0], Batch[3941], Train loss: 0.03706905618309975\n",
      "Epoch[0], Val loss: 0.03689444437623024\n",
      "Epoch[0], Batch[3942], Train loss: 0.037684567272663116\n",
      "Epoch[0], Val loss: 0.03609756380319595\n",
      "Epoch[0], Batch[3943], Train loss: 0.040085431188344955\n",
      "Epoch[0], Val loss: 0.03525447100400925\n",
      "Epoch[0], Batch[3944], Train loss: 0.03797835856676102\n",
      "Epoch[0], Val loss: 0.036151740700006485\n",
      "Epoch[0], Batch[3945], Train loss: 0.03726201131939888\n",
      "Epoch[0], Val loss: 0.03826192393898964\n",
      "Epoch[0], Batch[3946], Train loss: 0.03987257555127144\n",
      "Epoch[0], Val loss: 0.03740789741277695\n",
      "Epoch[0], Batch[3947], Train loss: 0.03980952873826027\n",
      "Epoch[0], Val loss: 0.03543442487716675\n",
      "Epoch[0], Batch[3948], Train loss: 0.038636237382888794\n",
      "Epoch[0], Val loss: 0.03801146522164345\n",
      "Epoch[0], Batch[3949], Train loss: 0.03880150243639946\n",
      "Epoch[0], Val loss: 0.037266939878463745\n",
      "Epoch[0], Batch[3950], Train loss: 0.03785998374223709\n",
      "Epoch[0], Val loss: 0.03615802526473999\n",
      "Epoch[0], Batch[3951], Train loss: 0.038881126791238785\n",
      "Epoch[0], Val loss: 0.03708737716078758\n",
      "Epoch[0], Batch[3952], Train loss: 0.03942687809467316\n",
      "Epoch[0], Val loss: 0.038851749151945114\n",
      "Epoch[0], Batch[3953], Train loss: 0.038320392370224\n",
      "Epoch[0], Val loss: 0.03785786032676697\n",
      "Epoch[0], Batch[3954], Train loss: 0.037575941532850266\n",
      "Epoch[0], Val loss: 0.03626979514956474\n",
      "Epoch[0], Batch[3955], Train loss: 0.03809046372771263\n",
      "Epoch[0], Val loss: 0.03694387897849083\n",
      "Epoch[0], Batch[3956], Train loss: 0.041207946836948395\n",
      "Epoch[0], Val loss: 0.03978646546602249\n",
      "Epoch[0], Batch[3957], Train loss: 0.038661036640405655\n",
      "Epoch[0], Val loss: 0.03674573078751564\n",
      "Epoch[0], Batch[3958], Train loss: 0.03841501846909523\n",
      "Epoch[0], Val loss: 0.038220781832933426\n",
      "Epoch[0], Batch[3959], Train loss: 0.03625462204217911\n",
      "Epoch[0], Val loss: 0.03974246606230736\n",
      "Epoch[0], Batch[3960], Train loss: 0.038175877183675766\n",
      "Epoch[0], Val loss: 0.03874409571290016\n",
      "Epoch[0], Batch[3961], Train loss: 0.037424493581056595\n",
      "Epoch[0], Val loss: 0.036140039563179016\n",
      "Epoch[0], Batch[3962], Train loss: 0.03854602202773094\n",
      "Epoch[0], Val loss: 0.040549445897340775\n",
      "Epoch[0], Batch[3963], Train loss: 0.04104587435722351\n",
      "Epoch[0], Val loss: 0.03574475273489952\n",
      "Epoch[0], Batch[3964], Train loss: 0.037965018302202225\n",
      "Epoch[0], Val loss: 0.035771097987890244\n",
      "Epoch[0], Batch[3965], Train loss: 0.04083843529224396\n",
      "Epoch[0], Val loss: 0.038059547543525696\n",
      "Epoch[0], Batch[3966], Train loss: 0.0382593497633934\n",
      "Epoch[0], Val loss: 0.03439242020249367\n",
      "Epoch[0], Batch[3967], Train loss: 0.03612375259399414\n",
      "Epoch[0], Val loss: 0.03681591525673866\n",
      "Epoch[0], Batch[3968], Train loss: 0.03879667818546295\n",
      "Epoch[0], Val loss: 0.03787575289607048\n",
      "Epoch[0], Batch[3969], Train loss: 0.038800712674856186\n",
      "Epoch[0], Val loss: 0.0392034538090229\n",
      "Epoch[0], Batch[3970], Train loss: 0.04149250686168671\n",
      "Epoch[0], Val loss: 0.034975215792655945\n",
      "Epoch[0], Batch[3971], Train loss: 0.038278963416814804\n",
      "Epoch[0], Val loss: 0.037911415100097656\n",
      "Epoch[0], Batch[3972], Train loss: 0.037171926349401474\n",
      "Epoch[0], Val loss: 0.03577987104654312\n",
      "Epoch[0], Batch[3973], Train loss: 0.03598456457257271\n",
      "Epoch[0], Val loss: 0.03706531599164009\n",
      "Epoch[0], Batch[3974], Train loss: 0.03806842491030693\n",
      "Epoch[0], Val loss: 0.03720852732658386\n",
      "Epoch[0], Batch[3975], Train loss: 0.039039429277181625\n",
      "Epoch[0], Val loss: 0.04012025520205498\n",
      "Epoch[0], Batch[3976], Train loss: 0.038997504860162735\n",
      "Epoch[0], Val loss: 0.036523353308439255\n",
      "Epoch[0], Batch[3977], Train loss: 0.03567405790090561\n",
      "Epoch[0], Val loss: 0.03884969279170036\n",
      "Epoch[0], Batch[3978], Train loss: 0.039631277322769165\n",
      "Epoch[0], Val loss: 0.037080295383930206\n",
      "Epoch[0], Batch[3979], Train loss: 0.03838716074824333\n",
      "Epoch[0], Val loss: 0.034710049629211426\n",
      "Epoch[0], Batch[3980], Train loss: 0.0399838387966156\n",
      "Epoch[0], Val loss: 0.03758255019783974\n",
      "Epoch[0], Batch[3981], Train loss: 0.04061514511704445\n",
      "Epoch[0], Val loss: 0.035777922719717026\n",
      "Epoch[0], Batch[3982], Train loss: 0.037953849881887436\n",
      "Epoch[0], Val loss: 0.03755868598818779\n",
      "Epoch[0], Batch[3983], Train loss: 0.04077305644750595\n",
      "Epoch[0], Val loss: 0.03432954475283623\n",
      "Epoch[0], Batch[3984], Train loss: 0.03767624869942665\n",
      "Epoch[0], Val loss: 0.03709879145026207\n",
      "Epoch[0], Batch[3985], Train loss: 0.037289924919605255\n",
      "Epoch[0], Val loss: 0.03583398088812828\n",
      "Epoch[0], Batch[3986], Train loss: 0.035949867218732834\n",
      "Epoch[0], Val loss: 0.03814205899834633\n",
      "Epoch[0], Batch[3987], Train loss: 0.039227407425642014\n",
      "Epoch[0], Val loss: 0.034513697028160095\n",
      "Epoch[0], Batch[3988], Train loss: 0.035186007618904114\n",
      "Epoch[0], Val loss: 0.03558380529284477\n",
      "Epoch[0], Batch[3989], Train loss: 0.03814712166786194\n",
      "Epoch[0], Val loss: 0.034453537315130234\n",
      "Epoch[0], Batch[3990], Train loss: 0.03863130137324333\n",
      "Epoch[0], Val loss: 0.036072853952646255\n",
      "Epoch[0], Batch[3991], Train loss: 0.03804618865251541\n",
      "Epoch[0], Val loss: 0.040852952748537064\n",
      "Epoch[0], Batch[3992], Train loss: 0.03741636127233505\n",
      "Epoch[0], Val loss: 0.038551270961761475\n",
      "Epoch[0], Batch[3993], Train loss: 0.03936474397778511\n",
      "Epoch[0], Val loss: 0.036771561950445175\n",
      "Epoch[0], Batch[3994], Train loss: 0.040624070912599564\n",
      "Epoch[0], Val loss: 0.036883000284433365\n",
      "Epoch[0], Batch[3995], Train loss: 0.038989294320344925\n",
      "Epoch[0], Val loss: 0.03781959414482117\n",
      "Epoch[0], Batch[3996], Train loss: 0.037413790822029114\n",
      "Epoch[0], Val loss: 0.03665340691804886\n",
      "Epoch[0], Batch[3997], Train loss: 0.03740610182285309\n",
      "Epoch[0], Val loss: 0.0379188098013401\n",
      "Epoch[0], Batch[3998], Train loss: 0.039497096091508865\n",
      "Epoch[0], Val loss: 0.03539168834686279\n",
      "Epoch[0], Batch[3999], Train loss: 0.03870987892150879\n",
      "Epoch[0], Val loss: 0.035926733165979385\n",
      "Epoch[0], Batch[4000], Train loss: 0.03754527494311333\n",
      "Epoch[0], Val loss: 0.037350110709667206\n",
      "Epoch[0], Batch[4001], Train loss: 0.03902842476963997\n",
      "Epoch[0], Val loss: 0.03686922416090965\n",
      "Epoch[0], Batch[4002], Train loss: 0.03877821937203407\n",
      "Epoch[0], Val loss: 0.03777480497956276\n",
      "Epoch[0], Batch[4003], Train loss: 0.03458757698535919\n",
      "Epoch[0], Val loss: 0.03739668428897858\n",
      "Epoch[0], Batch[4004], Train loss: 0.037470798939466476\n",
      "Epoch[0], Val loss: 0.03856950253248215\n",
      "Epoch[0], Batch[4005], Train loss: 0.038753241300582886\n",
      "Epoch[0], Val loss: 0.03702337294816971\n",
      "Epoch[0], Batch[4006], Train loss: 0.039308056235313416\n",
      "Epoch[0], Val loss: 0.03589489310979843\n",
      "Epoch[0], Batch[4007], Train loss: 0.03803073614835739\n",
      "Epoch[0], Val loss: 0.03800557553768158\n",
      "Epoch[0], Batch[4008], Train loss: 0.03556360676884651\n",
      "Epoch[0], Val loss: 0.03631511703133583\n",
      "Epoch[0], Batch[4009], Train loss: 0.040973931550979614\n",
      "Epoch[0], Val loss: 0.03536786511540413\n",
      "Epoch[0], Batch[4010], Train loss: 0.037148673087358475\n",
      "Epoch[0], Val loss: 0.036950670182704926\n",
      "Epoch[0], Batch[4011], Train loss: 0.038863860070705414\n",
      "Epoch[0], Val loss: 0.037811484187841415\n",
      "Epoch[0], Batch[4012], Train loss: 0.037859924137592316\n",
      "Epoch[0], Val loss: 0.03604575991630554\n",
      "Epoch[0], Batch[4013], Train loss: 0.03708365932106972\n",
      "Epoch[0], Val loss: 0.03625181317329407\n",
      "Epoch[0], Batch[4014], Train loss: 0.0381423681974411\n",
      "Epoch[0], Val loss: 0.034779783338308334\n",
      "Epoch[0], Batch[4015], Train loss: 0.04019875451922417\n",
      "Epoch[0], Val loss: 0.03775053843855858\n",
      "Epoch[0], Batch[4016], Train loss: 0.03651599586009979\n",
      "Epoch[0], Val loss: 0.03609602898359299\n",
      "Epoch[0], Batch[4017], Train loss: 0.03668797016143799\n",
      "Epoch[0], Val loss: 0.03631070256233215\n",
      "Epoch[0], Batch[4018], Train loss: 0.03741126134991646\n",
      "Epoch[0], Val loss: 0.03802730515599251\n",
      "Epoch[0], Batch[4019], Train loss: 0.03545089066028595\n",
      "Epoch[0], Val loss: 0.0376867949962616\n",
      "Epoch[0], Batch[4020], Train loss: 0.03721782937645912\n",
      "Epoch[0], Val loss: 0.0369686521589756\n",
      "Epoch[0], Batch[4021], Train loss: 0.03795727714896202\n",
      "Epoch[0], Val loss: 0.0391814261674881\n",
      "Epoch[0], Batch[4022], Train loss: 0.037190284579992294\n",
      "Epoch[0], Val loss: 0.037228867411613464\n",
      "Epoch[0], Batch[4023], Train loss: 0.03653823584318161\n",
      "Epoch[0], Val loss: 0.03644448146224022\n",
      "Epoch[0], Batch[4024], Train loss: 0.03483562171459198\n",
      "Epoch[0], Val loss: 0.0346602126955986\n",
      "Epoch[0], Batch[4025], Train loss: 0.03817751258611679\n",
      "Epoch[0], Val loss: 0.03784051910042763\n",
      "Epoch[0], Batch[4026], Train loss: 0.03685170039534569\n",
      "Epoch[0], Val loss: 0.03404105082154274\n",
      "Epoch[0], Batch[4027], Train loss: 0.038544122129678726\n",
      "Epoch[0], Val loss: 0.03807465359568596\n",
      "Epoch[0], Batch[4028], Train loss: 0.03826858103275299\n",
      "Epoch[0], Val loss: 0.037616580724716187\n",
      "Epoch[0], Batch[4029], Train loss: 0.037247397005558014\n",
      "Epoch[0], Val loss: 0.036165766417980194\n",
      "Epoch[0], Batch[4030], Train loss: 0.03729608282446861\n",
      "Epoch[0], Val loss: 0.03965301811695099\n",
      "Epoch[0], Batch[4031], Train loss: 0.03795702010393143\n",
      "Epoch[0], Val loss: 0.03700609505176544\n",
      "Epoch[0], Batch[4032], Train loss: 0.03734176978468895\n",
      "Epoch[0], Val loss: 0.03707525134086609\n",
      "Epoch[0], Batch[4033], Train loss: 0.03839642181992531\n",
      "Epoch[0], Val loss: 0.037013135850429535\n",
      "Epoch[0], Batch[4034], Train loss: 0.03876731917262077\n",
      "Epoch[0], Val loss: 0.03558067977428436\n",
      "Epoch[0], Batch[4035], Train loss: 0.03714529424905777\n",
      "Epoch[0], Val loss: 0.03559556230902672\n",
      "Epoch[0], Batch[4036], Train loss: 0.03590869903564453\n",
      "Epoch[0], Val loss: 0.035449497401714325\n",
      "Epoch[0], Batch[4037], Train loss: 0.03584086522459984\n",
      "Epoch[0], Val loss: 0.03674439340829849\n",
      "Epoch[0], Batch[4038], Train loss: 0.03572633117437363\n",
      "Epoch[0], Val loss: 0.03640429303050041\n",
      "Epoch[0], Batch[4039], Train loss: 0.037438876926898956\n",
      "Epoch[0], Val loss: 0.03611919656395912\n",
      "Epoch[0], Batch[4040], Train loss: 0.037716224789619446\n",
      "Epoch[0], Val loss: 0.03793980926275253\n",
      "Epoch[0], Batch[4041], Train loss: 0.03955701366066933\n",
      "Epoch[0], Val loss: 0.03690456971526146\n",
      "Epoch[0], Batch[4042], Train loss: 0.03540829196572304\n",
      "Epoch[0], Val loss: 0.03678436204791069\n",
      "Epoch[0], Batch[4043], Train loss: 0.03844786435365677\n",
      "Epoch[0], Val loss: 0.03686641901731491\n",
      "Epoch[0], Batch[4044], Train loss: 0.037671905010938644\n",
      "Epoch[0], Val loss: 0.035463038831949234\n",
      "Epoch[0], Batch[4045], Train loss: 0.0365653820335865\n",
      "Epoch[0], Val loss: 0.036280371248722076\n",
      "Epoch[0], Batch[4046], Train loss: 0.03812137246131897\n",
      "Epoch[0], Val loss: 0.03535648062825203\n",
      "Epoch[0], Batch[4047], Train loss: 0.036074452102184296\n",
      "Epoch[0], Val loss: 0.036197371780872345\n",
      "Epoch[0], Batch[4048], Train loss: 0.03814742714166641\n",
      "Epoch[0], Val loss: 0.03532269597053528\n",
      "Epoch[0], Batch[4049], Train loss: 0.03505433723330498\n",
      "Epoch[0], Val loss: 0.03668615221977234\n",
      "Epoch[0], Batch[4050], Train loss: 0.0385868065059185\n",
      "Epoch[0], Val loss: 0.03644633665680885\n",
      "Epoch[0], Batch[4051], Train loss: 0.03495320305228233\n",
      "Epoch[0], Val loss: 0.03366190940141678\n",
      "Epoch[0], Batch[4052], Train loss: 0.038118794560432434\n",
      "Epoch[0], Val loss: 0.034199152141809464\n",
      "Epoch[0], Batch[4053], Train loss: 0.04005390778183937\n",
      "Epoch[0], Val loss: 0.038789767771959305\n",
      "Epoch[0], Batch[4054], Train loss: 0.03854052722454071\n",
      "Epoch[0], Val loss: 0.037242740392684937\n",
      "Epoch[0], Batch[4055], Train loss: 0.03891768679022789\n",
      "Epoch[0], Val loss: 0.038028158247470856\n",
      "Epoch[0], Batch[4056], Train loss: 0.03631412610411644\n",
      "Epoch[0], Val loss: 0.03460800647735596\n",
      "Epoch[0], Batch[4057], Train loss: 0.04008479416370392\n",
      "Epoch[0], Val loss: 0.03631022572517395\n",
      "Epoch[0], Batch[4058], Train loss: 0.039251431822776794\n",
      "Epoch[0], Val loss: 0.036458760499954224\n",
      "Epoch[0], Batch[4059], Train loss: 0.0378214530646801\n",
      "Epoch[0], Val loss: 0.036862969398498535\n",
      "Epoch[0], Batch[4060], Train loss: 0.03738327696919441\n",
      "Epoch[0], Val loss: 0.03589529171586037\n",
      "Epoch[0], Batch[4061], Train loss: 0.03585153818130493\n",
      "Epoch[0], Val loss: 0.037139762192964554\n",
      "Epoch[0], Batch[4062], Train loss: 0.03690345585346222\n",
      "Epoch[0], Val loss: 0.03576613962650299\n",
      "Epoch[0], Batch[4063], Train loss: 0.038179051131010056\n",
      "Epoch[0], Val loss: 0.034676436334848404\n",
      "Epoch[0], Batch[4064], Train loss: 0.037503816187381744\n",
      "Epoch[0], Val loss: 0.037060875445604324\n",
      "Epoch[0], Batch[4065], Train loss: 0.0398697666823864\n",
      "Epoch[0], Val loss: 0.04032633826136589\n",
      "Epoch[0], Batch[4066], Train loss: 0.038202915340662\n",
      "Epoch[0], Val loss: 0.03605937212705612\n",
      "Epoch[0], Batch[4067], Train loss: 0.037709951400756836\n",
      "Epoch[0], Val loss: 0.03776492550969124\n",
      "Epoch[0], Batch[4068], Train loss: 0.03729413077235222\n",
      "Epoch[0], Val loss: 0.03736720234155655\n",
      "Epoch[0], Batch[4069], Train loss: 0.03538617864251137\n",
      "Epoch[0], Val loss: 0.03550199791789055\n",
      "Epoch[0], Batch[4070], Train loss: 0.036707039922475815\n",
      "Epoch[0], Val loss: 0.037090402096509933\n",
      "Epoch[0], Batch[4071], Train loss: 0.03768305853009224\n",
      "Epoch[0], Val loss: 0.03879142180085182\n",
      "Epoch[0], Batch[4072], Train loss: 0.036145471036434174\n",
      "Epoch[0], Val loss: 0.03711441159248352\n",
      "Epoch[0], Batch[4073], Train loss: 0.037988144904375076\n",
      "Epoch[0], Val loss: 0.03695297613739967\n",
      "Epoch[0], Batch[4074], Train loss: 0.039132848381996155\n",
      "Epoch[0], Val loss: 0.03646150603890419\n",
      "Epoch[0], Batch[4075], Train loss: 0.03727087751030922\n",
      "Epoch[0], Val loss: 0.03732019290328026\n",
      "Epoch[0], Batch[4076], Train loss: 0.03903540223836899\n",
      "Epoch[0], Val loss: 0.03598925471305847\n",
      "Epoch[0], Batch[4077], Train loss: 0.03806338086724281\n",
      "Epoch[0], Val loss: 0.0346832238137722\n",
      "Epoch[0], Batch[4078], Train loss: 0.03885214775800705\n",
      "Epoch[0], Val loss: 0.03912751004099846\n",
      "Epoch[0], Batch[4079], Train loss: 0.03648946434259415\n",
      "Epoch[0], Val loss: 0.03480469807982445\n",
      "Epoch[0], Batch[4080], Train loss: 0.037253256887197495\n",
      "Epoch[0], Val loss: 0.03655543550848961\n",
      "Epoch[0], Batch[4081], Train loss: 0.03752386197447777\n",
      "Epoch[0], Val loss: 0.036073800176382065\n",
      "Epoch[0], Batch[4082], Train loss: 0.038684770464897156\n",
      "Epoch[0], Val loss: 0.0355834886431694\n",
      "Epoch[0], Batch[4083], Train loss: 0.03794079273939133\n",
      "Epoch[0], Val loss: 0.034240543842315674\n",
      "Epoch[0], Batch[4084], Train loss: 0.036303117871284485\n",
      "Epoch[0], Val loss: 0.034732118248939514\n",
      "Epoch[0], Batch[4085], Train loss: 0.03956281766295433\n",
      "Epoch[0], Val loss: 0.03631874918937683\n",
      "Epoch[0], Batch[4086], Train loss: 0.035333387553691864\n",
      "Epoch[0], Val loss: 0.035737600177526474\n",
      "Epoch[0], Batch[4087], Train loss: 0.03855370730161667\n",
      "Epoch[0], Val loss: 0.03726179897785187\n",
      "Epoch[0], Batch[4088], Train loss: 0.037449415773153305\n",
      "Epoch[0], Val loss: 0.03562276065349579\n",
      "Epoch[0], Batch[4089], Train loss: 0.03625147417187691\n",
      "Epoch[0], Val loss: 0.036968622356653214\n",
      "Epoch[0], Batch[4090], Train loss: 0.038273606449365616\n",
      "Epoch[0], Val loss: 0.03515050187706947\n",
      "Epoch[0], Batch[4091], Train loss: 0.036507558077573776\n",
      "Epoch[0], Val loss: 0.03563518077135086\n",
      "Epoch[0], Batch[4092], Train loss: 0.037274010479450226\n",
      "Epoch[0], Val loss: 0.035501375794410706\n",
      "Epoch[0], Batch[4093], Train loss: 0.03836781159043312\n",
      "Epoch[0], Val loss: 0.0360669307410717\n",
      "Epoch[0], Batch[4094], Train loss: 0.03553934767842293\n",
      "Epoch[0], Val loss: 0.03953871503472328\n",
      "Epoch[0], Batch[4095], Train loss: 0.03689070791006088\n",
      "Epoch[0], Val loss: 0.03563957288861275\n",
      "Epoch[0], Batch[4096], Train loss: 0.038830529898405075\n",
      "Epoch[0], Val loss: 0.035578176379203796\n",
      "Epoch[0], Batch[4097], Train loss: 0.03670661523938179\n",
      "Epoch[0], Val loss: 0.03676939383149147\n",
      "Epoch[0], Batch[4098], Train loss: 0.03727065026760101\n",
      "Epoch[0], Val loss: 0.03752220794558525\n",
      "Epoch[0], Batch[4099], Train loss: 0.03907729685306549\n",
      "Epoch[0], Val loss: 0.03594717010855675\n",
      "Epoch[0], Batch[4100], Train loss: 0.03903573751449585\n",
      "Epoch[0], Val loss: 0.038285668939352036\n",
      "Epoch[0], Batch[4101], Train loss: 0.039937667548656464\n",
      "Epoch[0], Val loss: 0.03615822643041611\n",
      "Epoch[0], Batch[4102], Train loss: 0.03795621916651726\n",
      "Epoch[0], Val loss: 0.037324462085962296\n",
      "Epoch[0], Batch[4103], Train loss: 0.0393165685236454\n",
      "Epoch[0], Val loss: 0.035842008888721466\n",
      "Epoch[0], Batch[4104], Train loss: 0.03536290302872658\n",
      "Epoch[0], Val loss: 0.03559095412492752\n",
      "Epoch[0], Batch[4105], Train loss: 0.038131289184093475\n",
      "Epoch[0], Val loss: 0.036730121821165085\n",
      "Epoch[0], Batch[4106], Train loss: 0.0379757285118103\n",
      "Epoch[0], Val loss: 0.03560522571206093\n",
      "Epoch[0], Batch[4107], Train loss: 0.03865630552172661\n",
      "Epoch[0], Val loss: 0.0358177050948143\n",
      "Epoch[0], Batch[4108], Train loss: 0.03677230700850487\n",
      "Epoch[0], Val loss: 0.03675291687250137\n",
      "Epoch[0], Batch[4109], Train loss: 0.03673863410949707\n",
      "Epoch[0], Val loss: 0.035769905894994736\n",
      "Epoch[0], Batch[4110], Train loss: 0.037656769156455994\n",
      "Epoch[0], Val loss: 0.03700221702456474\n",
      "Epoch[0], Batch[4111], Train loss: 0.035270385444164276\n",
      "Epoch[0], Val loss: 0.0384608656167984\n",
      "Epoch[0], Batch[4112], Train loss: 0.036245036870241165\n",
      "Epoch[0], Val loss: 0.03827603906393051\n",
      "Epoch[0], Batch[4113], Train loss: 0.037932511419057846\n",
      "Epoch[0], Val loss: 0.035614654421806335\n",
      "Epoch[0], Batch[4114], Train loss: 0.03698661923408508\n",
      "Epoch[0], Val loss: 0.03985060006380081\n",
      "Epoch[0], Batch[4115], Train loss: 0.036208413541316986\n",
      "Epoch[0], Val loss: 0.03622011840343475\n",
      "Epoch[0], Batch[4116], Train loss: 0.03674589470028877\n",
      "Epoch[0], Val loss: 0.03680041804909706\n",
      "Epoch[0], Batch[4117], Train loss: 0.03759436681866646\n",
      "Epoch[0], Val loss: 0.0350518599152565\n",
      "Epoch[0], Batch[4118], Train loss: 0.03885394707322121\n",
      "Epoch[0], Val loss: 0.03781075030565262\n",
      "Epoch[0], Batch[4119], Train loss: 0.0390365831553936\n",
      "Epoch[0], Val loss: 0.03607159107923508\n",
      "Epoch[0], Batch[4120], Train loss: 0.03838811442255974\n",
      "Epoch[0], Val loss: 0.038790564984083176\n",
      "Epoch[0], Batch[4121], Train loss: 0.036472711712121964\n",
      "Epoch[0], Val loss: 0.03592995926737785\n",
      "Epoch[0], Batch[4122], Train loss: 0.036836620420217514\n",
      "Epoch[0], Val loss: 0.036484383046627045\n",
      "Epoch[0], Batch[4123], Train loss: 0.038521505892276764\n",
      "Epoch[0], Val loss: 0.03819067403674126\n",
      "Epoch[0], Batch[4124], Train loss: 0.03810783475637436\n",
      "Epoch[0], Val loss: 0.03734592720866203\n",
      "Epoch[0], Batch[4125], Train loss: 0.0384673997759819\n",
      "Epoch[0], Val loss: 0.03640715405344963\n",
      "Epoch[0], Batch[4126], Train loss: 0.03865686058998108\n",
      "Epoch[0], Val loss: 0.037921879440546036\n",
      "Epoch[0], Batch[4127], Train loss: 0.04028891772031784\n",
      "Epoch[0], Val loss: 0.03630746528506279\n",
      "Epoch[0], Batch[4128], Train loss: 0.03815854340791702\n",
      "Epoch[0], Val loss: 0.03883886709809303\n",
      "Epoch[0], Batch[4129], Train loss: 0.03636595606803894\n",
      "Epoch[0], Val loss: 0.03726004436612129\n",
      "Epoch[0], Batch[4130], Train loss: 0.04018234834074974\n",
      "Epoch[0], Val loss: 0.03548283129930496\n",
      "Epoch[0], Batch[4131], Train loss: 0.033541470766067505\n",
      "Epoch[0], Val loss: 0.037332478910684586\n",
      "Epoch[0], Batch[4132], Train loss: 0.03819246217608452\n",
      "Epoch[0], Val loss: 0.038498878479003906\n",
      "Epoch[0], Batch[4133], Train loss: 0.036397725343704224\n",
      "Epoch[0], Val loss: 0.037453118711709976\n",
      "Epoch[0], Batch[4134], Train loss: 0.03884497284889221\n",
      "Epoch[0], Val loss: 0.03347974270582199\n",
      "Epoch[0], Batch[4135], Train loss: 0.03772300109267235\n",
      "Epoch[0], Val loss: 0.03639545664191246\n",
      "Epoch[0], Batch[4136], Train loss: 0.04084690287709236\n",
      "Epoch[0], Val loss: 0.036902397871017456\n",
      "Epoch[0], Batch[4137], Train loss: 0.036196593195199966\n",
      "Epoch[0], Val loss: 0.037505120038986206\n",
      "Epoch[0], Batch[4138], Train loss: 0.03661492094397545\n",
      "Epoch[0], Val loss: 0.036345288157463074\n",
      "Epoch[0], Batch[4139], Train loss: 0.0364350900053978\n",
      "Epoch[0], Val loss: 0.03479146584868431\n",
      "Epoch[0], Batch[4140], Train loss: 0.03702932596206665\n",
      "Epoch[0], Val loss: 0.036666665226221085\n",
      "Epoch[0], Batch[4141], Train loss: 0.038963038474321365\n",
      "Epoch[0], Val loss: 0.03689068555831909\n",
      "Epoch[0], Batch[4142], Train loss: 0.03636503592133522\n",
      "Epoch[0], Val loss: 0.03698965534567833\n",
      "Epoch[0], Batch[4143], Train loss: 0.03785853460431099\n",
      "Epoch[0], Val loss: 0.036533426493406296\n",
      "Epoch[0], Batch[4144], Train loss: 0.03811872377991676\n",
      "Epoch[0], Val loss: 0.035705115646123886\n",
      "Epoch[0], Batch[4145], Train loss: 0.03862617164850235\n",
      "Epoch[0], Val loss: 0.03701390326023102\n",
      "Epoch[0], Batch[4146], Train loss: 0.04040509834885597\n",
      "Epoch[0], Val loss: 0.035517312586307526\n",
      "Epoch[0], Batch[4147], Train loss: 0.03928126022219658\n",
      "Epoch[0], Val loss: 0.03753717988729477\n",
      "Epoch[0], Batch[4148], Train loss: 0.037709880620241165\n",
      "Epoch[0], Val loss: 0.03980650007724762\n",
      "Epoch[0], Batch[4149], Train loss: 0.0368560366332531\n",
      "Epoch[0], Val loss: 0.04083788022398949\n",
      "Epoch[0], Batch[4150], Train loss: 0.038736212998628616\n",
      "Epoch[0], Val loss: 0.036275625228881836\n",
      "Epoch[0], Batch[4151], Train loss: 0.03768058493733406\n",
      "Epoch[0], Val loss: 0.04083611071109772\n",
      "Epoch[0], Batch[4152], Train loss: 0.039705581963062286\n",
      "Epoch[0], Val loss: 0.03704160824418068\n",
      "Epoch[0], Batch[4153], Train loss: 0.03753877803683281\n",
      "Epoch[0], Val loss: 0.03752993792295456\n",
      "Epoch[0], Batch[4154], Train loss: 0.035567380487918854\n",
      "Epoch[0], Val loss: 0.03631559759378433\n",
      "Epoch[0], Batch[4155], Train loss: 0.037173278629779816\n",
      "Epoch[0], Val loss: 0.035682473331689835\n",
      "Epoch[0], Batch[4156], Train loss: 0.038016755133867264\n",
      "Epoch[0], Val loss: 0.03536716848611832\n",
      "Epoch[0], Batch[4157], Train loss: 0.03735552728176117\n",
      "Epoch[0], Val loss: 0.035327598452568054\n",
      "Epoch[0], Batch[4158], Train loss: 0.03768591210246086\n",
      "Epoch[0], Val loss: 0.03885959833860397\n",
      "Epoch[0], Batch[4159], Train loss: 0.037499863654375076\n",
      "Epoch[0], Val loss: 0.036250039935112\n",
      "Epoch[0], Batch[4160], Train loss: 0.03786975145339966\n",
      "Epoch[0], Val loss: 0.03659597784280777\n",
      "Epoch[0], Batch[4161], Train loss: 0.03627070039510727\n",
      "Epoch[0], Val loss: 0.03442145884037018\n",
      "Epoch[0], Batch[4162], Train loss: 0.03810320422053337\n",
      "Epoch[0], Val loss: 0.03804852068424225\n",
      "Epoch[0], Batch[4163], Train loss: 0.03872348740696907\n",
      "Epoch[0], Val loss: 0.0376223623752594\n",
      "Epoch[0], Batch[4164], Train loss: 0.036498453468084335\n",
      "Epoch[0], Val loss: 0.03616252541542053\n",
      "Epoch[0], Batch[4165], Train loss: 0.03935476392507553\n",
      "Epoch[0], Val loss: 0.03675692901015282\n",
      "Epoch[0], Batch[4166], Train loss: 0.03754492849111557\n",
      "Epoch[0], Val loss: 0.03769667074084282\n",
      "Epoch[0], Batch[4167], Train loss: 0.03914504125714302\n",
      "Epoch[0], Val loss: 0.03558789938688278\n",
      "Epoch[0], Batch[4168], Train loss: 0.03685559704899788\n",
      "Epoch[0], Val loss: 0.03588389977812767\n",
      "Epoch[0], Batch[4169], Train loss: 0.038817327469587326\n",
      "Epoch[0], Val loss: 0.03819501772522926\n",
      "Epoch[0], Batch[4170], Train loss: 0.03945547342300415\n",
      "Epoch[0], Val loss: 0.034346651285886765\n",
      "Epoch[0], Batch[4171], Train loss: 0.034569285809993744\n",
      "Epoch[0], Val loss: 0.035184476524591446\n",
      "Epoch[0], Batch[4172], Train loss: 0.03734105825424194\n",
      "Epoch[0], Val loss: 0.03882654011249542\n",
      "Epoch[0], Batch[4173], Train loss: 0.0390034094452858\n",
      "Epoch[0], Val loss: 0.038002509623765945\n",
      "Epoch[0], Batch[4174], Train loss: 0.03781387209892273\n",
      "Epoch[0], Val loss: 0.038335688412189484\n",
      "Epoch[0], Batch[4175], Train loss: 0.037433210760354996\n",
      "Epoch[0], Val loss: 0.03736467286944389\n",
      "Epoch[0], Batch[4176], Train loss: 0.039847515523433685\n",
      "Epoch[0], Val loss: 0.03552732616662979\n",
      "Epoch[0], Batch[4177], Train loss: 0.037071555852890015\n",
      "Epoch[0], Val loss: 0.036478061228990555\n",
      "Epoch[0], Batch[4178], Train loss: 0.038372669368982315\n",
      "Epoch[0], Val loss: 0.03576193004846573\n",
      "Epoch[0], Batch[4179], Train loss: 0.03839984908699989\n",
      "Epoch[0], Val loss: 0.03513767570257187\n",
      "Epoch[0], Batch[4180], Train loss: 0.03826218098402023\n",
      "Epoch[0], Val loss: 0.03628111258149147\n",
      "Epoch[0], Batch[4181], Train loss: 0.03836292028427124\n",
      "Epoch[0], Val loss: 0.03696933761239052\n",
      "Epoch[0], Batch[4182], Train loss: 0.038436051458120346\n",
      "Epoch[0], Val loss: 0.03799009695649147\n",
      "Epoch[0], Batch[4183], Train loss: 0.04065157100558281\n",
      "Epoch[0], Val loss: 0.03648820519447327\n",
      "Epoch[0], Batch[4184], Train loss: 0.03955967351794243\n",
      "Epoch[0], Val loss: 0.03682854026556015\n",
      "Epoch[0], Batch[4185], Train loss: 0.03905322775244713\n",
      "Epoch[0], Val loss: 0.03521634265780449\n",
      "Epoch[0], Batch[4186], Train loss: 0.03858901932835579\n",
      "Epoch[0], Val loss: 0.03718316927552223\n",
      "Epoch[0], Batch[4187], Train loss: 0.03617245703935623\n",
      "Epoch[0], Val loss: 0.03842684626579285\n",
      "Epoch[0], Batch[4188], Train loss: 0.03720487281680107\n",
      "Epoch[0], Val loss: 0.03690417483448982\n",
      "Epoch[0], Batch[4189], Train loss: 0.03799756243824959\n",
      "Epoch[0], Val loss: 0.036509133875370026\n",
      "Epoch[0], Batch[4190], Train loss: 0.03817526251077652\n",
      "Epoch[0], Val loss: 0.035885006189346313\n",
      "Epoch[0], Batch[4191], Train loss: 0.0384286604821682\n",
      "Epoch[0], Val loss: 0.03804643824696541\n",
      "Epoch[0], Batch[4192], Train loss: 0.038264140486717224\n",
      "Epoch[0], Val loss: 0.03833048418164253\n",
      "Epoch[0], Batch[4193], Train loss: 0.036848247051239014\n",
      "Epoch[0], Val loss: 0.03619092330336571\n",
      "Epoch[0], Batch[4194], Train loss: 0.04051125422120094\n",
      "Epoch[0], Val loss: 0.034975335001945496\n",
      "Epoch[0], Batch[4195], Train loss: 0.03667469695210457\n",
      "Epoch[0], Val loss: 0.03667323663830757\n",
      "Epoch[0], Batch[4196], Train loss: 0.03682531416416168\n",
      "Epoch[0], Val loss: 0.03824928402900696\n",
      "Epoch[0], Batch[4197], Train loss: 0.038852207362651825\n",
      "Epoch[0], Val loss: 0.037182699888944626\n",
      "Epoch[0], Batch[4198], Train loss: 0.03695593401789665\n",
      "Epoch[0], Val loss: 0.03778346627950668\n",
      "Epoch[0], Batch[4199], Train loss: 0.0380309522151947\n",
      "Epoch[0], Val loss: 0.03599345684051514\n",
      "Epoch[0], Batch[4200], Train loss: 0.036871690303087234\n",
      "Epoch[0], Val loss: 0.03553002327680588\n",
      "Epoch[0], Batch[4201], Train loss: 0.03562190756201744\n",
      "Epoch[0], Val loss: 0.03664986789226532\n",
      "Epoch[0], Batch[4202], Train loss: 0.03742509335279465\n",
      "Epoch[0], Val loss: 0.04020654410123825\n",
      "Epoch[0], Batch[4203], Train loss: 0.03755359724164009\n",
      "Epoch[0], Val loss: 0.03873521089553833\n",
      "Epoch[0], Batch[4204], Train loss: 0.036555659025907516\n",
      "Epoch[0], Val loss: 0.0357784740626812\n",
      "Epoch[0], Batch[4205], Train loss: 0.037604670971632004\n",
      "Epoch[0], Val loss: 0.03450092673301697\n",
      "Epoch[0], Batch[4206], Train loss: 0.038765788078308105\n",
      "Epoch[0], Val loss: 0.038201138377189636\n",
      "Epoch[0], Batch[4207], Train loss: 0.03800104930996895\n",
      "Epoch[0], Val loss: 0.03661474213004112\n",
      "Epoch[0], Batch[4208], Train loss: 0.03906650096178055\n",
      "Epoch[0], Val loss: 0.035794325172901154\n",
      "Epoch[0], Batch[4209], Train loss: 0.03749172016978264\n",
      "Epoch[0], Val loss: 0.03537093102931976\n",
      "Epoch[0], Batch[4210], Train loss: 0.03859410434961319\n",
      "Epoch[0], Val loss: 0.037982407957315445\n",
      "Epoch[0], Batch[4211], Train loss: 0.037414297461509705\n",
      "Epoch[0], Val loss: 0.03748659789562225\n",
      "Epoch[0], Batch[4212], Train loss: 0.038732558488845825\n",
      "Epoch[0], Val loss: 0.0351952500641346\n",
      "Epoch[0], Batch[4213], Train loss: 0.04043695330619812\n",
      "Epoch[0], Val loss: 0.03974095731973648\n",
      "Epoch[0], Batch[4214], Train loss: 0.03887004405260086\n",
      "Epoch[0], Val loss: 0.03378112241625786\n",
      "Epoch[0], Batch[4215], Train loss: 0.03449089452624321\n",
      "Epoch[0], Val loss: 0.03575544059276581\n",
      "Epoch[0], Batch[4216], Train loss: 0.03877205774188042\n",
      "Epoch[0], Val loss: 0.034978337585926056\n",
      "Epoch[0], Batch[4217], Train loss: 0.0360306091606617\n",
      "Epoch[0], Val loss: 0.03502484783530235\n",
      "Epoch[0], Batch[4218], Train loss: 0.03912433981895447\n",
      "Epoch[0], Val loss: 0.037946686148643494\n",
      "Epoch[0], Batch[4219], Train loss: 0.03800428286194801\n",
      "Epoch[0], Val loss: 0.03555017337203026\n",
      "Epoch[0], Batch[4220], Train loss: 0.04117426648736\n",
      "Epoch[0], Val loss: 0.037095434963703156\n",
      "Epoch[0], Batch[4221], Train loss: 0.03882891684770584\n",
      "Epoch[0], Val loss: 0.038450028747320175\n",
      "Epoch[0], Batch[4222], Train loss: 0.038255754858255386\n",
      "Epoch[0], Val loss: 0.03309963271021843\n",
      "Epoch[0], Batch[4223], Train loss: 0.03911915421485901\n",
      "Epoch[0], Val loss: 0.03779473528265953\n",
      "Epoch[0], Batch[4224], Train loss: 0.037975382059812546\n",
      "Epoch[0], Val loss: 0.03750040382146835\n",
      "Epoch[0], Batch[4225], Train loss: 0.0350046306848526\n",
      "Epoch[0], Val loss: 0.034695859998464584\n",
      "Epoch[0], Batch[4226], Train loss: 0.03784283623099327\n",
      "Epoch[0], Val loss: 0.036599867045879364\n",
      "Epoch[0], Batch[4227], Train loss: 0.03637736663222313\n",
      "Epoch[0], Val loss: 0.03656153008341789\n",
      "Epoch[0], Batch[4228], Train loss: 0.03685877472162247\n",
      "Epoch[0], Val loss: 0.03628213703632355\n",
      "Epoch[0], Batch[4229], Train loss: 0.03684766963124275\n",
      "Epoch[0], Val loss: 0.03787651285529137\n",
      "Epoch[0], Batch[4230], Train loss: 0.03460143506526947\n",
      "Epoch[0], Val loss: 0.036747951060533524\n",
      "Epoch[0], Batch[4231], Train loss: 0.03851306810975075\n",
      "Epoch[0], Val loss: 0.03546058014035225\n",
      "Epoch[0], Batch[4232], Train loss: 0.03923613578081131\n",
      "Epoch[0], Val loss: 0.03376944735646248\n",
      "Epoch[0], Batch[4233], Train loss: 0.03840978071093559\n",
      "Epoch[0], Val loss: 0.03589004650712013\n",
      "Epoch[0], Batch[4234], Train loss: 0.037736646831035614\n",
      "Epoch[0], Val loss: 0.03936925157904625\n",
      "Epoch[0], Batch[4235], Train loss: 0.03812714293599129\n",
      "Epoch[0], Val loss: 0.0389074869453907\n",
      "Epoch[0], Batch[4236], Train loss: 0.03806040808558464\n",
      "Epoch[0], Val loss: 0.03668910264968872\n",
      "Epoch[0], Batch[4237], Train loss: 0.036016229540109634\n",
      "Epoch[0], Val loss: 0.037200409919023514\n",
      "Epoch[0], Batch[4238], Train loss: 0.038691479712724686\n",
      "Epoch[0], Val loss: 0.037301745265722275\n",
      "Epoch[0], Batch[4239], Train loss: 0.042557090520858765\n",
      "Epoch[0], Val loss: 0.037380240857601166\n",
      "Epoch[0], Batch[4240], Train loss: 0.035489555448293686\n",
      "Epoch[0], Val loss: 0.03594869002699852\n",
      "Epoch[0], Batch[4241], Train loss: 0.035909503698349\n",
      "Epoch[0], Val loss: 0.03700374811887741\n",
      "Epoch[0], Batch[4242], Train loss: 0.037251945585012436\n",
      "Epoch[0], Val loss: 0.03656991198658943\n",
      "Epoch[0], Batch[4243], Train loss: 0.03634738177061081\n",
      "Epoch[0], Val loss: 0.034703578799963\n",
      "Epoch[0], Batch[4244], Train loss: 0.03942636772990227\n",
      "Epoch[0], Val loss: 0.036974530667066574\n",
      "Epoch[0], Batch[4245], Train loss: 0.037818167358636856\n",
      "Epoch[0], Val loss: 0.03511103242635727\n",
      "Epoch[0], Batch[4246], Train loss: 0.036343008279800415\n",
      "Epoch[0], Val loss: 0.0370592437684536\n",
      "Epoch[0], Batch[4247], Train loss: 0.03712575510144234\n",
      "Epoch[0], Val loss: 0.034237366169691086\n",
      "Epoch[0], Batch[4248], Train loss: 0.037064213305711746\n",
      "Epoch[0], Val loss: 0.033638011664152145\n",
      "Epoch[0], Batch[4249], Train loss: 0.03652036935091019\n",
      "Epoch[0], Val loss: 0.03482232242822647\n",
      "Epoch[0], Batch[4250], Train loss: 0.03780152648687363\n",
      "Epoch[0], Val loss: 0.03699163347482681\n",
      "Epoch[0], Batch[4251], Train loss: 0.03664640337228775\n",
      "Epoch[0], Val loss: 0.03677893802523613\n",
      "Epoch[0], Batch[4252], Train loss: 0.03830287978053093\n",
      "Epoch[0], Val loss: 0.034934014081954956\n",
      "Epoch[0], Batch[4253], Train loss: 0.03827323764562607\n",
      "Epoch[0], Val loss: 0.03891604021191597\n",
      "Epoch[0], Batch[4254], Train loss: 0.038855262100696564\n",
      "Epoch[0], Val loss: 0.03625849261879921\n",
      "Epoch[0], Batch[4255], Train loss: 0.037018418312072754\n",
      "Epoch[0], Val loss: 0.03670138865709305\n",
      "Epoch[0], Batch[4256], Train loss: 0.03955680504441261\n",
      "Epoch[0], Val loss: 0.035279661417007446\n",
      "Epoch[0], Batch[4257], Train loss: 0.03681398555636406\n",
      "Epoch[0], Val loss: 0.03469925373792648\n",
      "Epoch[0], Batch[4258], Train loss: 0.03926195576786995\n",
      "Epoch[0], Val loss: 0.03817174956202507\n",
      "Epoch[0], Batch[4259], Train loss: 0.038052748888731\n",
      "Epoch[0], Val loss: 0.035828813910484314\n",
      "Epoch[0], Batch[4260], Train loss: 0.037849750369787216\n",
      "Epoch[0], Val loss: 0.03670182824134827\n",
      "Epoch[0], Batch[4261], Train loss: 0.036440979689359665\n",
      "Epoch[0], Val loss: 0.03774581477046013\n",
      "Epoch[0], Batch[4262], Train loss: 0.03729179501533508\n",
      "Epoch[0], Val loss: 0.03349180519580841\n",
      "Epoch[0], Batch[4263], Train loss: 0.03954053670167923\n",
      "Epoch[0], Val loss: 0.03540017083287239\n",
      "Epoch[0], Batch[4264], Train loss: 0.03868434205651283\n",
      "Epoch[0], Val loss: 0.03694981336593628\n",
      "Epoch[0], Batch[4265], Train loss: 0.03709114342927933\n",
      "Epoch[0], Val loss: 0.03655708208680153\n",
      "Epoch[0], Batch[4266], Train loss: 0.03761126101016998\n",
      "Epoch[0], Val loss: 0.03623761236667633\n",
      "Epoch[0], Batch[4267], Train loss: 0.035691458731889725\n",
      "Epoch[0], Val loss: 0.03588680177927017\n",
      "Epoch[0], Batch[4268], Train loss: 0.03806891664862633\n",
      "Epoch[0], Val loss: 0.036150552332401276\n",
      "Epoch[0], Batch[4269], Train loss: 0.03882328420877457\n",
      "Epoch[0], Val loss: 0.03417385369539261\n",
      "Epoch[0], Batch[4270], Train loss: 0.03566104918718338\n",
      "Epoch[0], Val loss: 0.037982840090990067\n",
      "Epoch[0], Batch[4271], Train loss: 0.038568880409002304\n",
      "Epoch[0], Val loss: 0.038623467087745667\n",
      "Epoch[0], Batch[4272], Train loss: 0.037811536341905594\n",
      "Epoch[0], Val loss: 0.034232836216688156\n",
      "Epoch[0], Batch[4273], Train loss: 0.03505752235651016\n",
      "Epoch[0], Val loss: 0.036125678569078445\n",
      "Epoch[0], Batch[4274], Train loss: 0.037230782210826874\n",
      "Epoch[0], Val loss: 0.034469958394765854\n",
      "Epoch[0], Batch[4275], Train loss: 0.038326144218444824\n",
      "Epoch[0], Val loss: 0.03668305277824402\n",
      "Epoch[0], Batch[4276], Train loss: 0.035294823348522186\n",
      "Epoch[0], Val loss: 0.03602638468146324\n",
      "Epoch[0], Batch[4277], Train loss: 0.03753937780857086\n",
      "Epoch[0], Val loss: 0.039640530943870544\n",
      "Epoch[0], Batch[4278], Train loss: 0.036886267364025116\n",
      "Epoch[0], Val loss: 0.036784302443265915\n",
      "Epoch[0], Batch[4279], Train loss: 0.036696553230285645\n",
      "Epoch[0], Val loss: 0.037280261516571045\n",
      "Epoch[0], Batch[4280], Train loss: 0.03628845885396004\n",
      "Epoch[0], Val loss: 0.03475790470838547\n",
      "Epoch[0], Batch[4281], Train loss: 0.03636913001537323\n",
      "Epoch[0], Val loss: 0.035933319479227066\n",
      "Epoch[0], Batch[4282], Train loss: 0.035545043647289276\n",
      "Epoch[0], Val loss: 0.035430263727903366\n",
      "Epoch[0], Batch[4283], Train loss: 0.039150577038526535\n",
      "Epoch[0], Val loss: 0.03504209592938423\n",
      "Epoch[0], Batch[4284], Train loss: 0.04015936702489853\n",
      "Epoch[0], Val loss: 0.03345080465078354\n",
      "Epoch[0], Batch[4285], Train loss: 0.03859687224030495\n",
      "Epoch[0], Val loss: 0.038374729454517365\n",
      "Epoch[0], Batch[4286], Train loss: 0.037520017474889755\n",
      "Epoch[0], Val loss: 0.0346670038998127\n",
      "Epoch[0], Batch[4287], Train loss: 0.03903912380337715\n",
      "Epoch[0], Val loss: 0.036778900772333145\n",
      "Epoch[0], Batch[4288], Train loss: 0.038548585027456284\n",
      "Epoch[0], Val loss: 0.03515850380063057\n",
      "Epoch[0], Batch[4289], Train loss: 0.03879686817526817\n",
      "Epoch[0], Val loss: 0.03803423047065735\n",
      "Epoch[0], Batch[4290], Train loss: 0.0393078550696373\n",
      "Epoch[0], Val loss: 0.03532734140753746\n",
      "Epoch[0], Batch[4291], Train loss: 0.03788280859589577\n",
      "Epoch[0], Val loss: 0.04017898440361023\n",
      "Epoch[0], Batch[4292], Train loss: 0.039889268577098846\n",
      "Epoch[0], Val loss: 0.039048273116350174\n",
      "Epoch[0], Batch[4293], Train loss: 0.03996860608458519\n",
      "Epoch[0], Val loss: 0.037447769194841385\n",
      "Epoch[0], Batch[4294], Train loss: 0.03657563403248787\n",
      "Epoch[0], Val loss: 0.03340066224336624\n",
      "Epoch[0], Batch[4295], Train loss: 0.0374772883951664\n",
      "Epoch[0], Val loss: 0.03681236132979393\n",
      "Epoch[0], Batch[4296], Train loss: 0.03962298855185509\n",
      "Epoch[0], Val loss: 0.03765135258436203\n",
      "Epoch[0], Batch[4297], Train loss: 0.03990580886602402\n",
      "Epoch[0], Val loss: 0.037002019584178925\n",
      "Epoch[0], Batch[4298], Train loss: 0.03801833838224411\n",
      "Epoch[0], Val loss: 0.03495364263653755\n",
      "Epoch[0], Batch[4299], Train loss: 0.036579981446266174\n",
      "Epoch[0], Val loss: 0.03657316789031029\n",
      "Epoch[0], Batch[4300], Train loss: 0.036322202533483505\n",
      "Epoch[0], Val loss: 0.03839634358882904\n",
      "Epoch[0], Batch[4301], Train loss: 0.03541833162307739\n",
      "Epoch[0], Val loss: 0.03538908809423447\n",
      "Epoch[0], Batch[4302], Train loss: 0.03761111944913864\n",
      "Epoch[0], Val loss: 0.03425394371151924\n",
      "Epoch[0], Batch[4303], Train loss: 0.03746393322944641\n",
      "Epoch[0], Val loss: 0.03630255162715912\n",
      "Epoch[0], Batch[4304], Train loss: 0.040038082748651505\n",
      "Epoch[0], Val loss: 0.036536794155836105\n",
      "Epoch[0], Batch[4305], Train loss: 0.03676125034689903\n",
      "Epoch[0], Val loss: 0.03632757440209389\n",
      "Epoch[0], Batch[4306], Train loss: 0.038125306367874146\n",
      "Epoch[0], Val loss: 0.03580617904663086\n",
      "Epoch[0], Batch[4307], Train loss: 0.037756066769361496\n",
      "Epoch[0], Val loss: 0.03885868191719055\n",
      "Epoch[0], Batch[4308], Train loss: 0.03721904382109642\n",
      "Epoch[0], Val loss: 0.03522656857967377\n",
      "Epoch[0], Batch[4309], Train loss: 0.038275908678770065\n",
      "Epoch[0], Val loss: 0.03593186289072037\n",
      "Epoch[0], Batch[4310], Train loss: 0.0391852892935276\n",
      "Epoch[0], Val loss: 0.03578576818108559\n",
      "Epoch[0], Batch[4311], Train loss: 0.03937254101037979\n",
      "Epoch[0], Val loss: 0.03468875586986542\n",
      "Epoch[0], Batch[4312], Train loss: 0.03630330041050911\n",
      "Epoch[0], Val loss: 0.03684071823954582\n",
      "Epoch[0], Batch[4313], Train loss: 0.03878973796963692\n",
      "Epoch[0], Val loss: 0.034548673778772354\n",
      "Epoch[0], Batch[4314], Train loss: 0.037671931087970734\n",
      "Epoch[0], Val loss: 0.03509601950645447\n",
      "Epoch[0], Batch[4315], Train loss: 0.03667813912034035\n",
      "Epoch[0], Val loss: 0.03769528120756149\n",
      "Epoch[0], Batch[4316], Train loss: 0.037403687834739685\n",
      "Epoch[0], Val loss: 0.03490639477968216\n",
      "Epoch[0], Batch[4317], Train loss: 0.034696146845817566\n",
      "Epoch[0], Val loss: 0.033863481134176254\n",
      "Epoch[0], Batch[4318], Train loss: 0.035362858325242996\n",
      "Epoch[0], Val loss: 0.03569626808166504\n",
      "Epoch[0], Batch[4319], Train loss: 0.03657954931259155\n",
      "Epoch[0], Val loss: 0.03574644774198532\n",
      "Epoch[0], Batch[4320], Train loss: 0.03815849870443344\n",
      "Epoch[0], Val loss: 0.03541256859898567\n",
      "Epoch[0], Batch[4321], Train loss: 0.03719344735145569\n",
      "Epoch[0], Val loss: 0.03538314998149872\n",
      "Epoch[0], Batch[4322], Train loss: 0.036050207912921906\n",
      "Epoch[0], Val loss: 0.03524749353528023\n",
      "Epoch[0], Batch[4323], Train loss: 0.037388112396001816\n",
      "Epoch[0], Val loss: 0.035976774990558624\n",
      "Epoch[0], Batch[4324], Train loss: 0.039054084569215775\n",
      "Epoch[0], Val loss: 0.036593977361917496\n",
      "Epoch[0], Batch[4325], Train loss: 0.03656476363539696\n",
      "Epoch[0], Val loss: 0.036983273923397064\n",
      "Epoch[0], Batch[4326], Train loss: 0.039311423897743225\n",
      "Epoch[0], Val loss: 0.037868063896894455\n",
      "Epoch[0], Batch[4327], Train loss: 0.0384758859872818\n",
      "Epoch[0], Val loss: 0.036891818046569824\n",
      "Epoch[0], Batch[4328], Train loss: 0.035679806023836136\n",
      "Epoch[0], Val loss: 0.03635129705071449\n",
      "Epoch[0], Batch[4329], Train loss: 0.039327215403318405\n",
      "Epoch[0], Val loss: 0.03784485161304474\n",
      "Epoch[0], Batch[4330], Train loss: 0.03929372876882553\n",
      "Epoch[0], Val loss: 0.03362782672047615\n",
      "Epoch[0], Batch[4331], Train loss: 0.0379682295024395\n",
      "Epoch[0], Val loss: 0.0364275723695755\n",
      "Epoch[0], Batch[4332], Train loss: 0.034582171589136124\n",
      "Epoch[0], Val loss: 0.03754719719290733\n",
      "Epoch[0], Batch[4333], Train loss: 0.03576969355344772\n",
      "Epoch[0], Val loss: 0.037813641130924225\n",
      "Epoch[0], Batch[4334], Train loss: 0.03624935820698738\n",
      "Epoch[0], Val loss: 0.035373393446207047\n",
      "Epoch[0], Batch[4335], Train loss: 0.0353676863014698\n",
      "Epoch[0], Val loss: 0.035149946808815\n",
      "Epoch[0], Batch[4336], Train loss: 0.036832574754953384\n",
      "Epoch[0], Val loss: 0.03609459474682808\n",
      "Epoch[0], Batch[4337], Train loss: 0.037029776722192764\n",
      "Epoch[0], Val loss: 0.03771539404988289\n",
      "Epoch[0], Batch[4338], Train loss: 0.036201655864715576\n",
      "Epoch[0], Val loss: 0.03471352159976959\n",
      "Epoch[0], Batch[4339], Train loss: 0.037068333476781845\n",
      "Epoch[0], Val loss: 0.035903654992580414\n",
      "Epoch[0], Batch[4340], Train loss: 0.03793490305542946\n",
      "Epoch[0], Val loss: 0.03643343597650528\n",
      "Epoch[0], Batch[4341], Train loss: 0.03711405396461487\n",
      "Epoch[0], Val loss: 0.03822684288024902\n",
      "Epoch[0], Batch[4342], Train loss: 0.0382721833884716\n",
      "Epoch[0], Val loss: 0.035930417478084564\n",
      "Epoch[0], Batch[4343], Train loss: 0.03835444152355194\n",
      "Epoch[0], Val loss: 0.03638627380132675\n",
      "Epoch[0], Batch[4344], Train loss: 0.036400794982910156\n",
      "Epoch[0], Val loss: 0.03595118597149849\n",
      "Epoch[0], Batch[4345], Train loss: 0.03967781364917755\n",
      "Epoch[0], Val loss: 0.03491124510765076\n",
      "Epoch[0], Batch[4346], Train loss: 0.03570697456598282\n",
      "Epoch[0], Val loss: 0.036122698336839676\n",
      "Epoch[0], Batch[4347], Train loss: 0.03761729970574379\n",
      "Epoch[0], Val loss: 0.03410807251930237\n",
      "Epoch[0], Batch[4348], Train loss: 0.03742918372154236\n",
      "Epoch[0], Val loss: 0.03641567751765251\n",
      "Epoch[0], Batch[4349], Train loss: 0.036346469074487686\n",
      "Epoch[0], Val loss: 0.03530630096793175\n",
      "Epoch[0], Batch[4350], Train loss: 0.03764227777719498\n",
      "Epoch[0], Val loss: 0.0336652472615242\n",
      "Epoch[0], Batch[4351], Train loss: 0.03879180923104286\n",
      "Epoch[0], Val loss: 0.03743263706564903\n",
      "Epoch[0], Batch[4352], Train loss: 0.03751157224178314\n",
      "Epoch[0], Val loss: 0.03649023547768593\n",
      "Epoch[0], Batch[4353], Train loss: 0.03716647997498512\n",
      "Epoch[0], Val loss: 0.035375189036130905\n",
      "Epoch[0], Batch[4354], Train loss: 0.03675924986600876\n",
      "Epoch[0], Val loss: 0.03556743264198303\n",
      "Epoch[0], Batch[4355], Train loss: 0.037470944225788116\n",
      "Epoch[0], Val loss: 0.03661873936653137\n",
      "Epoch[0], Batch[4356], Train loss: 0.03523677587509155\n",
      "Epoch[0], Val loss: 0.03541790693998337\n",
      "Epoch[0], Batch[4357], Train loss: 0.03956638649106026\n",
      "Epoch[0], Val loss: 0.036163054406642914\n",
      "Epoch[0], Batch[4358], Train loss: 0.03921760991215706\n",
      "Epoch[0], Val loss: 0.03763110935688019\n",
      "Epoch[0], Batch[4359], Train loss: 0.036906443536281586\n",
      "Epoch[0], Val loss: 0.03577712923288345\n",
      "Epoch[0], Batch[4360], Train loss: 0.036919135600328445\n",
      "Epoch[0], Val loss: 0.03437020257115364\n",
      "Epoch[0], Batch[4361], Train loss: 0.03723882883787155\n",
      "Epoch[0], Val loss: 0.03397241234779358\n",
      "Epoch[0], Batch[4362], Train loss: 0.03731196001172066\n",
      "Epoch[0], Val loss: 0.03543945029377937\n",
      "Epoch[0], Batch[4363], Train loss: 0.03661278635263443\n",
      "Epoch[0], Val loss: 0.03673107177019119\n",
      "Epoch[0], Batch[4364], Train loss: 0.03680499270558357\n",
      "Epoch[0], Val loss: 0.03704798221588135\n",
      "Epoch[0], Batch[4365], Train loss: 0.03561898320913315\n",
      "Epoch[0], Val loss: 0.03678978234529495\n",
      "Epoch[0], Batch[4366], Train loss: 0.03716428205370903\n",
      "Epoch[0], Val loss: 0.03587303310632706\n",
      "Epoch[0], Batch[4367], Train loss: 0.0373900830745697\n",
      "Epoch[0], Val loss: 0.035931508988142014\n",
      "Epoch[0], Batch[4368], Train loss: 0.037500958889722824\n",
      "Epoch[0], Val loss: 0.03433100879192352\n",
      "Epoch[0], Batch[4369], Train loss: 0.037061262875795364\n",
      "Epoch[0], Val loss: 0.033746350556612015\n",
      "Epoch[0], Batch[4370], Train loss: 0.035439759492874146\n",
      "Epoch[0], Val loss: 0.03758443892002106\n",
      "Epoch[0], Batch[4371], Train loss: 0.0355633981525898\n",
      "Epoch[0], Val loss: 0.03497981280088425\n",
      "Epoch[0], Batch[4372], Train loss: 0.03586586192250252\n",
      "Epoch[0], Val loss: 0.0358300618827343\n",
      "Epoch[0], Batch[4373], Train loss: 0.03567473590373993\n",
      "Epoch[0], Val loss: 0.03884414955973625\n",
      "Epoch[0], Batch[4374], Train loss: 0.03944244235754013\n",
      "Epoch[0], Val loss: 0.03556601703166962\n",
      "Epoch[0], Batch[4375], Train loss: 0.03639662265777588\n",
      "Epoch[0], Val loss: 0.03581983968615532\n",
      "Epoch[0], Batch[4376], Train loss: 0.0385124534368515\n",
      "Epoch[0], Val loss: 0.03829716518521309\n",
      "Epoch[0], Batch[4377], Train loss: 0.03696822375059128\n",
      "Epoch[0], Val loss: 0.037966176867485046\n",
      "Epoch[0], Batch[4378], Train loss: 0.03755778446793556\n",
      "Epoch[0], Val loss: 0.03598857298493385\n",
      "Epoch[0], Batch[4379], Train loss: 0.038239605724811554\n",
      "Epoch[0], Val loss: 0.03621479496359825\n",
      "Epoch[0], Batch[4380], Train loss: 0.0379750095307827\n",
      "Epoch[0], Val loss: 0.0354636088013649\n",
      "Epoch[0], Batch[4381], Train loss: 0.03760594129562378\n",
      "Epoch[0], Val loss: 0.036708708852529526\n",
      "Epoch[0], Batch[4382], Train loss: 0.03831324726343155\n",
      "Epoch[0], Val loss: 0.03924744948744774\n",
      "Epoch[0], Batch[4383], Train loss: 0.03608830273151398\n",
      "Epoch[0], Val loss: 0.03730609640479088\n",
      "Epoch[0], Batch[4384], Train loss: 0.03629627078771591\n",
      "Epoch[0], Val loss: 0.03713436424732208\n",
      "Epoch[0], Batch[4385], Train loss: 0.03973118215799332\n",
      "Epoch[0], Val loss: 0.03339283913373947\n",
      "Epoch[0], Batch[4386], Train loss: 0.03712235763669014\n",
      "Epoch[0], Val loss: 0.03371186926960945\n",
      "Epoch[0], Batch[4387], Train loss: 0.036601852625608444\n",
      "Epoch[0], Val loss: 0.03554857522249222\n",
      "Epoch[0], Batch[4388], Train loss: 0.0366380512714386\n",
      "Epoch[0], Val loss: 0.03605173900723457\n",
      "Epoch[0], Batch[4389], Train loss: 0.03717238828539848\n",
      "Epoch[0], Val loss: 0.037564534693956375\n",
      "Epoch[0], Batch[4390], Train loss: 0.036699723452329636\n",
      "Epoch[0], Val loss: 0.03529674932360649\n",
      "Epoch[0], Batch[4391], Train loss: 0.03561931475996971\n",
      "Epoch[0], Val loss: 0.039504729211330414\n",
      "Epoch[0], Batch[4392], Train loss: 0.03675670176744461\n",
      "Epoch[0], Val loss: 0.03561360388994217\n",
      "Epoch[0], Batch[4393], Train loss: 0.03546952083706856\n",
      "Epoch[0], Val loss: 0.03418026491999626\n",
      "Epoch[0], Batch[4394], Train loss: 0.037171870470047\n",
      "Epoch[0], Val loss: 0.035024166107177734\n",
      "Epoch[0], Batch[4395], Train loss: 0.038596782833337784\n",
      "Epoch[0], Val loss: 0.03403858095407486\n",
      "Epoch[0], Batch[4396], Train loss: 0.03669122979044914\n",
      "Epoch[0], Val loss: 0.03520272672176361\n",
      "Epoch[0], Batch[4397], Train loss: 0.039668355137109756\n",
      "Epoch[0], Val loss: 0.03676483407616615\n",
      "Epoch[0], Batch[4398], Train loss: 0.03835625946521759\n",
      "Epoch[0], Val loss: 0.037739865481853485\n",
      "Epoch[0], Batch[4399], Train loss: 0.036473095417022705\n",
      "Epoch[0], Val loss: 0.03600405901670456\n",
      "Epoch[0], Batch[4400], Train loss: 0.03642374649643898\n",
      "Epoch[0], Val loss: 0.036694299429655075\n",
      "Epoch[0], Batch[4401], Train loss: 0.03809543699026108\n",
      "Epoch[0], Val loss: 0.034129586070775986\n",
      "Epoch[0], Batch[4402], Train loss: 0.03587578982114792\n",
      "Epoch[0], Val loss: 0.03617621585726738\n",
      "Epoch[0], Batch[4403], Train loss: 0.03597404807806015\n",
      "Epoch[0], Val loss: 0.03613540157675743\n",
      "Epoch[0], Batch[4404], Train loss: 0.038417380303144455\n",
      "Epoch[0], Val loss: 0.03504233434796333\n",
      "Epoch[0], Batch[4405], Train loss: 0.03656837344169617\n",
      "Epoch[0], Val loss: 0.037803974002599716\n",
      "Epoch[0], Batch[4406], Train loss: 0.036326486617326736\n",
      "Epoch[0], Val loss: 0.0395461730659008\n",
      "Epoch[0], Batch[4407], Train loss: 0.03953200578689575\n",
      "Epoch[0], Val loss: 0.03626256808638573\n",
      "Epoch[0], Batch[4408], Train loss: 0.039190731942653656\n",
      "Epoch[0], Val loss: 0.03421517461538315\n",
      "Epoch[0], Batch[4409], Train loss: 0.03985367342829704\n",
      "Epoch[0], Val loss: 0.03552667424082756\n",
      "Epoch[0], Batch[4410], Train loss: 0.03695081174373627\n",
      "Epoch[0], Val loss: 0.03617583587765694\n",
      "Epoch[0], Batch[4411], Train loss: 0.03606141358613968\n",
      "Epoch[0], Val loss: 0.03503101319074631\n",
      "Epoch[0], Batch[4412], Train loss: 0.03796957805752754\n",
      "Epoch[0], Val loss: 0.0376293919980526\n",
      "Epoch[0], Batch[4413], Train loss: 0.038352109491825104\n",
      "Epoch[0], Val loss: 0.036811310797929764\n",
      "Epoch[0], Batch[4414], Train loss: 0.03585103154182434\n",
      "Epoch[0], Val loss: 0.036089878529310226\n",
      "Epoch[0], Batch[4415], Train loss: 0.03829595819115639\n",
      "Epoch[0], Val loss: 0.03580088168382645\n",
      "Epoch[0], Batch[4416], Train loss: 0.03765536844730377\n",
      "Epoch[0], Val loss: 0.03621216118335724\n",
      "Epoch[0], Batch[4417], Train loss: 0.03778679668903351\n",
      "Epoch[0], Val loss: 0.03717007488012314\n",
      "Epoch[0], Batch[4418], Train loss: 0.03809911385178566\n",
      "Epoch[0], Val loss: 0.03959336876869202\n",
      "Epoch[0], Batch[4419], Train loss: 0.04121549427509308\n",
      "Epoch[0], Val loss: 0.0379180833697319\n",
      "Epoch[0], Batch[4420], Train loss: 0.03626424819231033\n",
      "Epoch[0], Val loss: 0.03571067377924919\n",
      "Epoch[0], Batch[4421], Train loss: 0.03555932268500328\n",
      "Epoch[0], Val loss: 0.036569658666849136\n",
      "Epoch[0], Batch[4422], Train loss: 0.03790567070245743\n",
      "Epoch[0], Val loss: 0.0369589626789093\n",
      "Epoch[0], Batch[4423], Train loss: 0.03823944926261902\n",
      "Epoch[0], Val loss: 0.03691649064421654\n",
      "Epoch[0], Batch[4424], Train loss: 0.037004292011260986\n",
      "Epoch[0], Val loss: 0.03535304218530655\n",
      "Epoch[0], Batch[4425], Train loss: 0.03552555665373802\n",
      "Epoch[0], Val loss: 0.03614114597439766\n",
      "Epoch[0], Batch[4426], Train loss: 0.03669852763414383\n",
      "Epoch[0], Val loss: 0.03717423230409622\n",
      "Epoch[0], Batch[4427], Train loss: 0.03829113394021988\n",
      "Epoch[0], Val loss: 0.03393353894352913\n",
      "Epoch[0], Batch[4428], Train loss: 0.03779897838830948\n",
      "Epoch[0], Val loss: 0.03609885275363922\n",
      "Epoch[0], Batch[4429], Train loss: 0.03763937950134277\n",
      "Epoch[0], Val loss: 0.03504698723554611\n",
      "Epoch[0], Batch[4430], Train loss: 0.0389045812189579\n",
      "Epoch[0], Val loss: 0.03595268726348877\n",
      "Epoch[0], Batch[4431], Train loss: 0.03861584514379501\n",
      "Epoch[0], Val loss: 0.03764806315302849\n",
      "Epoch[0], Batch[4432], Train loss: 0.03624064847826958\n",
      "Epoch[0], Val loss: 0.03569885343313217\n",
      "Epoch[0], Batch[4433], Train loss: 0.03762442618608475\n",
      "Epoch[0], Val loss: 0.03671444579958916\n",
      "Epoch[0], Batch[4434], Train loss: 0.04110538959503174\n",
      "Epoch[0], Val loss: 0.03630765154957771\n",
      "Epoch[0], Batch[4435], Train loss: 0.0369473397731781\n",
      "Epoch[0], Val loss: 0.03660092130303383\n",
      "Epoch[0], Batch[4436], Train loss: 0.037372637540102005\n",
      "Epoch[0], Val loss: 0.035317495465278625\n",
      "Epoch[0], Batch[4437], Train loss: 0.035629063844680786\n",
      "Epoch[0], Val loss: 0.0346718356013298\n",
      "Epoch[0], Batch[4438], Train loss: 0.03805039823055267\n",
      "Epoch[0], Val loss: 0.03492952883243561\n",
      "Epoch[0], Batch[4439], Train loss: 0.036839358508586884\n",
      "Epoch[0], Val loss: 0.03878949210047722\n",
      "Epoch[0], Batch[4440], Train loss: 0.03726140409708023\n",
      "Epoch[0], Val loss: 0.03585238382220268\n",
      "Epoch[0], Batch[4441], Train loss: 0.03526507690548897\n",
      "Epoch[0], Val loss: 0.03759065270423889\n",
      "Epoch[0], Batch[4442], Train loss: 0.036464519798755646\n",
      "Epoch[0], Val loss: 0.03636990487575531\n",
      "Epoch[0], Batch[4443], Train loss: 0.0400431826710701\n",
      "Epoch[0], Val loss: 0.03486089035868645\n",
      "Epoch[0], Batch[4444], Train loss: 0.03836556524038315\n",
      "Epoch[0], Val loss: 0.03653835877776146\n",
      "Epoch[0], Batch[4445], Train loss: 0.035059522837400436\n",
      "Epoch[0], Val loss: 0.03691608086228371\n",
      "Epoch[0], Batch[4446], Train loss: 0.03658142313361168\n",
      "Epoch[0], Val loss: 0.03568890318274498\n",
      "Epoch[0], Batch[4447], Train loss: 0.03762807697057724\n",
      "Epoch[0], Val loss: 0.036283303052186966\n",
      "Epoch[0], Batch[4448], Train loss: 0.03448301926255226\n",
      "Epoch[0], Val loss: 0.03725822642445564\n",
      "Epoch[0], Batch[4449], Train loss: 0.035114552825689316\n",
      "Epoch[0], Val loss: 0.03663019463419914\n",
      "Epoch[0], Batch[4450], Train loss: 0.03883162885904312\n",
      "Epoch[0], Val loss: 0.038082100450992584\n",
      "Epoch[0], Batch[4451], Train loss: 0.03686082363128662\n",
      "Epoch[0], Val loss: 0.03580940142273903\n",
      "Epoch[0], Batch[4452], Train loss: 0.03760902211070061\n",
      "Epoch[0], Val loss: 0.036749232560396194\n",
      "Epoch[0], Batch[4453], Train loss: 0.038461778312921524\n",
      "Epoch[0], Val loss: 0.035247452557086945\n",
      "Epoch[0], Batch[4454], Train loss: 0.03656907379627228\n",
      "Epoch[0], Val loss: 0.036163151264190674\n",
      "Epoch[0], Batch[4455], Train loss: 0.03896021097898483\n",
      "Epoch[0], Val loss: 0.03606181591749191\n",
      "Epoch[0], Batch[4456], Train loss: 0.03839155286550522\n",
      "Epoch[0], Val loss: 0.03667624294757843\n",
      "Epoch[0], Batch[4457], Train loss: 0.03999658301472664\n",
      "Epoch[0], Val loss: 0.03595433756709099\n",
      "Epoch[0], Batch[4458], Train loss: 0.038683246821165085\n",
      "Epoch[0], Val loss: 0.03717479854822159\n",
      "Epoch[0], Batch[4459], Train loss: 0.03770197182893753\n",
      "Epoch[0], Val loss: 0.03559165075421333\n",
      "Epoch[0], Batch[4460], Train loss: 0.037681695073843\n",
      "Epoch[0], Val loss: 0.035264018923044205\n",
      "Epoch[0], Batch[4461], Train loss: 0.03760843351483345\n",
      "Epoch[0], Val loss: 0.03659042343497276\n",
      "Epoch[0], Batch[4462], Train loss: 0.037014689296483994\n",
      "Epoch[0], Val loss: 0.038267582654953\n",
      "Epoch[0], Batch[4463], Train loss: 0.03565873205661774\n",
      "Epoch[0], Val loss: 0.04007503762841225\n",
      "Epoch[0], Batch[4464], Train loss: 0.035420678555965424\n",
      "Epoch[0], Val loss: 0.033518098294734955\n",
      "Epoch[0], Batch[4465], Train loss: 0.037154268473386765\n",
      "Epoch[0], Val loss: 0.03692125156521797\n",
      "Epoch[0], Batch[4466], Train loss: 0.03984811156988144\n",
      "Epoch[0], Val loss: 0.0368262343108654\n",
      "Epoch[0], Batch[4467], Train loss: 0.03860681876540184\n",
      "Epoch[0], Val loss: 0.03781849145889282\n",
      "Epoch[0], Batch[4468], Train loss: 0.039034776389598846\n",
      "Epoch[0], Val loss: 0.04058525338768959\n",
      "Epoch[0], Batch[4469], Train loss: 0.03967727720737457\n",
      "Epoch[0], Val loss: 0.03709107264876366\n",
      "Epoch[0], Batch[4470], Train loss: 0.0361686609685421\n",
      "Epoch[0], Val loss: 0.03971981257200241\n",
      "Epoch[0], Batch[4471], Train loss: 0.0369618758559227\n",
      "Epoch[0], Val loss: 0.03799094259738922\n",
      "Epoch[0], Batch[4472], Train loss: 0.03966187685728073\n",
      "Epoch[0], Val loss: 0.037985071539878845\n",
      "Epoch[0], Batch[4473], Train loss: 0.03536391258239746\n",
      "Epoch[0], Val loss: 0.037706851959228516\n",
      "Epoch[0], Batch[4474], Train loss: 0.038499508053064346\n",
      "Epoch[0], Val loss: 0.034792788326740265\n",
      "Epoch[0], Batch[4475], Train loss: 0.03806941583752632\n",
      "Epoch[0], Val loss: 0.03278825432062149\n",
      "Epoch[0], Batch[4476], Train loss: 0.03761555254459381\n",
      "Epoch[0], Val loss: 0.03669167309999466\n",
      "Epoch[0], Batch[4477], Train loss: 0.040675122290849686\n",
      "Epoch[0], Val loss: 0.03475413843989372\n",
      "Epoch[0], Batch[4478], Train loss: 0.03604007512331009\n",
      "Epoch[0], Val loss: 0.03511713445186615\n",
      "Epoch[0], Batch[4479], Train loss: 0.036803387105464935\n",
      "Epoch[0], Val loss: 0.03700314462184906\n",
      "Epoch[0], Batch[4480], Train loss: 0.03711684048175812\n",
      "Epoch[0], Val loss: 0.03692863509058952\n",
      "Epoch[0], Batch[4481], Train loss: 0.03795015811920166\n",
      "Epoch[0], Val loss: 0.0344729982316494\n",
      "Epoch[0], Batch[4482], Train loss: 0.03776808828115463\n",
      "Epoch[0], Val loss: 0.03653929382562637\n",
      "Epoch[0], Batch[4483], Train loss: 0.03584916517138481\n",
      "Epoch[0], Val loss: 0.03543821722269058\n",
      "Epoch[0], Batch[4484], Train loss: 0.037241626530885696\n",
      "Epoch[0], Val loss: 0.03249302878975868\n",
      "Epoch[0], Batch[4485], Train loss: 0.0372798889875412\n",
      "Epoch[0], Val loss: 0.0358160063624382\n",
      "Epoch[0], Batch[4486], Train loss: 0.035769473761320114\n",
      "Epoch[0], Val loss: 0.03650211915373802\n",
      "Epoch[0], Batch[4487], Train loss: 0.035893987864255905\n",
      "Epoch[0], Val loss: 0.03681217133998871\n",
      "Epoch[0], Batch[4488], Train loss: 0.03869161754846573\n",
      "Epoch[0], Val loss: 0.037161633372306824\n",
      "Epoch[0], Batch[4489], Train loss: 0.03911525756120682\n",
      "Epoch[0], Val loss: 0.03670356050133705\n",
      "Epoch[0], Batch[4490], Train loss: 0.036760613322257996\n",
      "Epoch[0], Val loss: 0.03542907536029816\n",
      "Epoch[0], Batch[4491], Train loss: 0.035778213292360306\n",
      "Epoch[0], Val loss: 0.0356927290558815\n",
      "Epoch[0], Batch[4492], Train loss: 0.03843280300498009\n",
      "Epoch[0], Val loss: 0.03468639403581619\n",
      "Epoch[0], Batch[4493], Train loss: 0.03684836998581886\n",
      "Epoch[0], Val loss: 0.036108970642089844\n",
      "Epoch[0], Batch[4494], Train loss: 0.03652269393205643\n",
      "Epoch[0], Val loss: 0.033852800726890564\n",
      "Epoch[0], Batch[4495], Train loss: 0.03639226406812668\n",
      "Epoch[0], Val loss: 0.035579025745391846\n",
      "Epoch[0], Batch[4496], Train loss: 0.037823211401700974\n",
      "Epoch[0], Val loss: 0.03875308483839035\n",
      "Epoch[0], Batch[4497], Train loss: 0.03849377483129501\n",
      "Epoch[0], Val loss: 0.03725024685263634\n",
      "Epoch[0], Batch[4498], Train loss: 0.037829164415597916\n",
      "Epoch[0], Val loss: 0.03528117761015892\n",
      "Epoch[0], Batch[4499], Train loss: 0.03723748400807381\n",
      "Epoch[0], Val loss: 0.03581984341144562\n",
      "Epoch[0], Batch[4500], Train loss: 0.036190152168273926\n",
      "Epoch[0], Val loss: 0.03505279868841171\n",
      "Epoch[0], Batch[4501], Train loss: 0.03685501217842102\n",
      "Epoch[0], Val loss: 0.03346971049904823\n",
      "Epoch[0], Batch[4502], Train loss: 0.036899685859680176\n",
      "Epoch[0], Val loss: 0.036711521446704865\n",
      "Epoch[0], Batch[4503], Train loss: 0.03753035515546799\n",
      "Epoch[0], Val loss: 0.03836994245648384\n",
      "Epoch[0], Batch[4504], Train loss: 0.03588375076651573\n",
      "Epoch[0], Val loss: 0.03644948452711105\n",
      "Epoch[0], Batch[4505], Train loss: 0.035499539226293564\n",
      "Epoch[0], Val loss: 0.038058120757341385\n",
      "Epoch[0], Batch[4506], Train loss: 0.03645607456564903\n",
      "Epoch[0], Val loss: 0.034078530967235565\n",
      "Epoch[0], Batch[4507], Train loss: 0.03795641288161278\n",
      "Epoch[0], Val loss: 0.03490537777543068\n",
      "Epoch[0], Batch[4508], Train loss: 0.0353793129324913\n",
      "Epoch[0], Val loss: 0.03645865619182587\n",
      "Epoch[0], Batch[4509], Train loss: 0.03859379142522812\n",
      "Epoch[0], Val loss: 0.037072475999593735\n",
      "Epoch[0], Batch[4510], Train loss: 0.035256702452898026\n",
      "Epoch[0], Val loss: 0.03671393543481827\n",
      "Epoch[0], Batch[4511], Train loss: 0.037307750433683395\n",
      "Epoch[0], Val loss: 0.03689011558890343\n",
      "Epoch[0], Batch[4512], Train loss: 0.03531546890735626\n",
      "Epoch[0], Val loss: 0.036576446145772934\n",
      "Epoch[0], Batch[4513], Train loss: 0.03816809877753258\n",
      "Epoch[0], Val loss: 0.03716933727264404\n",
      "Epoch[0], Batch[4514], Train loss: 0.03793271258473396\n",
      "Epoch[0], Val loss: 0.034163035452365875\n",
      "Epoch[0], Batch[4515], Train loss: 0.03678034991025925\n",
      "Epoch[0], Val loss: 0.036505937576293945\n",
      "Epoch[0], Batch[4516], Train loss: 0.03444335609674454\n",
      "Epoch[0], Val loss: 0.034110452979803085\n",
      "Epoch[0], Batch[4517], Train loss: 0.0366641990840435\n",
      "Epoch[0], Val loss: 0.03681337088346481\n",
      "Epoch[0], Batch[4518], Train loss: 0.03573330491781235\n",
      "Epoch[0], Val loss: 0.03538942337036133\n",
      "Epoch[0], Batch[4519], Train loss: 0.040507085621356964\n",
      "Epoch[0], Val loss: 0.035291723906993866\n",
      "Epoch[0], Batch[4520], Train loss: 0.03810974583029747\n",
      "Epoch[0], Val loss: 0.03329206258058548\n",
      "Epoch[0], Batch[4521], Train loss: 0.03713011369109154\n",
      "Epoch[0], Val loss: 0.03618831932544708\n",
      "Epoch[0], Batch[4522], Train loss: 0.03724350407719612\n",
      "Epoch[0], Val loss: 0.035446859896183014\n",
      "Epoch[0], Batch[4523], Train loss: 0.03784918785095215\n",
      "Epoch[0], Val loss: 0.03716222196817398\n",
      "Epoch[0], Batch[4524], Train loss: 0.0338253527879715\n",
      "Epoch[0], Val loss: 0.03864804282784462\n",
      "Epoch[0], Batch[4525], Train loss: 0.03874070197343826\n",
      "Epoch[0], Val loss: 0.03703413903713226\n",
      "Epoch[0], Batch[4526], Train loss: 0.038121797144412994\n",
      "Epoch[0], Val loss: 0.03788619488477707\n",
      "Epoch[0], Batch[4527], Train loss: 0.0359976626932621\n",
      "Epoch[0], Val loss: 0.03731107711791992\n",
      "Epoch[0], Batch[4528], Train loss: 0.03873685002326965\n",
      "Epoch[0], Val loss: 0.035411667078733444\n",
      "Epoch[0], Batch[4529], Train loss: 0.03615463152527809\n",
      "Epoch[0], Val loss: 0.036189671605825424\n",
      "Epoch[0], Batch[4530], Train loss: 0.0365731455385685\n",
      "Epoch[0], Val loss: 0.03550494834780693\n",
      "Epoch[0], Batch[4531], Train loss: 0.035918641835451126\n",
      "Epoch[0], Val loss: 0.03523626923561096\n",
      "Epoch[0], Batch[4532], Train loss: 0.0385972261428833\n",
      "Epoch[0], Val loss: 0.033727459609508514\n",
      "Epoch[0], Batch[4533], Train loss: 0.036175381392240524\n",
      "Epoch[0], Val loss: 0.03646612912416458\n",
      "Epoch[0], Batch[4534], Train loss: 0.03622160851955414\n",
      "Epoch[0], Val loss: 0.03400177136063576\n",
      "Epoch[0], Batch[4535], Train loss: 0.03687493875622749\n",
      "Epoch[0], Val loss: 0.03709070757031441\n",
      "Epoch[0], Batch[4536], Train loss: 0.03706413879990578\n",
      "Epoch[0], Val loss: 0.0344768650829792\n",
      "Epoch[0], Batch[4537], Train loss: 0.03490642085671425\n",
      "Epoch[0], Val loss: 0.0347847081720829\n",
      "Epoch[0], Batch[4538], Train loss: 0.03816123306751251\n",
      "Epoch[0], Val loss: 0.03574806824326515\n",
      "Epoch[0], Batch[4539], Train loss: 0.03458268567919731\n",
      "Epoch[0], Val loss: 0.03490001708269119\n",
      "Epoch[0], Batch[4540], Train loss: 0.03637254983186722\n",
      "Epoch[0], Val loss: 0.036635883152484894\n",
      "Epoch[0], Batch[4541], Train loss: 0.0377991609275341\n",
      "Epoch[0], Val loss: 0.033765263855457306\n",
      "Epoch[0], Batch[4542], Train loss: 0.038105908781290054\n",
      "Epoch[0], Val loss: 0.03678553178906441\n",
      "Epoch[0], Batch[4543], Train loss: 0.03646593540906906\n",
      "Epoch[0], Val loss: 0.036955058574676514\n",
      "Epoch[0], Batch[4544], Train loss: 0.03568068519234657\n",
      "Epoch[0], Val loss: 0.03302140161395073\n",
      "Epoch[0], Batch[4545], Train loss: 0.038459762930870056\n",
      "Epoch[0], Val loss: 0.03671826049685478\n",
      "Epoch[0], Batch[4546], Train loss: 0.03735538572072983\n",
      "Epoch[0], Val loss: 0.03596392273902893\n",
      "Epoch[0], Batch[4547], Train loss: 0.03423688933253288\n",
      "Epoch[0], Val loss: 0.0360286645591259\n",
      "Epoch[0], Batch[4548], Train loss: 0.037434011697769165\n",
      "Epoch[0], Val loss: 0.03548894077539444\n",
      "Epoch[0], Batch[4549], Train loss: 0.038808371871709824\n",
      "Epoch[0], Val loss: 0.03779109567403793\n",
      "Epoch[0], Batch[4550], Train loss: 0.035987745970487595\n",
      "Epoch[0], Val loss: 0.03568268567323685\n",
      "Epoch[0], Batch[4551], Train loss: 0.03918052092194557\n",
      "Epoch[0], Val loss: 0.03680921718478203\n",
      "Epoch[0], Batch[4552], Train loss: 0.036215510219335556\n",
      "Epoch[0], Val loss: 0.03525741770863533\n",
      "Epoch[0], Batch[4553], Train loss: 0.0370599739253521\n",
      "Epoch[0], Val loss: 0.03907803073525429\n",
      "Epoch[0], Batch[4554], Train loss: 0.03681715205311775\n",
      "Epoch[0], Val loss: 0.03702947869896889\n",
      "Epoch[0], Batch[4555], Train loss: 0.037503160536289215\n",
      "Epoch[0], Val loss: 0.03470062464475632\n",
      "Epoch[0], Batch[4556], Train loss: 0.03792840987443924\n",
      "Epoch[0], Val loss: 0.033545996993780136\n",
      "Epoch[0], Batch[4557], Train loss: 0.037378132343292236\n",
      "Epoch[0], Val loss: 0.03352876007556915\n",
      "Epoch[0], Batch[4558], Train loss: 0.036267027258872986\n",
      "Epoch[0], Val loss: 0.036933042109012604\n",
      "Epoch[0], Batch[4559], Train loss: 0.036447372287511826\n",
      "Epoch[0], Val loss: 0.03482205048203468\n",
      "Epoch[0], Batch[4560], Train loss: 0.03744668886065483\n",
      "Epoch[0], Val loss: 0.03815489262342453\n",
      "Epoch[0], Batch[4561], Train loss: 0.03532131761312485\n",
      "Epoch[0], Val loss: 0.03735044226050377\n",
      "Epoch[0], Batch[4562], Train loss: 0.037860672920942307\n",
      "Epoch[0], Val loss: 0.03550641983747482\n",
      "Epoch[0], Batch[4563], Train loss: 0.03532988578081131\n",
      "Epoch[0], Val loss: 0.03586810827255249\n",
      "Epoch[0], Batch[4564], Train loss: 0.03639885410666466\n",
      "Epoch[0], Val loss: 0.033782023936510086\n",
      "Epoch[0], Batch[4565], Train loss: 0.03591747209429741\n",
      "Epoch[0], Val loss: 0.035900723189115524\n",
      "Epoch[0], Batch[4566], Train loss: 0.03758019953966141\n",
      "Epoch[0], Val loss: 0.03842764347791672\n",
      "Epoch[0], Batch[4567], Train loss: 0.03522537276148796\n",
      "Epoch[0], Val loss: 0.035654544830322266\n",
      "Epoch[0], Batch[4568], Train loss: 0.0374210961163044\n",
      "Epoch[0], Val loss: 0.03472805768251419\n",
      "Epoch[0], Batch[4569], Train loss: 0.03704860061407089\n",
      "Epoch[0], Val loss: 0.035340260714292526\n",
      "Epoch[0], Batch[4570], Train loss: 0.038615427911281586\n",
      "Epoch[0], Val loss: 0.0354086309671402\n",
      "Epoch[0], Batch[4571], Train loss: 0.0350346714258194\n",
      "Epoch[0], Val loss: 0.03573889285326004\n",
      "Epoch[0], Batch[4572], Train loss: 0.03706500679254532\n",
      "Epoch[0], Val loss: 0.03551075607538223\n",
      "Epoch[0], Batch[4573], Train loss: 0.036135829985141754\n",
      "Epoch[0], Val loss: 0.03732656314969063\n",
      "Epoch[0], Batch[4574], Train loss: 0.03648557513952255\n",
      "Epoch[0], Val loss: 0.035366903990507126\n",
      "Epoch[0], Batch[4575], Train loss: 0.037155844271183014\n",
      "Epoch[0], Val loss: 0.03575415164232254\n",
      "Epoch[0], Batch[4576], Train loss: 0.0344928540289402\n",
      "Epoch[0], Val loss: 0.036331646144390106\n",
      "Epoch[0], Batch[4577], Train loss: 0.03686992824077606\n",
      "Epoch[0], Val loss: 0.035277292132377625\n",
      "Epoch[0], Batch[4578], Train loss: 0.037686917930841446\n",
      "Epoch[0], Val loss: 0.03738792613148689\n",
      "Epoch[0], Batch[4579], Train loss: 0.037499163299798965\n",
      "Epoch[0], Val loss: 0.0357535257935524\n",
      "Epoch[0], Batch[4580], Train loss: 0.035954397171735764\n",
      "Epoch[0], Val loss: 0.03460429236292839\n",
      "Epoch[0], Batch[4581], Train loss: 0.03911546245217323\n",
      "Epoch[0], Val loss: 0.03506802022457123\n",
      "Epoch[0], Batch[4582], Train loss: 0.0387708805501461\n",
      "Epoch[0], Val loss: 0.03521786257624626\n",
      "Epoch[0], Batch[4583], Train loss: 0.03768698126077652\n",
      "Epoch[0], Val loss: 0.03481646254658699\n",
      "Epoch[0], Batch[4584], Train loss: 0.03777065500617027\n",
      "Epoch[0], Val loss: 0.03405381739139557\n",
      "Epoch[0], Batch[4585], Train loss: 0.03596080094575882\n",
      "Epoch[0], Val loss: 0.036003198474645615\n",
      "Epoch[0], Batch[4586], Train loss: 0.03593308851122856\n",
      "Epoch[0], Val loss: 0.03536669909954071\n",
      "Epoch[0], Batch[4587], Train loss: 0.03700784221291542\n",
      "Epoch[0], Val loss: 0.0360846184194088\n",
      "Epoch[0], Batch[4588], Train loss: 0.0371144637465477\n",
      "Epoch[0], Val loss: 0.03404083847999573\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/siddharth/DL-A3/dl-a3.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output[:,:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], encoded_tgt[:,\u001b[39m1\u001b[39m:])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer_decoder\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer_encoder\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/DLA3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/DLA3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(4):\n",
    "  batch = 1\n",
    "  for data, vis in train_loader:\n",
    "    # Training Loop\n",
    "      encoder.train()\n",
    "      decoder.train()\n",
    "      optimizer_encoder.zero_grad()\n",
    "      optimizer_decoder.zero_grad()\n",
    "      \n",
    "      context = encoder(data)\n",
    "      output, encoded_tgt = decoder(context, vis, 1.0)\n",
    "      output = output.permute(0,2,1)\n",
    "      loss = criterion(output[:,:,:-1], encoded_tgt[:,1:])\n",
    "      \n",
    "      loss.backward()\n",
    "      optimizer_decoder.step()\n",
    "      optimizer_encoder.step()\n",
    "      \n",
    "      print(f\"Epoch[{epoch}], Batch[{batch}], Train loss: {loss.item()}\")\n",
    "        \n",
    "    # Validation (with one minibatch only)\n",
    "      with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        for data, vis in eval_loader:\n",
    "          context_ = encoder(data)\n",
    "          output_, encoded_tgt_ = decoder(context_,vis,1.0)\n",
    "          output_ = output_.permute(0,2,1)\n",
    "          loss_val = criterion(output_[:,:,:-1], encoded_tgt_[:,1:])\n",
    "          print(f\"Epoch[{epoch}], Val loss: {loss_val.item()}\")\n",
    "        \n",
    "          checkpoint_encoder = {\n",
    "                'model_state_dict': encoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer_encoder.state_dict(),\n",
    "          }\n",
    "          checkpoint_decoder = {\n",
    "                'model_state_dict': decoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer_decoder.state_dict(),\n",
    "          }\n",
    "          \n",
    "          if loss_val < best_val_loss:\n",
    "              best_val_loss = loss_val\n",
    "              epochs_since_improvement = 0\n",
    "              torch.save(checkpoint_encoder, 'Model/encoder.pth')\n",
    "              torch.save(checkpoint_decoder, 'Model/decoder.pth')\n",
    "          else:\n",
    "              epochs_since_improvement += 1\n",
    "          break\n",
    "\n",
    "      # Check if we should stop training early\n",
    "      if epochs_since_improvement >= patience:\n",
    "        print(f\"Early stopping after {epoch} epochs , {batch} batches with no improvement.\")\n",
    "        break\n",
    "      batch += 1\n",
    "      \n",
    "  if loss_val < best_val_loss:\n",
    "      torch.save(checkpoint_encoder, 'Model/encoder.pth')\n",
    "      torch.save(checkpoint_decoder, 'Model/decoder.pth')\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:27:07.995271Z",
     "iopub.status.busy": "2023-11-13T04:27:07.994287Z",
     "iopub.status.idle": "2023-11-13T04:27:08.200504Z",
     "shell.execute_reply": "2023-11-13T04:27:08.199369Z",
     "shell.execute_reply.started": "2023-11-13T04:27:07.995219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [1], loss: 0.03648332133889198\n",
      "Batch [2], loss: 0.03610321879386902\n",
      "Batch [3], loss: 0.037116486579179764\n",
      "Batch [4], loss: 0.03736161068081856\n",
      "Batch [5], loss: 0.03656899183988571\n",
      "Batch [6], loss: 0.038916781544685364\n",
      "Batch [7], loss: 0.03733036667108536\n",
      "Batch [8], loss: 0.0360385924577713\n",
      "Batch [9], loss: 0.03690795600414276\n",
      "Batch [10], loss: 0.03645802661776543\n",
      "Batch [11], loss: 0.035165682435035706\n",
      "Batch [12], loss: 0.03644541651010513\n",
      "Batch [13], loss: 0.03623410314321518\n",
      "Batch [14], loss: 0.03796388581395149\n",
      "Batch [15], loss: 0.03917788714170456\n",
      "Batch [16], loss: 0.033612918108701706\n",
      "Batch [17], loss: 0.03582746163010597\n",
      "Batch [18], loss: 0.03809986263513565\n",
      "Batch [19], loss: 0.03623787313699722\n",
      "Batch [20], loss: 0.03773220628499985\n",
      "Batch [21], loss: 0.03618108853697777\n",
      "Batch [22], loss: 0.0351506844162941\n",
      "Batch [23], loss: 0.03792411461472511\n",
      "Batch [24], loss: 0.03622032701969147\n",
      "Batch [25], loss: 0.03626759722828865\n",
      "Batch [26], loss: 0.03695521131157875\n",
      "Batch [27], loss: 0.03529930114746094\n",
      "Batch [28], loss: 0.033899955451488495\n",
      "Batch [29], loss: 0.0356367863714695\n",
      "Batch [30], loss: 0.0346749871969223\n",
      "Batch [31], loss: 0.03514653816819191\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/siddharth/DL-A3/dl-a3.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m data, vis \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m   context_ \u001b[39m=\u001b[39m encoder_(data)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m   output_, encoded_tgt_ \u001b[39m=\u001b[39m decoder_(context_,vis,\u001b[39m1.0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m   output_ \u001b[39m=\u001b[39m output_\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m   loss_test \u001b[39m=\u001b[39m criterion(output_[:,:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], encoded_tgt_[:,\u001b[39m1\u001b[39m:])\n",
      "File \u001b[0;32m~/miniconda3/envs/DLA3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/DLA3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/siddharth/DL-A3/dl-a3.ipynb Cell 12\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(torch\u001b[39m.\u001b[39margmax(nn\u001b[39m.\u001b[39mSoftmax(dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)(outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:,\u001b[39m0\u001b[39m]),dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     (h_t1, c_t1) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdec_cells[\u001b[39m0\u001b[39;49m](\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdrop(torch\u001b[39m.\u001b[39;49mcat((\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalcontext(timestep,query[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])),dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)), (hidden_states[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m0\u001b[39;49m], cell_states[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m0\u001b[39;49m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     (h_t2, c_t2) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec_cells[\u001b[39m1\u001b[39m](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(h_t1), (hidden_states[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m], cell_states[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m hidden_states\u001b[39m.\u001b[39mappend([h_t1, h_t2])\n",
      "File \u001b[0;32m~/miniconda3/envs/DLA3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/DLA3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DLA3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1347\u001b[0m, in \u001b[0;36mLSTMCell.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     hx \u001b[39m=\u001b[39m (hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched \u001b[39melse\u001b[39;00m hx\n\u001b[0;32m-> 1347\u001b[0m ret \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm_cell(\n\u001b[1;32m   1348\u001b[0m     \u001b[39minput\u001b[39;49m, hx,\n\u001b[1;32m   1349\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_hh,\n\u001b[1;32m   1350\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_hh,\n\u001b[1;32m   1351\u001b[0m )\n\u001b[1;32m   1353\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched:\n\u001b[1;32m   1354\u001b[0m     ret \u001b[39m=\u001b[39m (ret[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m), ret[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder_ = Encoder().to(device)\n",
    "decoder_ = Decoder().to(device)\n",
    "checkpoint_enc = torch.load('Model/encoder.pth',map_location=torch.device('cpu'))\n",
    "checkpoint_dec = torch.load('Model/decoder.pth',map_location=torch.device('cpu'))\n",
    "encoder_.load_state_dict(checkpoint_enc['model_state_dict'])\n",
    "decoder_.load_state_dict(checkpoint_dec['model_state_dict'])\n",
    "encoder_.eval()\n",
    "decoder_.eval()\n",
    "# optimizer_encoder = optim.Adam(encoder_.parameters(), lr=0.0001)\n",
    "# optimizer_decoder = optim.Adam(decoder_.parameters(), lr=0.0001)\n",
    "optimizer_encoder.load_state_dict(checkpoint_enc['optimizer_state_dict'])\n",
    "optimizer_decoder.load_state_dict(checkpoint_dec['optimizer_state_dict'])\n",
    "\n",
    "criterion = nn.NLLLoss(ignore_index = 46)\n",
    "\n",
    "# with torch.no_grad():\n",
    "total_test = 0.0\n",
    "batch = 1\n",
    "for data, vis in train_loader:\n",
    "  context_ = encoder_(data)\n",
    "  output_, encoded_tgt_ = decoder_(context_,vis,1.0)\n",
    "  output_ = output_.permute(0,2,1)\n",
    "  loss_test = criterion(output_[:,:,:-1], encoded_tgt_[:,1:])\n",
    "  total_test += loss_test.item()\n",
    "  print(f\"Batch [{batch}], loss: {loss_test.item()}\")\n",
    "  batch += 1\n",
    "print(f\"Average log perplexity on the test set: {total_test / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, context):\n",
    "    model.context = context\n",
    "    start = torch.zeros(1,1, dtype = int).to(device) + 44\n",
    "    target_seq = model.embedding(start)\n",
    "    \n",
    "    initial_hidden1 = torch.rand(target_seq.shape[0], model.hidden_size).to(device)\n",
    "    initial_cell1 = torch.rand(target_seq.shape[0], model.hidden_size).to(device)\n",
    "    initial_hidden2 = torch.rand(target_seq.shape[0], model.hidden_size).to(device)\n",
    "    initial_cell2 = torch.rand(target_seq.shape[0], model.hidden_size).to(device)\n",
    "    \n",
    "    outputs = []\n",
    "    hidden_states = []\n",
    "    cell_states = []\n",
    "    query = [initial_hidden2]\n",
    "    for timestep in range(model.max_tgt+1):\n",
    "        if(timestep == 0):\n",
    "            (h_t1, c_t1) = model.dec_cells[0](model.drop(torch.cat((target_seq[:,timestep],model.calcontext(0,query[-1])),dim=1)), (initial_hidden1, initial_cell1))\n",
    "            (h_t2, c_t2) = model.dec_cells[1](model.drop(h_t1), (initial_hidden2, initial_cell2))\n",
    "        else:\n",
    "            input = model.embedding(torch.argmax(nn.Softmax(dim = 1)(outputs[-1][:,0]),dim=1))\n",
    "                \n",
    "            (h_t1, c_t1) = model.dec_cells[0](model.drop(torch.cat((input,model.calcontext(timestep,query[-1])),dim=1)), (hidden_states[-1][0], cell_states[-1][0]))\n",
    "            (h_t2, c_t2) = model.dec_cells[1](model.drop(h_t1), (hidden_states[-1][1], cell_states[-1][1]))\n",
    "        hidden_states.append([h_t1, h_t2])\n",
    "        cell_states.append([c_t1, c_t2])\n",
    "        query.append(h_t2)\n",
    "        out = model.linear(h_t2)\n",
    "        outputs.append(out.unsqueeze(1))\n",
    "\n",
    "    output_prob = torch.cat(outputs,dim = 1)\n",
    "    out = torch.argmax(nn.Softmax(dim = 2)(output_prob), dim = 2)\n",
    "    \n",
    "#         Example usage:\n",
    "#         Replace 'probabilities', 'beam_width', and 'max_length' with your actual values\n",
    "#         probabilities = torch.tensor(...)  # Shape: (sequence_length, vocab_size)\n",
    "#         beam_width = 3\n",
    "#         max_length = 10\n",
    "    decoded_sequences = beam_search_decoder(output_prob[0], beam_width = 15, max_length = 500, diversity_penalty_weight=0.7)\n",
    "    vis = []\n",
    "    for sequence, score in decoded_sequences:\n",
    "        vis.append(model.seq_to_vis(sequence))\n",
    "        # print(f\"Sequence: {sequence}, {self.seq_to_vis(sequence)}, Log-Likelihood Score: {score}\")\n",
    "\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"},\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"} \n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}c\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}a\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}l\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}i\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}:\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}k\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\":\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}o\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}{\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\"\n"
     ]
    }
   ],
   "source": [
    "with open('Data/A3 files/progression.txt') as f:\n",
    "    progression, progression_transformed = f.readlines()\n",
    "    progression_transformed = [progression_transformed]\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_.eval()\n",
    "    decoder_.eval()\n",
    "    context_ = encoder_(progression_transformed)\n",
    "    output = inference(decoder_,context_)\n",
    "    for vis in output:\n",
    "        print(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"},\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"} \n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}c\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}a\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}l\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}i\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}:\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}k\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\":\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}o\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}{\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"detail\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\"\n"
     ]
    }
   ],
   "source": [
    "for vis in output:\n",
    "    print(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"},\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"} \n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}i\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}a\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}:\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}e\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}k\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}l\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\":\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}c\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}o\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}y\n"
     ]
    }
   ],
   "source": [
    "with open('Data/A3 files/progression.txt') as f:\n",
    "    progression, progression_transformed = f.readlines()\n",
    "    progression_transformed = [progression_transformed]\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_.eval()\n",
    "    decoder_.eval()\n",
    "    context_ = encoder_(progression_transformed)\n",
    "    output = inference(decoder_,context_)\n",
    "    for vis in output:\n",
    "        print(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 293.852,
   "position": {
    "height": "180.852px",
    "left": "1097px",
    "right": "20px",
    "top": "120px",
    "width": "323px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
