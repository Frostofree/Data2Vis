{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9637163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4fe36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with open(f\"{path}.sources\") as f:\n",
    "            self.sources = f.readlines()\n",
    "            self.vocab_src = set()\n",
    "            for lines in self.sources:\n",
    "                self.vocab_src.update(lines[1:-1])\n",
    "                \n",
    "        with open(f\"{path}.targets\") as f:\n",
    "            self.targets = f.readlines()\n",
    "            self.vocab_tgt = set()\n",
    "            for lines in self.targets:\n",
    "                self.vocab_tgt.update(lines)\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sources[idx][1:-1], self.targets[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_src = set()\n",
    "vocab_tgt = set()\n",
    "train_dataset = CustomDataset('./Data/A3 files/train')\n",
    "eval_dataset = CustomDataset('./Data/A3 files/dev')\n",
    "test_dataset = CustomDataset('./Data/A3 files/test')\n",
    "vocab_src.update(train_dataset.vocab_src)\n",
    "vocab_src.update(eval_dataset.vocab_src)\n",
    "vocab_src.update(test_dataset.vocab_src)\n",
    "vocab_tgt.update(train_dataset.vocab_tgt)\n",
    "vocab_tgt.update(eval_dataset.vocab_tgt)\n",
    "vocab_tgt.update(test_dataset.vocab_tgt)\n",
    "vocab_src = list(vocab_src)\n",
    "vocab_tgt = list(vocab_tgt)\n",
    "temp = {}\n",
    "for ind, key in enumerate(vocab_src):\n",
    "    temp[key] = ind\n",
    "vocab_src = temp\n",
    "temp = {}\n",
    "for ind, key in enumerate(vocab_tgt):\n",
    "    temp[key] = ind\n",
    "vocab_tgt = temp\n",
    "vocab_src[\"END\"] = 84\n",
    "vocab_src[\"PAD\"] = 85\n",
    "vocab_tgt[\"STR\"] = 44\n",
    "vocab_tgt[\"END\"] = 45\n",
    "vocab_tgt[\"PAD\"] = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc390e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size = 32, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b072f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dims = 512, hidden_size = 512,num_layers = 2, max_src = 500, max_tgt = 500):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab_src),dims)\n",
    "        self.input_size = dims\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.max_src = max_src\n",
    "        self.max_tgt = max_tgt\n",
    "        self.encoder = nn.LSTM(\n",
    "            dims, hidden_size, num_layers, batch_first = True,bidirectional=True, dropout = 0.5\n",
    "        )\n",
    "        \n",
    "    def encode_inp(self, x):\n",
    "        encoded_x = torch.zeros(len(x), self.max_src, dtype = int) + 85\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[i])):\n",
    "                encoded_x[i][j] = vocab_src[x[i][j]]\n",
    "            encoded_x[i][len(x[i])] = vocab_src[\"END\"]\n",
    "        return encoded_x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded_x = self.encode_inp(x)\n",
    "        input_seq = self.embedding(encoded_x)\n",
    "        hidden = torch.zeros(2*self.num_layers,input_seq.shape[0],self.hidden_size)\n",
    "        cell = torch.zeros(2*self.num_layers,input_seq.shape[0],self.hidden_size)\n",
    "        out, _ = self.encoder(input_seq,(hidden, cell))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd415842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,dims = 512, hidden_size = 512,num_layers = 2, max_src = 500, max_tgt = 500):\n",
    "        super(Decoder, self).__init__()\n",
    "#         self.embedding = embedding\n",
    "        self.embedding = nn.Embedding(len(vocab_tgt),dims)\n",
    "        self.input_size = dims\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.max_src = max_src\n",
    "        self.max_tgt = max_tgt\n",
    "        self.dec_cells = nn.ModuleList([nn.LSTMCell(2*hidden_size+dims, hidden_size), nn.LSTMCell(hidden_size, hidden_size)])\n",
    "        self.linear = nn.Linear(hidden_size,len(vocab_tgt)-1)\n",
    "    \n",
    "    def encode_inp(self, x):\n",
    "        encoded_x = torch.zeros(len(x), self.max_tgt+1, dtype = int) + 46\n",
    "        encoded_x_end = torch.zeros(len(x))\n",
    "        for i in range(len(x)):\n",
    "            encoded_x[i][0] = vocab_tgt[\"STR\"]\n",
    "            for j in range(len(x[i])):\n",
    "                encoded_x[i][j+1] = vocab_tgt[x[i][j]]\n",
    "            encoded_x[i][len(x[i])+1] = vocab_tgt[\"END\"]\n",
    "            encoded_x_end[i] = len(x[i])+1\n",
    "        return encoded_x, encoded_x_end\n",
    "    \n",
    "    def calcontext(self, timestep, query):\n",
    "        extended_query = torch.cat((query, query), dim = 1)\n",
    "        permuted_context = self.context.permute(1,0,2)\n",
    "#         for encoder_timestep in range(self.context.shape[1]):\n",
    "#             scores.append(torch.sum(self.context[:,encoder_timestep] * extended_query, dim = 1, keepdims=True))\n",
    "#         scores = torch.cat(scores, dim = 1)\n",
    "        scores = torch.sum(permuted_context * extended_query, dim = 2).permute(1,0)\n",
    "        weights = nn.Softmax(dim = 1)(scores).unsqueeze(2)\n",
    "        alignment = torch.sum(weights * self.context,  dim = 1)\n",
    "        \n",
    "        return alignment\n",
    "        \n",
    "    \n",
    "    def forward(self, context, target_,teacher_ratio):\n",
    "        self.context = context\n",
    "        encoded_x, encoded_x_end = self.encode_inp(target_)\n",
    "        target_seq = self.embedding(encoded_x)\n",
    "        \n",
    "        initial_hidden1 = torch.rand(target_seq.shape[0], self.hidden_size)\n",
    "        initial_cell1 = torch.rand(target_seq.shape[0], self.hidden_size)\n",
    "        initial_hidden2 = torch.rand(target_seq.shape[0], self.hidden_size)\n",
    "        initial_cell2 = torch.rand(target_seq.shape[0], self.hidden_size)\n",
    "        \n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        cell_states = []\n",
    "        query = [initial_hidden2]\n",
    "        for timestep in range(self.max_tgt+1):\n",
    "            if(timestep == 0):\n",
    "                (h_t1, c_t1) = self.dec_cells[0](torch.cat((target_seq[:,timestep],self.calcontext(0,query[-1])),dim=1), (initial_hidden1, initial_cell1))\n",
    "                (h_t2, c_t2) = self.dec_cells[1](h_t1, (initial_hidden2, initial_cell2))\n",
    "            else:\n",
    "                input = []\n",
    "                if(torch.rand(1).item() < teacher_ratio):\n",
    "                    input = target_seq[:,timestep]\n",
    "                else:\n",
    "                    input = self.embedding(torch.argmax(outputs[-1],dim=1))\n",
    "                    \n",
    "                (h_t1, c_t1) = self.dec_cells[0](torch.cat((input,self.calcontext(timestep,query[-1])),dim=1), (hidden_states[-1][0], cell_states[-1][0]))\n",
    "                (h_t2, c_t2) = self.dec_cells[1](h_t1, (hidden_states[-1][1], cell_states[-1][1]))\n",
    "            hidden_states.append([h_t1, h_t2])\n",
    "            cell_states.append([c_t1, c_t2])\n",
    "            query.append(h_t2)\n",
    "            out = self.linear(h_t2)\n",
    "            outputs.append(out.unsqueeze(1))\n",
    "    \n",
    "\n",
    "        output_prob = torch.cat(outputs,dim = 1)\n",
    "        \n",
    "        return nn.LogSoftmax(dim = 2)(output_prob), encoded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3094543",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "optimizer_encoder = optim.Adam(encoder.parameters(), lr=0.0001)\n",
    "optimizer_decoder = optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "criterion = nn.NLLLoss(ignore_index = 46)\n",
    "\n",
    "# Set up early stopping parameters\n",
    "patience = 100  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "loss_val = 0.0\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Loop\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    batch = 1\n",
    "    for data, vis in train_loader:\n",
    "        optimizer_encoder.zero_grad()\n",
    "        optimizer_decoder.zero_grad()\n",
    "        \n",
    "        context = encoder(data)\n",
    "        output, encoded_tgt = decoder(context, vis, 1.0)\n",
    "        output = output.permute(0,2,1)\n",
    "        loss = criterion(output[:,:,:-1], encoded_tgt[:,1:])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_decoder.step()\n",
    "        optimizer_encoder.step()\n",
    "        \n",
    "        print(f\"Epoch[{epoch}], Batch[{batch}], loss: {loss.item()}\")\n",
    "\n",
    "        # Calculate Perplexity\n",
    "        perplexity = torch.exp(loss)\n",
    "        print(f\"Epoch[{epoch}], Batch[{batch}], perplexity: {perplexity.item()}\")\n",
    "        batch += 1\n",
    "        \n",
    "    \n",
    "#     # Validation Loop\n",
    "#     encoder.eval()\n",
    "#     decoder.eval()\n",
    "#     with torch.no_grad():\n",
    "#         context_ = encoder(test_source)\n",
    "#         output_ = decoder(context_,test_target,0.0)\n",
    "#         loss_val = criterion(output_[:,:-1,:], test_target_encoded[:,1:,:])\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Eval Loss: {loss_val.item():.4f}')\n",
    "    \n",
    "#         if loss_val < best_val_loss:\n",
    "#             best_val_loss = loss_val\n",
    "#             epochs_since_improvement = 0\n",
    "#             torch.save(encoder.state_dict(), 'Model/encoder.pth')\n",
    "#             torch.save(decoder.state_dict(), 'Model/decoder.pth')\n",
    "\n",
    "#         else:\n",
    "#             epochs_since_improvement += 1\n",
    "\n",
    "#     # Check if we should stop training early\n",
    "#     if epochs_since_improvement >= patience:\n",
    "#         print(f\"Early stopping after {epoch+1} epochs with no improvement.\")\n",
    "#         break\n",
    "\n",
    "# if loss_val < best_val_loss:\n",
    "#     torch.save(encoder.state_dict(), 'Model/encoder.pth')\n",
    "#     torch.save(decoder.state_dict(), 'Model/decoder.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
